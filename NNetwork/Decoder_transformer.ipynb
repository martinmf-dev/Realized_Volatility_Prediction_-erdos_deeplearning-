{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fd6fa5",
   "metadata": {},
   "source": [
    "# A transformer with both encoder and decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e831d0a",
   "metadata": {},
   "source": [
    "### Intuitively, encoders \"understand and interpret\", while decoders \"write and describe\". In file Transformer_with_frozen_conv_1.ipynb, we opted for only using encoder: Our goal is timeseries regression (in the sense that we are looking to create one singular value as the target) and not timeseries prediction (In the sense that we are looking to create a new timeseries), so there was no need for us to implement a decoder to \"create a new timeseries in sequence\". However, it is a fact that our target value is the sum up a new timeseries, so, we now investigate in using autoregression with decoder to generate a new timeseries and then sum its time steps up. It is unclear if this will help at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1a1a7",
   "metadata": {},
   "source": [
    "## Import and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94c730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing, visualization\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);\n",
    "importlib.reload(visualization);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8353533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run this cell if needed. AMD gpus might need this. \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../dotenv_env/deep_learning.env\")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" # To, possibly fix memory leak issues. \n",
    "\n",
    "print(os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374d9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c78ae",
   "metadata": {},
   "source": [
    "## Data preparations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158aba2",
   "metadata": {},
   "source": [
    "### Load time id order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb76708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463a251",
   "metadata": {},
   "source": [
    "### Load timeseries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1924dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abd4d6",
   "metadata": {},
   "source": [
    "### Load target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d11c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")\n",
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)\n",
    "df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42e128",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdd64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine2/Desktop/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:396: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy[\"sub_int_num\"]=np.nan\n",
      "/home/machine2/Desktop/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:396: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy[\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)\n",
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]\n",
    "\n",
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3e8e2",
   "metadata": {},
   "source": [
    "## The model (Only autoregression, no teacher forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26b507",
   "metadata": {},
   "source": [
    "### As mentioned, we will use autoregression with decoder to create a new timeseries. One thing to note is that we will NOT be implementing teacher forcing with masked decoder for now. Teacher forcing is a powerful tool, but the context is not the same: We do not have another timeseries to as ground truth to train toward, so we have \"nothing to hide\" with the masking. Instead of teacher forcing, we are using the loss calculated with the actual target (future RV) and the aoturegression created timeseries to train. This might not even be possible: I foresee memory explosion since, normally, autoregression is done with no_grad() context and can take a huge amount of memory (since grad will keep the computation graph, which is HUGE if we are doing autoregression). But we will see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7183172",
   "metadata": {},
   "source": [
    "### Create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1d23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d948c",
   "metadata": {},
   "source": [
    "### Create components needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7383b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1353: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:30.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "ts_emb_dim=32\n",
    "n_diff=2\n",
    "ts_dim=n_diff+1\n",
    "\n",
    "pos_embedder=training.pos_emb_cross_attn(length=60,ts_dim=ts_dim,emb_dim=ts_emb_dim,dropout=0.2,num_heads=4,keep_mag=True).to(device=device)\n",
    "\n",
    "ts_encoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "ts_decoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "output_ff=nn.Sequential(\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=1)\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed9678",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76560c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder_decoder_model=training.encoder_decoder_autoregressionOnly(\n",
    "    pos_emb_model=pos_embedder,\n",
    "    output_feedforward=output_ff,\n",
    "    encoder_dropout=0.2,\n",
    "    decoder_dropout=0.2,\n",
    "    encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "    decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "    n_diff=n_diff,\n",
    "    encoder_layer_num=2,\n",
    "    decoder_layer_num=2,\n",
    "    input_scaler=10000,\n",
    "    ts_emb_dim=ts_emb_dim,\n",
    "    encoder_num_heads=4,\n",
    "    decoder_num_heads=4,\n",
    "    encoder_keep_mag=True,\n",
    "    decoder_keep_mag=True,\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff33d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(trans_encoder_decoder_model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15bc3951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "encoder_decoder_autoregression                               --\n",
       "├─frozen_diff_conv: 1-1                                      --\n",
       "│    └─Conv1d: 2-1                                           (2)\n",
       "├─pos_emb_cross_attn: 1-2                                    --\n",
       "│    └─Linear: 2-2                                           128\n",
       "│    └─Embedding: 2-3                                        1,920\n",
       "│    └─MultiheadAttention: 2-4                               3,168\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1             1,056\n",
       "│    └─LayerNorm: 2-5                                        64\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─ts_encoder: 2-6                                       --\n",
       "│    │    └─MultiheadAttention: 3-2                          4,224\n",
       "│    │    └─LayerNorm: 3-3                                   64\n",
       "│    │    └─ModuleList: 3-4                                  4,192\n",
       "│    │    └─LayerNorm: 3-5                                   64\n",
       "│    └─ts_encoder: 2-7                                       --\n",
       "│    │    └─MultiheadAttention: 3-6                          4,224\n",
       "│    │    └─LayerNorm: 3-7                                   64\n",
       "│    │    └─ModuleList: 3-8                                  4,192\n",
       "│    │    └─LayerNorm: 3-9                                   64\n",
       "├─ModuleList: 1-4                                            --\n",
       "│    └─ts_decoder: 2-8                                       --\n",
       "│    │    └─MultiheadAttention: 3-10                         4,224\n",
       "│    │    └─LayerNorm: 3-11                                  64\n",
       "│    │    └─MultiheadAttention: 3-12                         4,224\n",
       "│    │    └─LayerNorm: 3-13                                  64\n",
       "│    │    └─ModuleList: 3-14                                 4,192\n",
       "│    └─ts_decoder: 2-9                                       --\n",
       "│    │    └─MultiheadAttention: 3-15                         4,224\n",
       "│    │    └─LayerNorm: 3-16                                  64\n",
       "│    │    └─MultiheadAttention: 3-17                         4,224\n",
       "│    │    └─LayerNorm: 3-18                                  64\n",
       "│    │    └─ModuleList: 3-19                                 4,192\n",
       "├─Sequential: 1-5                                            --\n",
       "│    └─Linear: 2-10                                          33\n",
       "=====================================================================================\n",
       "Total params: 48,995\n",
       "Trainable params: 48,993\n",
       "Non-trainable params: 2\n",
       "====================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(trans_encoder_decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67e7b1",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3366e38",
   "metadata": {},
   "source": [
    "### As of now, the model has issue with memory: it is using all the 12 GB of my AMD GPU. I need to look deeper in attempt to fix this. According to my reading, since grad context keeps all the computation graphs, and autoregression may have HUGE graph, this is kinda expected. The only way, for now, I can see to bypass this is doing teacher training and doing autoregression under no_grad() context. But we will see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff097ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "HIP out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 11.98 GiB of which 0 bytes is free. Of the allocated memory 10.77 GiB is allocated by PyTorch, and 886.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreg_training_loop_rmspe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_encoder_decoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mot_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_train_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_val_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:178\u001b[39m, in \u001b[36mreg_training_loop_rmspe\u001b[39m\u001b[34m(optimizer, model, train_loader, val_loader, device, ot_steps, recall_best, eps, list_train_loss, list_val_loss, report_interval, n_epochs, scaler, norm_train_target, train_target, scheduler)\u001b[39m\n\u001b[32m    174\u001b[39m     train_target_mean=train_loader.dataset.feat_norm_dict[train_target][\u001b[32m0\u001b[39m]\n\u001b[32m    175\u001b[39m     train_target_std=train_loader.dataset.feat_norm_dict[train_target][\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m pred=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m#Moved here\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_train_target:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:997\u001b[39m, in \u001b[36mencoder_decoder_autoregression.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    995\u001b[39m     next_steps=target_input\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_layers: \n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m         next_steps=\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    998\u001b[39m     target_input=torch.cat([target_input,next_steps[:,-\u001b[32m1\u001b[39m:,:]],dim=\u001b[32m1\u001b[39m)\n\u001b[32m    999\u001b[39m out=\u001b[38;5;28mself\u001b[39m.output_feedforward(target_input[:,\u001b[32m1\u001b[39m:,:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:855\u001b[39m, in \u001b[36mts_decoder.forward\u001b[39m\u001b[34m(self, ground_target, encoder_memory, ground_target_mask)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    852\u001b[39m             ground_target,\n\u001b[32m    853\u001b[39m             encoder_memory,\n\u001b[32m    854\u001b[39m             ground_target_mask=\u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m     self_attn,_=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mground_target_mask\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keep_mag: \n\u001b[32m    857\u001b[39m         ground_target=(ground_target+self_attn+\u001b[38;5;28mself\u001b[39m.decoder_norm1(ground_target+self_attn))/\u001b[32m2\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/activation.py:1377\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1351\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1352\u001b[39m         query,\n\u001b[32m   1353\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1374\u001b[39m         is_causal=is_causal,\n\u001b[32m   1375\u001b[39m     )\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/functional.py:6468\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6464\u001b[39m     attn_output_weights = torch.baddbmm(\n\u001b[32m   6465\u001b[39m         attn_mask, q_scaled, k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m   6466\u001b[39m     )\n\u001b[32m   6467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6468\u001b[39m     attn_output_weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6469\u001b[39m attn_output_weights = softmax(attn_output_weights, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   6470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dropout_p > \u001b[32m0.0\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: HIP out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 11.98 GiB of which 0 bytes is free. Of the allocated memory 10.77 GiB is allocated by PyTorch, and 886.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(\n",
    "    optimizer=optimizer,\n",
    "    model=trans_encoder_decoder_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    ot_steps=20,\n",
    "    report_interval=5,\n",
    "    n_epochs=200,\n",
    "    list_train_loss=train_loss,\n",
    "    list_val_loss=val_loss,\n",
    "    device=device,\n",
    "    eps=1e-8,\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa253fa",
   "metadata": {},
   "source": [
    "## The Model (With only teacher forcing, using the current timeseries as ground_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3bc98",
   "metadata": {},
   "source": [
    "### The reason we went, first to the autoregression only method was that there was no ground target, but what if we just use the current sequence as the ground target in the decoder? Intuitively this is asking the cross attention layers: For the time steps in my query (the ground target), how can I adjust the query with the values (encoder output), based on the attention of keys (encoder output) on the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f0437",
   "metadata": {},
   "source": [
    "### Create dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda4a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fadb6",
   "metadata": {},
   "source": [
    "### Create components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc14a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_emb_dim=32\n",
    "n_diff=2\n",
    "ts_dim=n_diff+1\n",
    "\n",
    "pos_embedder=training.pos_emb_cross_attn(length=60,ts_dim=ts_dim,emb_dim=ts_emb_dim,dropout=0.2,num_heads=4,keep_mag=True).to(device=device)\n",
    "\n",
    "ts_encoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "ts_decoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "output_ff=nn.Sequential(\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=1)\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba9378",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd6155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder_decoder_tf_model=training.encoder_decoder_teacherforcing(\n",
    "    pos_emb_model=pos_embedder,\n",
    "    output_feedforward=output_ff,\n",
    "    encoder_dropout=0.2,\n",
    "    decoder_dropout=0.2,\n",
    "    encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "    decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "    n_diff=n_diff,\n",
    "    encoder_layer_num=2,\n",
    "    decoder_layer_num=2,\n",
    "    input_scaler=10000,\n",
    "    ts_emb_dim=ts_emb_dim,\n",
    "    encoder_num_heads=4,\n",
    "    decoder_num_heads=4,\n",
    "    encoder_keep_mag=True,\n",
    "    decoder_keep_mag=True,\n",
    "    return_sum=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107bebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(trans_encoder_decoder_tf_model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a420c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "encoder_decoder_teacherforcing                               --\n",
       "├─frozen_diff_conv: 1-1                                      --\n",
       "│    └─Conv1d: 2-1                                           (2)\n",
       "├─pos_emb_cross_attn: 1-2                                    --\n",
       "│    └─Linear: 2-2                                           128\n",
       "│    └─Embedding: 2-3                                        1,920\n",
       "│    └─MultiheadAttention: 2-4                               3,168\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1             1,056\n",
       "│    └─LayerNorm: 2-5                                        64\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─ts_encoder: 2-6                                       --\n",
       "│    │    └─MultiheadAttention: 3-2                          4,224\n",
       "│    │    └─LayerNorm: 3-3                                   64\n",
       "│    │    └─ModuleList: 3-4                                  4,192\n",
       "│    │    └─LayerNorm: 3-5                                   64\n",
       "│    └─ts_encoder: 2-7                                       --\n",
       "│    │    └─MultiheadAttention: 3-6                          4,224\n",
       "│    │    └─LayerNorm: 3-7                                   64\n",
       "│    │    └─ModuleList: 3-8                                  4,192\n",
       "│    │    └─LayerNorm: 3-9                                   64\n",
       "├─ModuleList: 1-4                                            --\n",
       "│    └─ts_decoder: 2-8                                       --\n",
       "│    │    └─MultiheadAttention: 3-10                         4,224\n",
       "│    │    └─LayerNorm: 3-11                                  64\n",
       "│    │    └─MultiheadAttention: 3-12                         4,224\n",
       "│    │    └─LayerNorm: 3-13                                  64\n",
       "│    │    └─ModuleList: 3-14                                 4,192\n",
       "│    └─ts_decoder: 2-9                                       --\n",
       "│    │    └─MultiheadAttention: 3-15                         4,224\n",
       "│    │    └─LayerNorm: 3-16                                  64\n",
       "│    │    └─MultiheadAttention: 3-17                         4,224\n",
       "│    │    └─LayerNorm: 3-18                                  64\n",
       "│    │    └─ModuleList: 3-19                                 4,192\n",
       "├─Sequential: 1-5                                            --\n",
       "│    └─Linear: 2-10                                          33\n",
       "=====================================================================================\n",
       "Total params: 48,995\n",
       "Trainable params: 48,993\n",
       "Non-trainable params: 2\n",
       "====================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(trans_encoder_decoder_tf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e797de",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d2745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new best validation loss at epoch  1  with validation loss of  tensor(0.2360, device='cuda:0') .\n",
      "At  24.368682146072388  epoch  1 has training loss  tensor(0.2746, device='cuda:0')  and validation loss  tensor(0.2360, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  2  with validation loss of  tensor(0.2356, device='cuda:0') .\n",
      "A new best validation loss at epoch  3  with validation loss of  tensor(0.2337, device='cuda:0') .\n",
      "A new best validation loss at epoch  4  with validation loss of  tensor(0.2316, device='cuda:0') .\n",
      "At  126.59550452232361  epoch  5 has training loss  tensor(0.2489, device='cuda:0')  and validation loss  tensor(0.2322, device='cuda:0') .\n",
      "\n",
      "At  254.07837462425232  epoch  10 has training loss  tensor(0.2474, device='cuda:0')  and validation loss  tensor(0.2334, device='cuda:0') .\n",
      "\n",
      "At epoch 10, learning rate has been updated from 0.001 to 0.0005, reloading previous best model weights from epoch 4 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  11  with validation loss of  tensor(0.2305, device='cuda:0') .\n",
      "At  381.963880777359  epoch  15 has training loss  tensor(0.2455, device='cuda:0')  and validation loss  tensor(0.2338, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  16  with validation loss of  tensor(0.2302, device='cuda:0') .\n",
      "At  510.6774184703827  epoch  20 has training loss  tensor(0.2447, device='cuda:0')  and validation loss  tensor(0.2303, device='cuda:0') .\n",
      "\n",
      "At epoch 22, learning rate has been updated from 0.0005 to 0.00025, reloading previous best model weights from epoch 16 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  23  with validation loss of  tensor(0.2300, device='cuda:0') .\n",
      "At  640.1796228885651  epoch  25 has training loss  tensor(0.2438, device='cuda:0')  and validation loss  tensor(0.2301, device='cuda:0') .\n",
      "\n",
      "At epoch 29, learning rate has been updated from 0.00025 to 0.000125, reloading previous best model weights from epoch 23 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  770.478236913681  epoch  30 has training loss  tensor(0.2426, device='cuda:0')  and validation loss  tensor(0.2304, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  31  with validation loss of  tensor(0.2300, device='cuda:0') .\n",
      "A new best validation loss at epoch  34  with validation loss of  tensor(0.2299, device='cuda:0') .\n",
      "At  900.7824077606201  epoch  35 has training loss  tensor(0.2425, device='cuda:0')  and validation loss  tensor(0.2303, device='cuda:0') .\n",
      "\n",
      "At  1030.697425365448  epoch  40 has training loss  tensor(0.2423, device='cuda:0')  and validation loss  tensor(0.2314, device='cuda:0') .\n",
      "\n",
      "At epoch 40, learning rate has been updated from 0.000125 to 6.25e-05, reloading previous best model weights from epoch 34 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  42  with validation loss of  tensor(0.2298, device='cuda:0') .\n",
      "At  1160.9671623706818  epoch  45 has training loss  tensor(0.2418, device='cuda:0')  and validation loss  tensor(0.2302, device='cuda:0') .\n",
      "\n",
      "At epoch 48, learning rate has been updated from 6.25e-05 to 3.125e-05, reloading previous best model weights from epoch 42 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  1291.4897248744965  epoch  50 has training loss  tensor(0.2415, device='cuda:0')  and validation loss  tensor(0.2300, device='cuda:0') .\n",
      "\n",
      "At epoch 54, learning rate has been updated from 3.125e-05 to 1.5625e-05, reloading previous best model weights from epoch 42 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  1422.023981332779  epoch  55 has training loss  tensor(0.2414, device='cuda:0')  and validation loss  tensor(0.2302, device='cuda:0') .\n",
      "\n",
      "At  1552.516384601593  epoch  60 has training loss  tensor(0.2414, device='cuda:0')  and validation loss  tensor(0.2305, device='cuda:0') .\n",
      "\n",
      "At epoch 60, learning rate has been updated from 1.5625e-05 to 7.8125e-06, reloading previous best model weights from epoch 42 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "The validation loss has not improved for  20  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 42  with validation loss:  tensor(0.2298, device='cuda:0') .\n",
      " The total number of epoch trained is  62 .\n",
      " Training completed in:  1604.637042760849 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('frozen_conv.frozen_conv.weight',\n",
       "              tensor([[[-1.,  1.]]], device='cuda:0')),\n",
       "             ('pos_emb.ts_proj.weight',\n",
       "              tensor([[ 0.5243, -0.2829,  0.1898],\n",
       "                      [ 0.1887,  0.1524, -0.3744],\n",
       "                      [-0.4206, -0.2532,  0.3629],\n",
       "                      [-0.0923,  0.0470,  0.2774],\n",
       "                      [-0.3300, -0.5331, -0.2996],\n",
       "                      [-0.0594, -0.2248, -0.1744],\n",
       "                      [-0.2332, -0.2448, -0.3765],\n",
       "                      [ 0.3739,  0.1150,  0.2523],\n",
       "                      [ 0.2041, -0.5227, -0.2917],\n",
       "                      [-0.3198,  0.3190,  0.1713],\n",
       "                      [-0.1335,  0.1406,  0.0728],\n",
       "                      [-0.3787,  0.2856, -0.1774],\n",
       "                      [ 0.3807, -0.4102, -0.2325],\n",
       "                      [ 0.1481, -0.0115, -0.1855],\n",
       "                      [ 0.0272,  0.3295,  0.0888],\n",
       "                      [-0.5111, -0.2753, -0.0785],\n",
       "                      [ 0.5766, -0.0617, -0.3039],\n",
       "                      [ 0.0300, -0.1933,  0.3012],\n",
       "                      [-0.2925, -0.1909,  0.1449],\n",
       "                      [ 0.1767, -0.2107, -0.2445],\n",
       "                      [ 0.3357, -0.1387, -0.0987],\n",
       "                      [ 0.1075, -0.1032,  0.1972],\n",
       "                      [-0.0604, -0.3734,  0.3556],\n",
       "                      [-0.5212,  0.1433,  0.1022],\n",
       "                      [ 0.2228, -0.6037, -0.4338],\n",
       "                      [-0.0549,  0.4311,  0.1873],\n",
       "                      [-0.0392, -0.1887, -0.2457],\n",
       "                      [-0.1709, -0.0382,  0.3300],\n",
       "                      [ 0.1392, -0.4524, -0.2727],\n",
       "                      [-0.4222,  0.2107, -0.3827],\n",
       "                      [ 0.2909, -0.3976, -0.2844],\n",
       "                      [ 0.1624,  0.1208, -0.0796]], device='cuda:0')),\n",
       "             ('pos_emb.ts_proj.bias',\n",
       "              tensor([-0.3019,  0.2836,  0.3164,  0.2308, -0.3342,  0.0526,  0.5367, -0.4126,\n",
       "                       0.1862, -0.0467,  0.5539,  0.2931,  0.3527, -0.3284, -0.2934,  0.0896,\n",
       "                       0.1727,  0.5797,  0.2616, -0.5172, -0.1208, -0.5043,  0.1644, -0.0633,\n",
       "                       0.2328, -0.3714, -0.1860,  0.4513, -0.2515,  0.2336, -0.1001,  0.1191],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_emb.weight',\n",
       "              tensor([[-0.2467,  0.4863,  1.6131,  ...,  1.1534,  0.2963, -0.8521],\n",
       "                      [-0.3376, -0.9835, -1.0627,  ...,  1.1016, -0.1834,  1.7877],\n",
       "                      [ 0.0472, -1.4857, -0.4454,  ..., -1.4425,  0.2244,  0.5991],\n",
       "                      ...,\n",
       "                      [ 0.9575,  0.3539,  0.5004,  ..., -0.1378,  0.2249,  0.4509],\n",
       "                      [ 0.0184,  1.2176,  0.3489,  ..., -0.7481,  0.4166, -1.2956],\n",
       "                      [-0.5357, -0.8191,  1.2668,  ..., -0.4813,  1.7434, -0.7242]],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.in_proj_weight',\n",
       "              tensor([[-0.1231, -0.0008,  0.0761,  ..., -0.1161,  0.0555,  0.1632],\n",
       "                      [-0.1503,  0.2324, -0.1166,  ...,  0.1914,  0.1203,  0.1693],\n",
       "                      [ 0.1713, -0.0553, -0.0579,  ..., -0.1844, -0.1792, -0.0780],\n",
       "                      ...,\n",
       "                      [-0.0116,  0.1481,  0.0026,  ..., -0.0613,  0.0562, -0.0893],\n",
       "                      [-0.2117, -0.0792, -0.1069,  ...,  0.2165, -0.0380,  0.0240],\n",
       "                      [-0.0772,  0.0710,  0.1214,  ...,  0.1416,  0.1012, -0.1734]],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.in_proj_bias',\n",
       "              tensor([ 3.0062e-03, -3.7004e-02,  3.5549e-02, -4.2981e-02, -7.8523e-02,\n",
       "                       7.8567e-02, -7.4644e-03,  3.3989e-02,  9.3950e-03, -8.3335e-03,\n",
       "                      -1.1059e-02, -3.1734e-02, -3.6551e-02, -1.5213e-02, -1.2508e-02,\n",
       "                       1.4380e-03,  9.1067e-03, -7.4100e-02, -5.7430e-02, -1.0535e-01,\n",
       "                      -2.7744e-02, -1.6745e-02, -2.5163e-02,  7.0414e-03, -5.2830e-02,\n",
       "                      -4.7440e-02,  1.1116e-02,  1.8214e-03,  4.2013e-03, -4.0678e-03,\n",
       "                      -3.3766e-02,  1.9769e-02, -1.5446e-04,  1.0149e-04,  1.6614e-04,\n",
       "                       1.8696e-04, -1.2649e-04, -1.9372e-04, -5.0371e-05, -1.5630e-05,\n",
       "                       9.1292e-04, -1.0931e-04,  2.1912e-04, -2.0570e-04,  1.3884e-03,\n",
       "                       1.2917e-03,  9.1767e-04,  1.3007e-03,  9.5151e-04,  2.6730e-04,\n",
       "                       3.2024e-05,  4.4008e-04,  1.7973e-05, -3.3391e-06, -6.2993e-04,\n",
       "                      -2.8540e-04, -3.7127e-04, -3.1858e-05, -1.2717e-03, -3.3200e-04,\n",
       "                      -1.0735e-03,  3.9330e-04, -2.5387e-04, -1.4299e-04,  1.3293e-02,\n",
       "                       6.1166e-03, -1.1777e-02,  9.7340e-03,  5.5767e-03,  2.0809e-02,\n",
       "                      -6.1126e-03,  6.3271e-03, -6.0017e-07, -2.5717e-03, -1.6241e-02,\n",
       "                       1.4782e-02, -4.9354e-03,  2.2137e-02, -4.8672e-03,  5.1387e-03,\n",
       "                      -2.4967e-02,  3.6847e-02,  1.7531e-02,  1.2584e-02,  8.6297e-03,\n",
       "                      -1.6730e-02, -2.0741e-02, -2.7764e-03,  2.8159e-03, -1.5240e-02,\n",
       "                      -1.7978e-02, -2.7458e-02,  2.6943e-02, -1.9226e-02, -1.3017e-02,\n",
       "                      -5.1705e-03], device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.out_proj.weight',\n",
       "              tensor([[ 0.0666, -0.0550,  0.0465,  ..., -0.0664, -0.0596,  0.0212],\n",
       "                      [-0.1272, -0.0075,  0.0988,  ...,  0.0927, -0.0546, -0.0932],\n",
       "                      [ 0.0809,  0.0052, -0.0947,  ..., -0.0144, -0.3120,  0.0570],\n",
       "                      ...,\n",
       "                      [ 0.2611,  0.0193, -0.1007,  ..., -0.0282, -0.1919, -0.0735],\n",
       "                      [ 0.0590,  0.0455,  0.0423,  ..., -0.0334, -0.0478, -0.0057],\n",
       "                      [ 0.0256, -0.0153,  0.1413,  ...,  0.0381,  0.1662, -0.0841]],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.out_proj.bias',\n",
       "              tensor([-0.0552, -0.0572,  0.0276, -0.0219, -0.0131,  0.0287,  0.0417, -0.0462,\n",
       "                       0.0137,  0.0191,  0.0618,  0.0211, -0.0130, -0.0554,  0.0034,  0.0573,\n",
       "                      -0.0203,  0.1088,  0.0338, -0.0598, -0.0354, -0.0423,  0.0737,  0.0475,\n",
       "                       0.0119, -0.0572, -0.0141,  0.0454, -0.0143,  0.0341, -0.0240, -0.0203],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_norm.weight',\n",
       "              tensor([0.7693, 0.7927, 0.7920, 0.8385, 0.8735, 0.7715, 0.8902, 0.7407, 0.8927,\n",
       "                      0.8388, 0.8758, 0.7107, 0.8781, 0.8537, 0.8097, 0.8425, 0.8733, 0.9320,\n",
       "                      0.8618, 0.8903, 0.8158, 0.8657, 0.8183, 0.8893, 0.8740, 0.8947, 0.8166,\n",
       "                      0.7940, 0.7864, 0.9630, 0.8256, 0.7720], device='cuda:0')),\n",
       "             ('pos_emb.pos_norm.bias',\n",
       "              tensor([-0.0560, -0.0629,  0.0400, -0.0467, -0.0284,  0.0489,  0.0376, -0.0672,\n",
       "                       0.0022,  0.0337,  0.0602,  0.0242,  0.0052, -0.0458,  0.0014,  0.0674,\n",
       "                      -0.0167,  0.0966,  0.0286, -0.0469, -0.0560, -0.0348,  0.0858,  0.0329,\n",
       "                       0.0082, -0.0644, -0.0150,  0.0418, -0.0177,  0.0701, -0.0582, -0.0249],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.in_proj_weight',\n",
       "              tensor([[-0.0857, -0.0577,  0.2444,  ..., -0.0296,  0.0919, -0.0608],\n",
       "                      [ 0.1100,  0.0844,  0.1361,  ..., -0.1715, -0.0415,  0.1161],\n",
       "                      [-0.0781, -0.1478,  0.1472,  ...,  0.0464, -0.0973,  0.1314],\n",
       "                      ...,\n",
       "                      [ 0.0859, -0.0762,  0.0158,  ...,  0.0495, -0.1533,  0.0390],\n",
       "                      [-0.1645, -0.2093,  0.1861,  ...,  0.1269, -0.1537,  0.0024],\n",
       "                      [-0.0320,  0.0125,  0.1054,  ...,  0.1544, -0.0901, -0.0615]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.in_proj_bias',\n",
       "              tensor([ 3.9076e-04,  6.5478e-02,  2.3768e-03, -2.3574e-03, -2.2096e-02,\n",
       "                       1.0212e-02,  4.6086e-02,  5.2526e-02,  3.2172e-02,  1.8615e-02,\n",
       "                       1.4165e-02,  3.0548e-02, -2.9795e-02,  1.3548e-02, -1.7994e-02,\n",
       "                       2.4305e-02, -4.0787e-03, -3.4270e-02,  6.6212e-02, -2.5628e-02,\n",
       "                       1.0302e-04,  5.8059e-02, -3.9465e-02, -6.9291e-02, -2.7621e-02,\n",
       "                      -3.1315e-02,  4.7886e-02, -1.3429e-02, -2.6409e-03,  1.7148e-02,\n",
       "                      -2.0842e-02, -2.5637e-02,  1.8355e-04,  5.4450e-04, -1.3355e-04,\n",
       "                      -1.3366e-04, -3.1109e-04, -1.1327e-05, -2.6298e-06, -8.2522e-05,\n",
       "                      -1.0431e-04,  4.0279e-04, -4.1191e-04, -1.1963e-04, -9.5492e-04,\n",
       "                       4.1849e-04,  1.7599e-05,  1.8665e-04, -5.6309e-06,  4.5584e-04,\n",
       "                      -7.3484e-04,  8.0313e-05,  2.3242e-04, -5.5557e-04, -1.2065e-05,\n",
       "                       1.2808e-03,  2.2437e-04, -5.6524e-05, -3.3464e-04, -3.7693e-05,\n",
       "                       7.5631e-05, -2.4822e-04, -1.0486e-04,  1.3916e-04, -1.3690e-02,\n",
       "                      -1.2279e-01, -6.0034e-02, -1.4106e-02, -6.5099e-02,  5.7555e-03,\n",
       "                       1.0870e-01, -2.1801e-02,  3.7518e-02, -7.4257e-02, -1.0403e-02,\n",
       "                       3.1751e-02,  6.3104e-03, -1.5674e-02,  6.2430e-02,  6.7906e-02,\n",
       "                       1.5055e-02,  3.3312e-02, -2.9195e-02, -6.8381e-04,  7.9454e-02,\n",
       "                       2.6565e-02,  1.5417e-02, -9.8908e-04,  5.7144e-02, -7.2398e-02,\n",
       "                      -2.9876e-02, -2.4360e-02,  3.4461e-02, -2.0167e-02,  1.4147e-02,\n",
       "                       4.0709e-02], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.0651, -0.0177,  0.1170,  ..., -0.1347,  0.0672, -0.0917],\n",
       "                      [-0.1027, -0.0586, -0.0736,  ...,  0.0920,  0.1047,  0.0722],\n",
       "                      [ 0.1887, -0.2206, -0.0701,  ...,  0.0298, -0.1325, -0.1706],\n",
       "                      ...,\n",
       "                      [ 0.0857,  0.2289, -0.0285,  ..., -0.0831, -0.1344, -0.0740],\n",
       "                      [ 0.0408, -0.1611, -0.0106,  ...,  0.0180,  0.0331,  0.1752],\n",
       "                      [ 0.0923,  0.2026,  0.1122,  ..., -0.0312, -0.0286,  0.1495]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.0942, -0.0235,  0.0471, -0.0182, -0.0847,  0.0534,  0.0551, -0.1091,\n",
       "                       0.0256,  0.0197,  0.1162,  0.1028,  0.0470, -0.0734, -0.0382,  0.0676,\n",
       "                      -0.0160,  0.2070,  0.0530, -0.0818, -0.0575, -0.0889,  0.1670,  0.0080,\n",
       "                       0.0378, -0.1402, -0.0632,  0.0700, -0.1032,  0.0793, -0.0565, -0.0092],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm1.weight',\n",
       "              tensor([0.6604, 0.8014, 0.7758, 0.8456, 0.9644, 0.8520, 0.8763, 0.7457, 0.8002,\n",
       "                      0.7460, 0.8491, 0.6014, 0.9212, 0.7463, 0.7834, 0.7895, 0.8172, 0.9053,\n",
       "                      0.7757, 0.8541, 0.7601, 0.8697, 0.8151, 0.8625, 0.8927, 0.8804, 0.8545,\n",
       "                      0.7591, 0.7835, 0.9905, 0.7937, 0.7866], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm1.bias',\n",
       "              tensor([-4.9046e-02, -7.1880e-02,  4.9692e-02, -3.4719e-02, -1.4125e-02,\n",
       "                       7.8056e-02,  2.4349e-02, -5.8112e-02,  1.4162e-03,  2.5291e-02,\n",
       "                       4.3003e-02, -5.2113e-04,  9.3189e-03, -2.6015e-02, -1.0475e-02,\n",
       "                       4.9107e-02, -1.2934e-02,  7.6449e-02,  3.0627e-02, -3.1213e-02,\n",
       "                      -4.4244e-02, -1.8864e-02,  6.7877e-02,  2.7901e-02,  1.2069e-02,\n",
       "                      -8.6616e-02, -9.5137e-03,  3.5201e-02, -6.9819e-03,  6.7429e-02,\n",
       "                      -2.8050e-02, -4.0924e-06], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.0.weight',\n",
       "              tensor([[-0.1589, -0.1213, -0.0090,  ..., -0.0449, -0.1178, -0.1426],\n",
       "                      [-0.1838,  0.1027, -0.0931,  ..., -0.0645, -0.1459,  0.0330],\n",
       "                      [ 0.0442,  0.0107,  0.1440,  ..., -0.1254,  0.0247, -0.1932],\n",
       "                      ...,\n",
       "                      [-0.1061, -0.1138,  0.0863,  ...,  0.0906, -0.0673,  0.0654],\n",
       "                      [-0.1469, -0.0694,  0.1819,  ...,  0.0644,  0.0149, -0.1423],\n",
       "                      [ 0.1299,  0.0315,  0.1107,  ..., -0.1147, -0.1634,  0.0757]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.0.bias',\n",
       "              tensor([ 0.0160, -0.0121, -0.0749,  0.0793, -0.1146, -0.0078,  0.0056, -0.0333,\n",
       "                      -0.1110,  0.1009, -0.0430, -0.2746, -0.1503, -0.0748,  0.0640,  0.0192,\n",
       "                      -0.1369,  0.0358, -0.1050, -0.2184,  0.0659, -0.0185, -0.1371, -0.1457,\n",
       "                      -0.0069, -0.2198, -0.1009, -0.0512, -0.0457, -0.1439, -0.0746, -0.0268,\n",
       "                      -0.1934, -0.1520, -0.2091, -0.1585, -0.0549, -0.0420, -0.1320, -0.2390,\n",
       "                      -0.0301,  0.0931,  0.0775, -0.0207, -0.0065, -0.2174, -0.1460, -0.0485,\n",
       "                      -0.1442, -0.1657, -0.0240, -0.0430, -0.2342,  0.0700, -0.0507,  0.0509,\n",
       "                      -0.0385, -0.1399, -0.0317,  0.0525, -0.1493,  0.0862, -0.1013,  0.0546],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.2.weight',\n",
       "              tensor([[ 0.0478, -0.0395,  0.0982,  ..., -0.0017,  0.0521,  0.0704],\n",
       "                      [-0.1075,  0.0329,  0.1203,  ...,  0.1909,  0.0191, -0.0319],\n",
       "                      [-0.1289, -0.0846, -0.0464,  ..., -0.2306,  0.0038, -0.0546],\n",
       "                      ...,\n",
       "                      [-0.0423, -0.0457,  0.0840,  ..., -0.0596, -0.0293,  0.0253],\n",
       "                      [ 0.0052,  0.0567,  0.0365,  ..., -0.1615, -0.0922,  0.0324],\n",
       "                      [ 0.1605,  0.0399, -0.0396,  ..., -0.0720, -0.0115,  0.0462]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.2.bias',\n",
       "              tensor([-0.0752, -0.1530,  0.0583,  0.0539, -0.1190, -0.0074,  0.0425, -0.0292,\n",
       "                       0.0656,  0.0885,  0.0599,  0.0176, -0.0270, -0.0475,  0.0657,  0.0839,\n",
       "                      -0.0302,  0.1002,  0.0302,  0.0421,  0.0385,  0.0245,  0.1248,  0.0411,\n",
       "                       0.0366, -0.0619,  0.0668,  0.0126,  0.1039,  0.0240, -0.0693, -0.0479],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm2.weight',\n",
       "              tensor([0.7918, 0.8200, 0.8231, 0.8589, 0.9153, 0.9641, 0.8849, 0.8123, 0.7913,\n",
       "                      0.7977, 0.8659, 0.7756, 0.8668, 0.8850, 0.8286, 0.8491, 0.8480, 0.9214,\n",
       "                      0.8626, 0.8701, 0.8254, 0.8398, 0.8158, 0.8455, 0.8663, 0.7942, 0.8547,\n",
       "                      0.8153, 0.7844, 0.9576, 0.8112, 0.7858], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm2.bias',\n",
       "              tensor([-0.0278, -0.0512,  0.0479, -0.0361, -0.0262,  0.0678,  0.0071, -0.0451,\n",
       "                      -0.0105,  0.0245,  0.0257, -0.0362, -0.0167,  0.0035,  0.0086,  0.0248,\n",
       "                       0.0037,  0.0551,  0.0007, -0.0127,  0.0075,  0.0015,  0.0529,  0.0035,\n",
       "                      -0.0071, -0.0611,  0.0016,  0.0204,  0.0512,  0.0251, -0.0217, -0.0095],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.in_proj_weight',\n",
       "              tensor([[-0.1309, -0.0782,  0.2747,  ...,  0.0404, -0.1002, -0.1484],\n",
       "                      [-0.0100, -0.1477, -0.0887,  ...,  0.1451,  0.0134, -0.1002],\n",
       "                      [ 0.2084, -0.0503,  0.0901,  ..., -0.1200,  0.1400,  0.1876],\n",
       "                      ...,\n",
       "                      [ 0.1778,  0.0042,  0.1776,  ..., -0.1682, -0.0737, -0.1035],\n",
       "                      [ 0.1365, -0.1255, -0.0725,  ..., -0.1669,  0.0344, -0.0691],\n",
       "                      [ 0.2710, -0.0317,  0.2617,  ...,  0.0948,  0.0770,  0.1847]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.in_proj_bias',\n",
       "              tensor([ 1.6352e-02, -1.5222e-02, -2.7435e-02,  1.6997e-02, -2.5280e-02,\n",
       "                      -3.6986e-02,  3.2610e-02,  2.9154e-02,  3.5756e-02,  5.0532e-02,\n",
       "                      -5.3228e-02,  1.0151e-02,  1.8914e-02,  2.8553e-02,  1.9177e-02,\n",
       "                       3.1675e-02,  1.0937e-02, -6.8878e-03,  5.3203e-02,  4.2689e-02,\n",
       "                       2.8012e-02, -1.6727e-02,  2.2797e-02,  3.7702e-03,  4.2133e-02,\n",
       "                      -9.2815e-03,  3.6703e-02,  6.6466e-02, -5.3733e-02, -3.9813e-02,\n",
       "                      -5.5667e-02, -7.3637e-03,  2.1179e-05,  5.6928e-05,  1.6662e-04,\n",
       "                      -4.1785e-07, -1.1188e-04, -6.6413e-05,  3.1282e-05,  2.7543e-05,\n",
       "                       1.5508e-05, -4.8210e-05, -5.7735e-05, -8.4760e-05, -1.0240e-04,\n",
       "                      -1.6498e-04, -1.6776e-05,  4.4722e-05,  1.0666e-04,  2.8041e-05,\n",
       "                       2.6683e-05, -1.2865e-04, -5.2027e-05,  2.8099e-06,  5.7005e-05,\n",
       "                      -7.0133e-05, -2.3509e-05,  1.5154e-04,  4.1493e-05,  3.5384e-05,\n",
       "                      -2.8327e-05,  1.1512e-04,  5.9010e-04,  7.5139e-05,  2.9953e-02,\n",
       "                      -7.6004e-02, -5.3340e-04,  8.1826e-03,  4.6425e-03, -1.9377e-03,\n",
       "                      -1.0973e-02, -5.8123e-03,  1.7487e-02,  2.5306e-02, -2.6116e-02,\n",
       "                      -1.0982e-02,  7.4072e-03, -1.6609e-02,  6.7311e-03, -6.0299e-03,\n",
       "                       3.2332e-03,  1.4636e-02, -9.7991e-02, -6.1032e-02,  1.2227e-02,\n",
       "                      -2.3079e-02, -2.6073e-02,  2.8804e-02, -2.2059e-02, -2.7460e-02,\n",
       "                       1.8705e-02, -3.6062e-02,  8.2624e-02, -9.2484e-03, -7.3978e-02,\n",
       "                       5.0702e-02], device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.out_proj.weight',\n",
       "              tensor([[-0.0746,  0.0143,  0.0727,  ...,  0.0678, -0.0746, -0.0685],\n",
       "                      [-0.1691, -0.0141,  0.1317,  ...,  0.0564,  0.0252, -0.0834],\n",
       "                      [ 0.0112, -0.0076, -0.0253,  ...,  0.1491,  0.1478,  0.0110],\n",
       "                      ...,\n",
       "                      [ 0.0847,  0.1614, -0.2720,  ...,  0.1126, -0.0328,  0.0933],\n",
       "                      [ 0.0220, -0.0029,  0.0393,  ...,  0.1534,  0.0789, -0.0141],\n",
       "                      [ 0.0677, -0.1303,  0.0839,  ...,  0.1613, -0.0841, -0.0869]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.0453, -0.0567,  0.0284, -0.0699, -0.0931,  0.0152,  0.0155, -0.0756,\n",
       "                       0.0037,  0.0032,  0.0693, -0.0547,  0.0445, -0.0111,  0.0029,  0.0389,\n",
       "                       0.0143,  0.1099, -0.0461, -0.0326,  0.0038, -0.0164,  0.1093, -0.0220,\n",
       "                      -0.0183, -0.1753, -0.0220,  0.0380, -0.0050,  0.0199, -0.0399, -0.0022],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm1.weight',\n",
       "              tensor([0.7647, 0.8394, 0.8162, 0.8727, 0.8959, 0.9347, 0.8480, 0.7658, 0.8339,\n",
       "                      0.7980, 0.8267, 0.7924, 0.8486, 0.9028, 0.7928, 0.8477, 0.8367, 0.8828,\n",
       "                      0.8891, 0.8565, 0.8755, 0.8321, 0.8018, 0.8236, 0.8917, 0.9603, 0.8628,\n",
       "                      0.7803, 0.8560, 0.9903, 0.8139, 0.8104], device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm1.bias',\n",
       "              tensor([-0.0074, -0.0463,  0.0337, -0.0468, -0.0074,  0.0321, -0.0034, -0.0240,\n",
       "                      -0.0039,  0.0174,  0.0075, -0.0410, -0.0269, -0.0004,  0.0192,  0.0079,\n",
       "                       0.0048,  0.0313, -0.0181, -0.0007,  0.0097,  0.0120,  0.0293,  0.0169,\n",
       "                      -0.0240, -0.0408,  0.0007,  0.0152,  0.0284,  0.0115, -0.0083, -0.0008],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.0.weight',\n",
       "              tensor([[-0.1589, -0.1213, -0.0090,  ..., -0.0449, -0.1178, -0.1426],\n",
       "                      [-0.1838,  0.1027, -0.0931,  ..., -0.0645, -0.1459,  0.0330],\n",
       "                      [ 0.0442,  0.0107,  0.1440,  ..., -0.1254,  0.0247, -0.1932],\n",
       "                      ...,\n",
       "                      [-0.1061, -0.1138,  0.0863,  ...,  0.0906, -0.0673,  0.0654],\n",
       "                      [-0.1469, -0.0694,  0.1819,  ...,  0.0644,  0.0149, -0.1423],\n",
       "                      [ 0.1299,  0.0315,  0.1107,  ..., -0.1147, -0.1634,  0.0757]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.0.bias',\n",
       "              tensor([ 0.0160, -0.0121, -0.0749,  0.0793, -0.1146, -0.0078,  0.0056, -0.0333,\n",
       "                      -0.1110,  0.1009, -0.0430, -0.2746, -0.1503, -0.0748,  0.0640,  0.0192,\n",
       "                      -0.1369,  0.0358, -0.1050, -0.2184,  0.0659, -0.0185, -0.1371, -0.1457,\n",
       "                      -0.0069, -0.2198, -0.1009, -0.0512, -0.0457, -0.1439, -0.0746, -0.0268,\n",
       "                      -0.1934, -0.1520, -0.2091, -0.1585, -0.0549, -0.0420, -0.1320, -0.2390,\n",
       "                      -0.0301,  0.0931,  0.0775, -0.0207, -0.0065, -0.2174, -0.1460, -0.0485,\n",
       "                      -0.1442, -0.1657, -0.0240, -0.0430, -0.2342,  0.0700, -0.0507,  0.0509,\n",
       "                      -0.0385, -0.1399, -0.0317,  0.0525, -0.1493,  0.0862, -0.1013,  0.0546],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.2.weight',\n",
       "              tensor([[ 0.0478, -0.0395,  0.0982,  ..., -0.0017,  0.0521,  0.0704],\n",
       "                      [-0.1075,  0.0329,  0.1203,  ...,  0.1909,  0.0191, -0.0319],\n",
       "                      [-0.1289, -0.0846, -0.0464,  ..., -0.2306,  0.0038, -0.0546],\n",
       "                      ...,\n",
       "                      [-0.0423, -0.0457,  0.0840,  ..., -0.0596, -0.0293,  0.0253],\n",
       "                      [ 0.0052,  0.0567,  0.0365,  ..., -0.1615, -0.0922,  0.0324],\n",
       "                      [ 0.1605,  0.0399, -0.0396,  ..., -0.0720, -0.0115,  0.0462]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.2.bias',\n",
       "              tensor([-0.0752, -0.1530,  0.0583,  0.0539, -0.1190, -0.0074,  0.0425, -0.0292,\n",
       "                       0.0656,  0.0885,  0.0599,  0.0176, -0.0270, -0.0475,  0.0657,  0.0839,\n",
       "                      -0.0302,  0.1002,  0.0302,  0.0421,  0.0385,  0.0245,  0.1248,  0.0411,\n",
       "                       0.0366, -0.0619,  0.0668,  0.0126,  0.1039,  0.0240, -0.0693, -0.0479],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm2.weight',\n",
       "              tensor([0.8032, 0.8424, 0.8254, 0.8753, 0.8245, 0.8930, 0.8594, 0.8519, 0.8132,\n",
       "                      0.8030, 0.8025, 0.7930, 0.8285, 0.9241, 0.8162, 0.8407, 0.8096, 0.8757,\n",
       "                      0.7524, 0.8537, 0.8471, 0.8239, 0.7947, 0.7996, 0.9242, 0.9004, 0.8748,\n",
       "                      0.8316, 0.9003, 0.9145, 0.8242, 0.8100], device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm2.bias',\n",
       "              tensor([ 0.0010, -0.0526,  0.0571, -0.0232,  0.0474,  0.0106, -0.0072, -0.0066,\n",
       "                      -0.0437,  0.0263, -0.0182, -0.0583, -0.0641,  0.0028,  0.0140,  0.0211,\n",
       "                      -0.0240,  0.0101,  0.0233,  0.0126,  0.0165,  0.0233,  0.0036,  0.0332,\n",
       "                      -0.0172,  0.0247, -0.0021,  0.0063,  0.0480, -0.0225, -0.0081, -0.0303],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.in_proj_weight',\n",
       "              tensor([[ 1.8134e-01,  1.1529e-01,  1.1530e-01,  ..., -1.9448e-01,\n",
       "                        1.8422e-01,  2.3894e-01],\n",
       "                      [-1.2397e-01,  4.5207e-02, -8.1844e-02,  ...,  6.5512e-02,\n",
       "                       -7.1931e-02,  1.3955e-01],\n",
       "                      [ 1.9323e-01, -8.0288e-02,  1.1686e-04,  ..., -2.4550e-01,\n",
       "                        2.6280e-02, -1.1538e-01],\n",
       "                      ...,\n",
       "                      [-1.8157e-01, -1.1446e-01,  1.1977e-01,  ...,  1.4490e-01,\n",
       "                        1.8285e-01, -7.7089e-02],\n",
       "                      [-1.8180e-01, -9.3029e-02, -1.8051e-01,  ...,  2.8113e-01,\n",
       "                        1.6048e-02,  6.2523e-02],\n",
       "                      [ 3.2973e-03, -2.3543e-01, -1.7784e-02,  ...,  9.9511e-02,\n",
       "                       -2.1956e-01,  8.9868e-02]], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.in_proj_bias',\n",
       "              tensor([ 6.1420e-03, -1.7289e-02, -7.6377e-02, -1.2644e-02, -1.0588e-01,\n",
       "                       9.3235e-03, -7.4502e-02,  9.8430e-02,  2.0601e-02, -6.0012e-02,\n",
       "                       9.3253e-04,  3.6723e-02,  9.3423e-03,  5.8035e-02,  3.8202e-02,\n",
       "                       8.0863e-02, -8.3526e-02, -5.7854e-02,  8.2236e-03, -1.2203e-02,\n",
       "                       5.7406e-02, -1.2832e-01, -1.9755e-02, -3.5635e-02, -4.7777e-03,\n",
       "                       6.0975e-02,  2.7015e-02, -2.5294e-02, -5.3012e-02,  4.2629e-02,\n",
       "                       3.8385e-02, -4.2203e-03, -1.0913e-04, -1.3810e-04, -1.8360e-04,\n",
       "                      -1.2602e-04,  1.1477e-04, -3.6429e-04, -5.2885e-05, -7.2403e-06,\n",
       "                      -3.7917e-04, -1.4843e-04,  1.1822e-04, -2.4478e-04,  1.4965e-04,\n",
       "                      -1.2054e-04, -1.2539e-04, -3.0499e-04, -9.1501e-05, -5.7974e-07,\n",
       "                       4.1447e-05,  2.4171e-05, -7.8052e-05,  3.2743e-05,  1.3228e-04,\n",
       "                       7.6917e-05,  1.7183e-05, -5.2526e-05, -1.6237e-05,  9.1919e-05,\n",
       "                       4.5604e-04, -2.1632e-04, -1.3991e-04,  8.5175e-05,  3.1673e-02,\n",
       "                       6.4214e-02, -7.4086e-03, -4.9691e-02, -3.2877e-02, -7.7823e-03,\n",
       "                       1.6464e-02,  1.3988e-02, -1.4018e-02,  1.5923e-02, -5.0684e-03,\n",
       "                      -1.2213e-02, -2.8537e-03,  1.7284e-02, -4.2853e-03, -2.8184e-02,\n",
       "                      -1.0831e-02, -1.1075e-02, -2.6034e-03,  1.9378e-02,  1.1214e-02,\n",
       "                      -3.2835e-02,  1.8047e-02,  4.6012e-02, -2.7333e-02, -2.6944e-02,\n",
       "                       3.1693e-02,  3.1871e-02, -4.7174e-02, -1.9880e-02,  9.5333e-03,\n",
       "                      -3.4937e-02], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.out_proj.weight',\n",
       "              tensor([[ 0.0924, -0.0999, -0.1071,  ...,  0.1480,  0.1075, -0.1351],\n",
       "                      [-0.0448, -0.1310,  0.1592,  ..., -0.0122,  0.0482, -0.0114],\n",
       "                      [ 0.0011, -0.2178,  0.0610,  ..., -0.0490,  0.1339, -0.0833],\n",
       "                      ...,\n",
       "                      [ 0.1436, -0.1651,  0.0761,  ..., -0.2278, -0.1342, -0.0785],\n",
       "                      [-0.1431, -0.0651, -0.0119,  ..., -0.0887, -0.1100, -0.1471],\n",
       "                      [-0.0344, -0.0038, -0.2553,  ...,  0.1265,  0.1233, -0.0511]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.out_proj.bias',\n",
       "              tensor([ 0.0045, -0.0385,  0.0053, -0.0042, -0.0728,  0.0659, -0.0084, -0.0477,\n",
       "                       0.0060,  0.0070,  0.0215, -0.0122, -0.0226,  0.0143, -0.0344,  0.0436,\n",
       "                       0.0154,  0.0979, -0.0357,  0.0032,  0.0101,  0.0195,  0.0570, -0.0115,\n",
       "                      -0.0187, -0.1233, -0.0053,  0.0176,  0.0341,  0.0029, -0.0207,  0.0092],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm1.weight',\n",
       "              tensor([0.8211, 0.8973, 0.7896, 0.7794, 0.8774, 0.9313, 0.8504, 0.9018, 0.8261,\n",
       "                      0.8175, 0.8701, 0.8541, 0.8758, 0.8453, 0.8433, 0.8985, 0.8512, 1.0096,\n",
       "                      0.8809, 0.8602, 0.8017, 0.8824, 0.8503, 0.8520, 0.8556, 0.9213, 0.8285,\n",
       "                      0.8457, 0.8926, 0.9181, 0.8373, 0.8491], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm1.bias',\n",
       "              tensor([ 0.0102, -0.0386,  0.0019, -0.0010, -0.0407,  0.0426, -0.0135, -0.0322,\n",
       "                       0.0030,  0.0058,  0.0101, -0.0128, -0.0493,  0.0170, -0.0288,  0.0322,\n",
       "                       0.0158,  0.0830, -0.0332,  0.0088,  0.0059,  0.0251,  0.0374, -0.0095,\n",
       "                      -0.0182, -0.0918,  0.0016,  0.0120,  0.0365,  0.0042, -0.0106,  0.0050],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.in_proj_weight',\n",
       "              tensor([[ 0.0488, -0.1115, -0.0246,  ..., -0.0358, -0.2227,  0.0747],\n",
       "                      [-0.2452,  0.1517, -0.0856,  ...,  0.0814,  0.0825, -0.3072],\n",
       "                      [ 0.1135,  0.0006,  0.0984,  ...,  0.1188,  0.0078, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0088,  0.1769,  0.1388,  ...,  0.0853, -0.2462, -0.0212],\n",
       "                      [-0.1051, -0.0240,  0.1521,  ...,  0.1335, -0.0853, -0.1226],\n",
       "                      [ 0.0028,  0.1433,  0.1654,  ..., -0.1406,  0.1010, -0.0821]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.in_proj_bias',\n",
       "              tensor([ 5.7941e-02, -1.3652e-02, -3.3256e-02, -4.0252e-02,  7.8844e-03,\n",
       "                      -3.7990e-03,  5.6426e-03,  3.8241e-02, -2.5984e-02,  9.8772e-02,\n",
       "                      -5.0112e-02,  4.5502e-02,  3.3300e-03, -6.6569e-02,  4.1834e-02,\n",
       "                       4.4098e-02,  8.2156e-03,  7.1663e-02, -2.2703e-02, -1.7542e-02,\n",
       "                      -1.7442e-02, -2.1639e-02, -1.7377e-02, -4.0567e-03, -1.0612e-01,\n",
       "                       4.1080e-02,  2.3033e-02, -2.3860e-02, -4.7868e-02,  4.6102e-02,\n",
       "                      -6.1604e-02, -2.1300e-02, -2.7767e-05,  1.8395e-04, -2.1742e-04,\n",
       "                       7.4896e-05, -1.1088e-04,  1.6940e-04, -2.9863e-04,  4.5314e-05,\n",
       "                      -2.3826e-04,  3.3249e-04, -1.6046e-05,  4.6238e-04,  8.4529e-05,\n",
       "                      -3.7732e-05, -1.5074e-04,  1.0992e-05,  5.3538e-05, -6.5188e-05,\n",
       "                       1.1304e-04,  3.6310e-05,  9.8507e-06, -1.5983e-05,  3.2806e-04,\n",
       "                       1.4645e-04, -1.3902e-04,  2.1011e-04, -1.2367e-05, -2.0412e-04,\n",
       "                      -1.0703e-05, -2.7744e-05, -4.1492e-04,  3.4666e-05,  2.0167e-02,\n",
       "                       1.9294e-02,  1.0746e-02,  1.8161e-02, -2.1538e-02, -3.6563e-02,\n",
       "                      -1.1729e-02,  1.5824e-02,  2.3498e-02,  7.5295e-02,  1.0953e-02,\n",
       "                      -1.5679e-02,  3.1759e-02, -6.5657e-02, -1.7860e-02, -3.2424e-04,\n",
       "                       4.8822e-02,  2.1925e-02, -3.4427e-02,  5.6341e-03, -8.9421e-03,\n",
       "                      -1.0214e-03,  9.3976e-03, -5.7532e-02,  2.3710e-02, -2.3136e-02,\n",
       "                      -6.7989e-03,  1.8121e-02,  9.6699e-03,  4.1473e-02, -1.3228e-02,\n",
       "                      -3.1809e-02], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.out_proj.weight',\n",
       "              tensor([[ 9.7375e-02, -3.8083e-02,  1.5720e-01,  ..., -8.2844e-02,\n",
       "                       -1.4692e-01, -1.7345e-02],\n",
       "                      [ 1.8782e-01,  7.5479e-03,  5.6468e-02,  ...,  8.3096e-02,\n",
       "                       -2.1559e-01, -5.9459e-02],\n",
       "                      [ 2.9704e-02, -1.3618e-01, -9.8012e-02,  ..., -4.3336e-02,\n",
       "                        7.1078e-02,  1.1997e-02],\n",
       "                      ...,\n",
       "                      [ 9.6225e-02, -5.2840e-02, -4.5249e-02,  ...,  1.1491e-01,\n",
       "                       -1.3701e-01,  5.0999e-02],\n",
       "                      [ 8.7491e-02, -7.4306e-03, -6.9883e-03,  ..., -1.6978e-02,\n",
       "                        1.1115e-04,  8.3357e-02],\n",
       "                      [ 4.3101e-02,  8.1062e-02,  5.1532e-02,  ...,  3.9650e-02,\n",
       "                       -6.8143e-02,  1.2807e-01]], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.out_proj.bias',\n",
       "              tensor([ 0.0098, -0.0566,  0.0046,  0.0040, -0.0452,  0.0590, -0.0154, -0.0439,\n",
       "                       0.0002,  0.0167,  0.0097, -0.0003, -0.0684,  0.0183, -0.0306,  0.0506,\n",
       "                       0.0153,  0.0807, -0.0367,  0.0091,  0.0052,  0.0293,  0.0288, -0.0091,\n",
       "                      -0.0028, -0.0803,  0.0066,  0.0082,  0.0250,  0.0045, -0.0088, -0.0097],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm2.weight',\n",
       "              tensor([0.8111, 0.8671, 0.7535, 0.7746, 0.8332, 0.8561, 0.8386, 0.9195, 0.8105,\n",
       "                      0.7925, 0.8517, 0.8660, 0.8287, 0.7109, 0.8336, 0.9016, 0.8272, 0.9370,\n",
       "                      0.8730, 0.8424, 0.8161, 0.8617, 0.8055, 0.8409, 0.8037, 0.9327, 0.8052,\n",
       "                      0.8557, 0.8699, 0.8638, 0.8770, 0.8170], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm2.bias',\n",
       "              tensor([ 1.5379e-02, -5.6710e-02,  1.0069e-03,  7.3185e-03, -1.5632e-05,\n",
       "                       2.2147e-02, -1.6725e-02, -3.0625e-02, -3.6631e-03,  1.6883e-02,\n",
       "                      -6.4247e-04, -4.6991e-03, -1.2006e-01,  2.1036e-02, -1.8666e-02,\n",
       "                       4.6454e-02,  1.2158e-02,  5.1187e-02, -2.8638e-02,  1.5525e-02,\n",
       "                       5.1970e-04,  3.5090e-02, -6.5143e-03, -4.6877e-03, -5.7174e-03,\n",
       "                      -5.6363e-02,  1.2710e-02,  1.0357e-02,  3.7263e-02,  5.5294e-03,\n",
       "                       8.6171e-04, -1.4512e-02], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.0.weight',\n",
       "              tensor([[ 0.1942,  0.2160,  0.1148,  ...,  0.0137,  0.0289,  0.0358],\n",
       "                      [ 0.0188, -0.0980,  0.0354,  ..., -0.0228,  0.4109, -0.1035],\n",
       "                      [-0.0710,  0.1747,  0.1740,  ...,  0.0782, -0.0925,  0.1080],\n",
       "                      ...,\n",
       "                      [-0.0162, -0.0157,  0.1829,  ..., -0.0006,  0.2555, -0.0665],\n",
       "                      [ 0.0711,  0.0441,  0.0283,  ..., -0.0334,  0.0944,  0.0794],\n",
       "                      [-0.0986,  0.1661,  0.0513,  ..., -0.0824,  0.0121, -0.0574]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.0.bias',\n",
       "              tensor([-0.0240, -0.0292, -0.0263,  0.0306,  0.0498,  0.0252,  0.0610,  0.0037,\n",
       "                       0.0644, -0.1367, -0.0077, -0.1354, -0.0931,  0.0443,  0.1253, -0.0569,\n",
       "                      -0.1446, -0.0027,  0.1267, -0.1551, -0.1205, -0.0390, -0.0285, -0.0014,\n",
       "                       0.0334,  0.0069, -0.0385, -0.0859,  0.0188, -0.1648, -0.0221,  0.0062,\n",
       "                       0.0170, -0.0077, -0.0101, -0.0925, -0.1386,  0.0521, -0.1224,  0.0219,\n",
       "                      -0.0764, -0.1314, -0.1003,  0.0379,  0.0293,  0.1768, -0.0427, -0.2523,\n",
       "                      -0.1017,  0.1148, -0.1618, -0.0901, -0.0075,  0.0513,  0.0084,  0.0059,\n",
       "                      -0.1267, -0.1364, -0.0212,  0.0116, -0.0427, -0.1307,  0.0206,  0.0819],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.2.weight',\n",
       "              tensor([[-0.0442,  0.1170,  0.0550,  ..., -0.1747, -0.0362, -0.0230],\n",
       "                      [-0.0793,  0.0893, -0.1893,  ..., -0.0946, -0.0733,  0.0667],\n",
       "                      [-0.0721,  0.0736, -0.1248,  ..., -0.0706, -0.0309,  0.0725],\n",
       "                      ...,\n",
       "                      [-0.2553,  0.1058,  0.0040,  ..., -0.1037,  0.0523, -0.1094],\n",
       "                      [ 0.0513, -0.0059,  0.1422,  ...,  0.0536,  0.0377, -0.0824],\n",
       "                      [-0.1472,  0.0874, -0.1464,  ..., -0.1791, -0.0899, -0.0042]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.2.bias',\n",
       "              tensor([ 0.1009, -0.0170, -0.0263,  0.0153,  0.0952, -0.0446,  0.0222,  0.0534,\n",
       "                       0.0219,  0.1093, -0.1124, -0.0366,  0.0827, -0.0580, -0.0814, -0.0536,\n",
       "                      -0.0058,  0.0522,  0.0957, -0.0217, -0.1060, -0.0175,  0.0440, -0.0032,\n",
       "                       0.0145,  0.0693,  0.0889, -0.0015, -0.0317, -0.0619, -0.0443, -0.0717],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.in_proj_weight',\n",
       "              tensor([[ 0.1032, -0.0764, -0.0338,  ..., -0.3462, -0.0759,  0.0588],\n",
       "                      [-0.1803, -0.0366,  0.0587,  ..., -0.0335, -0.2702, -0.1835],\n",
       "                      [-0.1095, -0.0871, -0.1022,  ..., -0.0088, -0.1337, -0.0631],\n",
       "                      ...,\n",
       "                      [-0.1372,  0.1262, -0.2504,  ...,  0.1562, -0.0932, -0.0644],\n",
       "                      [-0.1021, -0.0507,  0.0533,  ...,  0.0390,  0.0706,  0.0569],\n",
       "                      [ 0.0489,  0.1828,  0.1174,  ..., -0.0605,  0.1025, -0.0393]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.in_proj_bias',\n",
       "              tensor([ 3.8762e-02, -6.9956e-03, -5.4446e-02, -3.9858e-02,  3.8683e-02,\n",
       "                      -5.5651e-02, -8.4083e-02,  5.8710e-02,  7.7136e-02, -1.3328e-02,\n",
       "                       2.3679e-02,  5.9608e-02, -3.0713e-03, -5.7874e-02,  4.9824e-02,\n",
       "                      -5.4228e-03, -1.7332e-03, -2.3426e-03, -5.1634e-02, -4.4909e-02,\n",
       "                      -1.3043e-02,  2.9508e-02, -5.2499e-02, -4.2734e-02, -9.0687e-02,\n",
       "                      -3.9678e-02,  8.9478e-02, -7.5696e-02, -1.8099e-02,  4.4351e-02,\n",
       "                       1.3596e-01,  5.0212e-02, -1.9062e-06, -2.2975e-04, -8.1902e-07,\n",
       "                      -2.5944e-04, -4.1780e-04,  1.0763e-05,  2.8408e-05, -5.9497e-05,\n",
       "                      -3.1989e-04,  3.8465e-05, -3.5404e-05,  9.7850e-05, -1.7220e-05,\n",
       "                      -1.6090e-04,  5.7025e-05, -1.0522e-04, -1.1697e-04,  5.0903e-05,\n",
       "                      -6.5813e-05, -1.8366e-04, -6.3908e-06, -2.7353e-04,  3.1709e-04,\n",
       "                      -4.3124e-04, -5.0788e-04, -6.3033e-04,  2.1326e-04,  1.8285e-04,\n",
       "                       4.2535e-05,  4.6109e-04,  4.3241e-06, -3.1890e-04, -5.3692e-02,\n",
       "                      -1.3465e-02,  3.0029e-02, -2.2864e-02,  2.2607e-02, -1.1045e-02,\n",
       "                      -5.0964e-04, -2.1506e-02,  2.5545e-02,  1.8509e-02, -3.4068e-02,\n",
       "                      -5.3875e-02, -3.9529e-02, -4.1114e-02, -1.1681e-02, -1.3114e-02,\n",
       "                       2.3758e-02,  1.7229e-02, -1.0586e-03,  6.1144e-03,  2.7897e-02,\n",
       "                       2.8820e-02, -2.2118e-02,  3.1276e-02, -5.1536e-02,  1.3690e-02,\n",
       "                       8.3695e-02,  9.8274e-03, -9.1298e-03,  4.5108e-02, -3.0639e-03,\n",
       "                       8.5842e-03], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.out_proj.weight',\n",
       "              tensor([[ 0.1123,  0.0039, -0.0957,  ...,  0.1090, -0.0574, -0.0324],\n",
       "                      [ 0.0261, -0.0276, -0.0563,  ..., -0.0088,  0.0220,  0.1528],\n",
       "                      [ 0.0785, -0.0553, -0.1602,  ...,  0.0971,  0.0810,  0.0147],\n",
       "                      ...,\n",
       "                      [ 0.1340,  0.1973, -0.1702,  ..., -0.0747,  0.0400,  0.0347],\n",
       "                      [-0.0787,  0.0696, -0.0815,  ...,  0.0431, -0.0057,  0.0659],\n",
       "                      [-0.1208, -0.0170, -0.1111,  ...,  0.1867,  0.0217,  0.1092]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.out_proj.bias',\n",
       "              tensor([ 0.0134, -0.0124,  0.0205,  0.0145, -0.0978,  0.1054, -0.0212, -0.0010,\n",
       "                      -0.0135, -0.0081, -0.0192,  0.0381, -0.0047,  0.0087, -0.0217,  0.0120,\n",
       "                       0.0177,  0.1525, -0.0083,  0.0158, -0.0103,  0.0079, -0.0091, -0.0179,\n",
       "                       0.0008,  0.0026,  0.0203,  0.0144,  0.0097,  0.0394,  0.0222,  0.0168],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm1.weight',\n",
       "              tensor([0.7800, 0.9033, 0.7809, 0.7834, 0.8430, 0.8858, 0.8396, 0.8384, 0.7686,\n",
       "                      0.8101, 0.8003, 0.8596, 0.7840, 0.7595, 0.8295, 0.7286, 0.8336, 1.0169,\n",
       "                      0.8120, 0.8335, 0.8188, 0.8413, 0.8272, 0.8237, 0.7652, 0.8169, 0.7816,\n",
       "                      0.8403, 0.8017, 0.8566, 0.8088, 0.7776], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm1.bias',\n",
       "              tensor([ 0.0126, -0.0271,  0.0209,  0.0138, -0.0092,  0.0424, -0.0154,  0.0025,\n",
       "                      -0.0074, -0.0079, -0.0181,  0.0284, -0.0091,  0.0035, -0.0094,  0.0109,\n",
       "                       0.0076,  0.0877, -0.0006,  0.0174, -0.0073,  0.0199, -0.0336, -0.0112,\n",
       "                       0.0035,  0.0093,  0.0268,  0.0169,  0.0175,  0.0390,  0.0299,  0.0129],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.in_proj_weight',\n",
       "              tensor([[-0.1410, -0.0386, -0.0445,  ...,  0.1061, -0.0055, -0.1702],\n",
       "                      [ 0.0921,  0.1236,  0.0425,  ..., -0.0618, -0.0206,  0.0816],\n",
       "                      [ 0.0374, -0.0717, -0.0026,  ...,  0.0262, -0.1474,  0.1744],\n",
       "                      ...,\n",
       "                      [ 0.1345, -0.0406,  0.0038,  ...,  0.1949,  0.0069,  0.0856],\n",
       "                      [-0.2194, -0.1092, -0.0268,  ..., -0.1144, -0.1062, -0.0418],\n",
       "                      [-0.0124, -0.0453,  0.0381,  ..., -0.0430,  0.1170, -0.1531]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.in_proj_bias',\n",
       "              tensor([-4.2021e-02, -3.7537e-02, -2.3479e-02,  9.6580e-03, -3.6173e-02,\n",
       "                       2.7912e-02, -4.3817e-02,  8.5077e-02,  3.5880e-02, -1.6904e-02,\n",
       "                       8.9353e-02, -6.3359e-02,  1.0774e-01, -9.7786e-02, -6.2398e-02,\n",
       "                       8.2347e-03, -3.1909e-02, -6.0583e-03, -6.9724e-02,  1.5312e-02,\n",
       "                      -3.8167e-03, -1.5175e-02, -1.9111e-02,  5.8258e-03, -7.3892e-02,\n",
       "                      -3.7551e-02, -9.3588e-02,  2.3631e-02, -2.9928e-02, -1.2633e-01,\n",
       "                      -9.1331e-02, -2.0572e-02, -1.6951e-04,  2.5021e-05, -1.4494e-04,\n",
       "                       1.8690e-06, -2.4201e-04, -5.0336e-05,  1.6633e-04, -1.6632e-04,\n",
       "                       8.3561e-05, -6.9679e-05, -2.7179e-04, -3.7869e-05, -6.1157e-06,\n",
       "                      -2.3717e-06, -5.6099e-05,  1.4879e-04,  4.2233e-04,  1.0057e-04,\n",
       "                       2.0795e-05, -2.6262e-05, -9.8725e-05, -1.6735e-04, -1.8469e-05,\n",
       "                      -8.6025e-05, -6.0148e-04,  7.8636e-04,  8.7222e-04,  2.1077e-05,\n",
       "                       3.8157e-05,  1.3285e-03,  1.0204e-03, -2.4515e-04, -6.6413e-03,\n",
       "                       7.3426e-02,  1.7873e-02, -9.0974e-03, -1.5201e-02, -4.2252e-03,\n",
       "                       4.0120e-02,  1.5969e-02, -1.0349e-02,  2.1423e-02, -3.9850e-02,\n",
       "                       4.4517e-02,  3.6951e-03,  1.3340e-02, -1.8990e-02,  1.9958e-02,\n",
       "                      -2.0822e-02, -9.6458e-03, -1.2515e-02,  4.8718e-03,  2.0256e-03,\n",
       "                       4.3549e-02, -1.5992e-02,  2.9019e-03,  2.2977e-02, -2.6388e-02,\n",
       "                      -1.4305e-02, -2.4169e-02, -2.2922e-02, -2.0407e-03, -1.9860e-02,\n",
       "                      -4.6880e-02], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.out_proj.weight',\n",
       "              tensor([[ 0.1386,  0.0832, -0.1708,  ..., -0.0112, -0.1630, -0.1183],\n",
       "                      [ 0.0161,  0.1103, -0.1587,  ...,  0.0934,  0.0531, -0.0248],\n",
       "                      [-0.1018,  0.0878, -0.0738,  ...,  0.0311,  0.0715,  0.0842],\n",
       "                      ...,\n",
       "                      [ 0.0284,  0.1107, -0.0165,  ...,  0.1472,  0.0201, -0.1424],\n",
       "                      [-0.1368, -0.0495, -0.0949,  ..., -0.1282,  0.1487, -0.2646],\n",
       "                      [ 0.0938, -0.1324, -0.1712,  ..., -0.0039,  0.0089,  0.0773]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.out_proj.bias',\n",
       "              tensor([ 0.0163, -0.0025,  0.0235,  0.0159, -0.0026,  0.0568, -0.0160,  0.0092,\n",
       "                      -0.0066, -0.0128, -0.0220,  0.0379, -0.0041,  0.0076, -0.0113,  0.0062,\n",
       "                       0.0081,  0.0792, -0.0017,  0.0205, -0.0088,  0.0324, -0.0396, -0.0125,\n",
       "                       0.0297,  0.0046,  0.0044,  0.0090,  0.0120,  0.0172,  0.0219,  0.0098],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm2.weight',\n",
       "              tensor([0.7858, 0.7950, 0.7464, 0.8015, 0.7683, 0.7387, 0.8309, 0.7627, 0.7193,\n",
       "                      0.7701, 0.7809, 0.8459, 0.7525, 0.6484, 0.8083, 0.6943, 0.8158, 0.4945,\n",
       "                      0.7840, 0.8133, 0.7403, 0.8109, 0.7642, 0.8151, 0.6742, 0.7693, 0.7012,\n",
       "                      0.7744, 0.7952, 0.7533, 0.7923, 0.8054], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm2.bias',\n",
       "              tensor([ 0.0202, -0.0341,  0.0284,  0.0215,  0.0757, -0.0792, -0.0069,  0.0250,\n",
       "                      -0.0003, -0.0127, -0.0232,  0.0377, -0.0010,  0.0083,  0.0029,  0.0057,\n",
       "                      -0.0089, -0.2191,  0.0110,  0.0299, -0.0051,  0.0442, -0.0852, -0.0052,\n",
       "                       0.0492,  0.0516,  0.0203,  0.0128,  0.0205,  0.0218,  0.0295,  0.0007],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.0.weight',\n",
       "              tensor([[ 0.1942,  0.2160,  0.1148,  ...,  0.0137,  0.0289,  0.0358],\n",
       "                      [ 0.0188, -0.0980,  0.0354,  ..., -0.0228,  0.4109, -0.1035],\n",
       "                      [-0.0710,  0.1747,  0.1740,  ...,  0.0782, -0.0925,  0.1080],\n",
       "                      ...,\n",
       "                      [-0.0162, -0.0157,  0.1829,  ..., -0.0006,  0.2555, -0.0665],\n",
       "                      [ 0.0711,  0.0441,  0.0283,  ..., -0.0334,  0.0944,  0.0794],\n",
       "                      [-0.0986,  0.1661,  0.0513,  ..., -0.0824,  0.0121, -0.0574]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.0.bias',\n",
       "              tensor([-0.0240, -0.0292, -0.0263,  0.0306,  0.0498,  0.0252,  0.0610,  0.0037,\n",
       "                       0.0644, -0.1367, -0.0077, -0.1354, -0.0931,  0.0443,  0.1253, -0.0569,\n",
       "                      -0.1446, -0.0027,  0.1267, -0.1551, -0.1205, -0.0390, -0.0285, -0.0014,\n",
       "                       0.0334,  0.0069, -0.0385, -0.0859,  0.0188, -0.1648, -0.0221,  0.0062,\n",
       "                       0.0170, -0.0077, -0.0101, -0.0925, -0.1386,  0.0521, -0.1224,  0.0219,\n",
       "                      -0.0764, -0.1314, -0.1003,  0.0379,  0.0293,  0.1768, -0.0427, -0.2523,\n",
       "                      -0.1017,  0.1148, -0.1618, -0.0901, -0.0075,  0.0513,  0.0084,  0.0059,\n",
       "                      -0.1267, -0.1364, -0.0212,  0.0116, -0.0427, -0.1307,  0.0206,  0.0819],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.2.weight',\n",
       "              tensor([[-0.0442,  0.1170,  0.0550,  ..., -0.1747, -0.0362, -0.0230],\n",
       "                      [-0.0793,  0.0893, -0.1893,  ..., -0.0946, -0.0733,  0.0667],\n",
       "                      [-0.0721,  0.0736, -0.1248,  ..., -0.0706, -0.0309,  0.0725],\n",
       "                      ...,\n",
       "                      [-0.2553,  0.1058,  0.0040,  ..., -0.1037,  0.0523, -0.1094],\n",
       "                      [ 0.0513, -0.0059,  0.1422,  ...,  0.0536,  0.0377, -0.0824],\n",
       "                      [-0.1472,  0.0874, -0.1464,  ..., -0.1791, -0.0899, -0.0042]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.2.bias',\n",
       "              tensor([ 0.1009, -0.0170, -0.0263,  0.0153,  0.0952, -0.0446,  0.0222,  0.0534,\n",
       "                       0.0219,  0.1093, -0.1124, -0.0366,  0.0827, -0.0580, -0.0814, -0.0536,\n",
       "                      -0.0058,  0.0522,  0.0957, -0.0217, -0.1060, -0.0175,  0.0440, -0.0032,\n",
       "                       0.0145,  0.0693,  0.0889, -0.0015, -0.0317, -0.0619, -0.0443, -0.0717],\n",
       "                     device='cuda:0')),\n",
       "             ('output_feedforward.0.weight',\n",
       "              tensor([[ 0.0548,  0.0093,  0.0439,  0.0545, -0.0268,  0.0051, -0.0650,  0.0004,\n",
       "                       -0.0184,  0.0295, -0.0631, -0.0004,  0.0301, -0.0123, -0.0618,  0.0046,\n",
       "                        0.0428,  0.0113, -0.0318,  0.0563, -0.0281, -0.0197,  0.0114, -0.0934,\n",
       "                        0.0057,  0.0342, -0.0334, -0.0191, -0.0342,  0.0094, -0.0450,  0.0125]],\n",
       "                     device='cuda:0')),\n",
       "             ('output_feedforward.0.bias', tensor([0.0580], device='cuda:0'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(\n",
    "    optimizer=optimizer,\n",
    "    model=trans_encoder_decoder_tf_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    ot_steps=20,\n",
    "    report_interval=5,\n",
    "    n_epochs=200,\n",
    "    list_train_loss=train_loss,\n",
    "    list_val_loss=val_loss,\n",
    "    device=device,\n",
    "    eps=1e-8,\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa6546-4ebd-44d5-8316-21438df4d6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
