{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08fd6fa5",
   "metadata": {},
   "source": [
    "# A transformer with both encoder and decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e831d0a",
   "metadata": {},
   "source": [
    "### Intuitively, encoders \"understand and interpret\", while decoders \"write and describe\". In file Transformer_with_frozen_conv_1.ipynb, we opted for only using encoder: Our goal is timeseries regression (in the sense that we are looking to create one singular value as the target) and not timeseries prediction (In the sense that we are looking to create a new timeseries), so there was no need for us to implement a decoder to \"create a new timeseries in sequence\". However, it is a fact that our target value is the sum up a new timeseries, so, we now investigate in using autoregression with decoder to generate a new timeseries and then sum its time steps up. It is unclear if this will help at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1a1a7",
   "metadata": {},
   "source": [
    "## Import and preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f94c730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing, visualization\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);\n",
    "importlib.reload(visualization);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8353533",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Only run this cell if needed. AMD gpus might need this. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m load_dotenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../dotenv_env/deep_learning.env\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#Only run this cell if needed. AMD gpus might need this. \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../dotenv_env/deep_learning.env\")\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" # To, possibly fix memory leak issues. \n",
    "\n",
    "print(os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374d9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c78ae",
   "metadata": {},
   "source": [
    "## Data preparations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158aba2",
   "metadata": {},
   "source": [
    "### Load time id order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb76708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463a251",
   "metadata": {},
   "source": [
    "### Load timeseries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1924dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abd4d6",
   "metadata": {},
   "source": [
    "### Load target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d11c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")\n",
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)\n",
    "df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42e128",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abdd64d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machine2/Desktop/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:396: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy[\"sub_int_num\"]=np.nan\n",
      "/home/machine2/Desktop/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:396: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy[\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)\n",
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]\n",
    "\n",
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3e8e2",
   "metadata": {},
   "source": [
    "## The model (Only autoregression, no teacher forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26b507",
   "metadata": {},
   "source": [
    "### As mentioned, we will use autoregression with decoder to create a new timeseries. One thing to note is that we will NOT be implementing teacher forcing with masked decoder for now. Teacher forcing is a powerful tool, but the context is not the same: We do not have another timeseries to as ground truth to train toward, so we have \"nothing to hide\" with the masking. Instead of teacher forcing, we are using the loss calculated with the actual target (future RV) and the aoturegression created timeseries to train. This might not even be possible: I foresee memory explosion since, normally, autoregression is done with no_grad() context and can take a huge amount of memory (since grad will keep the computation graph, which is HUGE if we are doing autoregression). But we will see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7183172",
   "metadata": {},
   "source": [
    "### Create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1d23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d948c",
   "metadata": {},
   "source": [
    "### Create components needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7383b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1353: UserWarning: expandable_segments not supported on this platform (Triggered internally at /pytorch/c10/hip/HIPAllocatorConfig.h:30.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "ts_emb_dim=32\n",
    "n_diff=2\n",
    "ts_dim=n_diff+1\n",
    "\n",
    "pos_embedder=training.pos_emb_cross_attn(length=60,ts_dim=ts_dim,emb_dim=ts_emb_dim,dropout=0.2,num_heads=4,keep_mag=True).to(device=device)\n",
    "\n",
    "ts_encoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "ts_decoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "output_ff=nn.Sequential(\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=1)\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed9678",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76560c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder_decoder_model=training.encoder_decoder_autoregressionOnly(\n",
    "    pos_emb_model=pos_embedder,\n",
    "    output_feedforward=output_ff,\n",
    "    encoder_dropout=0.2,\n",
    "    decoder_dropout=0.2,\n",
    "    encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "    decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "    n_diff=n_diff,\n",
    "    encoder_layer_num=2,\n",
    "    decoder_layer_num=2,\n",
    "    input_scaler=10000,\n",
    "    ts_emb_dim=ts_emb_dim,\n",
    "    encoder_num_heads=4,\n",
    "    decoder_num_heads=4,\n",
    "    encoder_keep_mag=True,\n",
    "    decoder_keep_mag=True,\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff33d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(trans_encoder_decoder_model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15bc3951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "encoder_decoder_autoregression                               --\n",
       "├─frozen_diff_conv: 1-1                                      --\n",
       "│    └─Conv1d: 2-1                                           (2)\n",
       "├─pos_emb_cross_attn: 1-2                                    --\n",
       "│    └─Linear: 2-2                                           128\n",
       "│    └─Embedding: 2-3                                        1,920\n",
       "│    └─MultiheadAttention: 2-4                               3,168\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1             1,056\n",
       "│    └─LayerNorm: 2-5                                        64\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─ts_encoder: 2-6                                       --\n",
       "│    │    └─MultiheadAttention: 3-2                          4,224\n",
       "│    │    └─LayerNorm: 3-3                                   64\n",
       "│    │    └─ModuleList: 3-4                                  4,192\n",
       "│    │    └─LayerNorm: 3-5                                   64\n",
       "│    └─ts_encoder: 2-7                                       --\n",
       "│    │    └─MultiheadAttention: 3-6                          4,224\n",
       "│    │    └─LayerNorm: 3-7                                   64\n",
       "│    │    └─ModuleList: 3-8                                  4,192\n",
       "│    │    └─LayerNorm: 3-9                                   64\n",
       "├─ModuleList: 1-4                                            --\n",
       "│    └─ts_decoder: 2-8                                       --\n",
       "│    │    └─MultiheadAttention: 3-10                         4,224\n",
       "│    │    └─LayerNorm: 3-11                                  64\n",
       "│    │    └─MultiheadAttention: 3-12                         4,224\n",
       "│    │    └─LayerNorm: 3-13                                  64\n",
       "│    │    └─ModuleList: 3-14                                 4,192\n",
       "│    └─ts_decoder: 2-9                                       --\n",
       "│    │    └─MultiheadAttention: 3-15                         4,224\n",
       "│    │    └─LayerNorm: 3-16                                  64\n",
       "│    │    └─MultiheadAttention: 3-17                         4,224\n",
       "│    │    └─LayerNorm: 3-18                                  64\n",
       "│    │    └─ModuleList: 3-19                                 4,192\n",
       "├─Sequential: 1-5                                            --\n",
       "│    └─Linear: 2-10                                          33\n",
       "=====================================================================================\n",
       "Total params: 48,995\n",
       "Trainable params: 48,993\n",
       "Non-trainable params: 2\n",
       "====================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(trans_encoder_decoder_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67e7b1",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3366e38",
   "metadata": {},
   "source": [
    "### As of now, the model has issue with memory: it is using all the 12 GB of my AMD GPU. I need to look deeper in attempt to fix this. According to my reading, since grad context keeps all the computation graphs, and autoregression may have HUGE graph, this is kinda expected. The only way, for now, I can see to bypass this is doing teacher training and doing autoregression under no_grad() context. But we will see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff097ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "HIP out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 11.98 GiB of which 0 bytes is free. Of the allocated memory 10.77 GiB is allocated by PyTorch, and 886.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreg_training_loop_rmspe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_encoder_decoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mot_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_train_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_val_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:178\u001b[39m, in \u001b[36mreg_training_loop_rmspe\u001b[39m\u001b[34m(optimizer, model, train_loader, val_loader, device, ot_steps, recall_best, eps, list_train_loss, list_val_loss, report_interval, n_epochs, scaler, norm_train_target, train_target, scheduler)\u001b[39m\n\u001b[32m    174\u001b[39m     train_target_mean=train_loader.dataset.feat_norm_dict[train_target][\u001b[32m0\u001b[39m]\n\u001b[32m    175\u001b[39m     train_target_std=train_loader.dataset.feat_norm_dict[train_target][\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m pred=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m#Moved here\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_train_target:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:997\u001b[39m, in \u001b[36mencoder_decoder_autoregression.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    995\u001b[39m     next_steps=target_input\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_layers: \n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m         next_steps=\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    998\u001b[39m     target_input=torch.cat([target_input,next_steps[:,-\u001b[32m1\u001b[39m:,:]],dim=\u001b[32m1\u001b[39m)\n\u001b[32m    999\u001b[39m out=\u001b[38;5;28mself\u001b[39m.output_feedforward(target_input[:,\u001b[32m1\u001b[39m:,:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:855\u001b[39m, in \u001b[36mts_decoder.forward\u001b[39m\u001b[34m(self, ground_target, encoder_memory, ground_target_mask)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    852\u001b[39m             ground_target,\n\u001b[32m    853\u001b[39m             encoder_memory,\n\u001b[32m    854\u001b[39m             ground_target_mask=\u001b[38;5;28;01mNone\u001b[39;00m): \n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m     self_attn,_=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mground_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mground_target_mask\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keep_mag: \n\u001b[32m    857\u001b[39m         ground_target=(ground_target+self_attn+\u001b[38;5;28mself\u001b[39m.decoder_norm1(ground_target+self_attn))/\u001b[32m2\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/activation.py:1377\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1351\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1352\u001b[39m         query,\n\u001b[32m   1353\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1374\u001b[39m         is_causal=is_causal,\n\u001b[32m   1375\u001b[39m     )\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/functional.py:6468\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6464\u001b[39m     attn_output_weights = torch.baddbmm(\n\u001b[32m   6465\u001b[39m         attn_mask, q_scaled, k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m   6466\u001b[39m     )\n\u001b[32m   6467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6468\u001b[39m     attn_output_weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6469\u001b[39m attn_output_weights = softmax(attn_output_weights, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   6470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dropout_p > \u001b[32m0.0\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: HIP out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 11.98 GiB of which 0 bytes is free. Of the allocated memory 10.77 GiB is allocated by PyTorch, and 886.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(\n",
    "    optimizer=optimizer,\n",
    "    model=trans_encoder_decoder_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    ot_steps=20,\n",
    "    report_interval=5,\n",
    "    n_epochs=200,\n",
    "    list_train_loss=train_loss,\n",
    "    list_val_loss=val_loss,\n",
    "    device=device,\n",
    "    eps=1e-8,\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa253fa",
   "metadata": {},
   "source": [
    "## The Model (With only teacher forcing, using the current timeseries as ground_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3bc98",
   "metadata": {},
   "source": [
    "### The reason we went, first to the autoregression only method was that there was no ground target, but what if we just use the current sequence as the ground target in the decoder? Intuitively this is asking the cross attention layers: For the time steps in my query (the ground target), how can I adjust the query with the values (encoder output), based on the attention of keys (encoder output) on the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1e8b3",
   "metadata": {},
   "source": [
    "### A comment on masking\n",
    "\n",
    "The formula for masked attention is $Attention(Q,K,V,M)=softmax(QK^T/\\sqrt{d_Q}\\ + \\ M)\\ V$ (where $d_Q$ is the number of row of $Q$, e.g the length of the timeseries in our context, and $M$ stands for the mask). \n",
    "\n",
    "In our case, we are using the \"casual mask\": (example of dimension length of Query timeseries being 3) \n",
    "\n",
    "$$M:=\\begin{pmatrix}\n",
    "    0 & -\\inf & -\\inf \\\\ \n",
    "    0 & 0 & -\\inf \\\\ \n",
    "    0 & 0 & 0 \n",
    "\\end{pmatrix}$$\n",
    "\n",
    "For sake of understanding, consider following example in full: \n",
    "\n",
    "$$X:=\\begin{pmatrix}\n",
    "    1 & 0 \\\\ \n",
    "    0.5 & 1 \\\\ \n",
    "    0 & 1.5 \n",
    "\\end{pmatrix}$$\n",
    "\n",
    "For context, in above, the \"length of timeseries $X$\" is the number of row of $X$, i.e. 3. And the \"dimension of each time step of $X$\" is number of column of $X$ i.e. 2. \n",
    "We will use a pythonic notation $X[i]$ to indicate the $i^th$ row of $X$ for $0\\leq i\\leq 2$. \n",
    "\n",
    "We take $Q=K=V:=X$ (so we are doing a self attention), and have that (we are setting $\\sqrt{d_Q}$ to be 1, for simplicity): \n",
    "\n",
    "$$QK^T=\\begin{pmatrix}\n",
    "    1 & 0.5 & 0 \\\\ \n",
    "    0.5 & 1.25 & 1.5 \\\\ \n",
    "    0 & 1.5 & 2.25 \n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "And so \n",
    "\n",
    "$$QK^T+M=\\begin{pmatrix}\n",
    "    1 & -\\inf & -\\inf \\\\ \n",
    "    0.5 & 1.25 & -\\inf \\\\ \n",
    "    0 & 1.5 & 2.25 \n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then we apply the softmax on this matrix, recall definition of the soft max: \n",
    "\n",
    "$$softmax(x_{i}):=\\frac{e^{x_{i}}}{\\sum\\limits_{j\\in \\text{ index set}} e^{x_{j}}}$$ \n",
    "\n",
    "We should get something close to: \n",
    "\n",
    "$$Attn=\\begin{pmatrix}\n",
    "    1 & 0 & 0 \\\\ \n",
    "    0.32 & 0.68 & 0 \\\\ \n",
    "    0.06 & 0.21 & 0.73 \n",
    "\\end{pmatrix}\\ V = \n",
    "\\begin{pmatrix}\n",
    "    1\\ * \\ X[0] \\\\ \n",
    "    0.32\\ * \\ X[0]\\ + \\ 0.68\\ * \\ X[1] \\\\ \n",
    "    0.06\\ * \\ X[0]\\ + \\ 0.21\\ * \\ X[1]\\ + \\ 0.73\\ * \\ X[2]\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Intuitively, $Attn[i]$ is only determined by $X[j]$ for $0\\leq j\\leq i$ for all $i$. This makes it so that \"Attention with casual masking can not look into the future for current step\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f0437",
   "metadata": {},
   "source": [
    "### Create dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda4a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fadb6",
   "metadata": {},
   "source": [
    "### Create components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc14a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_emb_dim=32\n",
    "n_diff=2\n",
    "ts_dim=n_diff+1\n",
    "\n",
    "pos_embedder=training.pos_emb_cross_attn(length=60,ts_dim=ts_dim,emb_dim=ts_emb_dim,dropout=0.2,num_heads=4,keep_mag=True).to(device=device)\n",
    "\n",
    "ts_encoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "ts_decoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "output_ff=nn.Sequential(\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=1)\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba9378",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd6155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder_decoder_tf_model=training.encoder_decoder_teacherforcing(\n",
    "    pos_emb_model=pos_embedder,\n",
    "    output_feedforward=output_ff,\n",
    "    encoder_dropout=0.2,\n",
    "    decoder_dropout=0.2,\n",
    "    encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "    decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "    n_diff=n_diff,\n",
    "    encoder_layer_num=2,\n",
    "    decoder_layer_num=2,\n",
    "    input_scaler=10000,\n",
    "    ts_emb_dim=ts_emb_dim,\n",
    "    encoder_num_heads=4,\n",
    "    decoder_num_heads=4,\n",
    "    encoder_keep_mag=True,\n",
    "    decoder_keep_mag=True,\n",
    "    return_sum=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107bebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(trans_encoder_decoder_tf_model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a420c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "encoder_decoder_teacherforcing                               --\n",
       "├─frozen_diff_conv: 1-1                                      --\n",
       "│    └─Conv1d: 2-1                                           (2)\n",
       "├─pos_emb_cross_attn: 1-2                                    --\n",
       "│    └─Linear: 2-2                                           128\n",
       "│    └─Embedding: 2-3                                        1,920\n",
       "│    └─MultiheadAttention: 2-4                               3,168\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1             1,056\n",
       "│    └─LayerNorm: 2-5                                        64\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─ts_encoder: 2-6                                       --\n",
       "│    │    └─MultiheadAttention: 3-2                          4,224\n",
       "│    │    └─LayerNorm: 3-3                                   64\n",
       "│    │    └─ModuleList: 3-4                                  4,192\n",
       "│    │    └─LayerNorm: 3-5                                   64\n",
       "│    └─ts_encoder: 2-7                                       --\n",
       "│    │    └─MultiheadAttention: 3-6                          4,224\n",
       "│    │    └─LayerNorm: 3-7                                   64\n",
       "│    │    └─ModuleList: 3-8                                  4,192\n",
       "│    │    └─LayerNorm: 3-9                                   64\n",
       "├─ModuleList: 1-4                                            --\n",
       "│    └─ts_decoder: 2-8                                       --\n",
       "│    │    └─MultiheadAttention: 3-10                         4,224\n",
       "│    │    └─LayerNorm: 3-11                                  64\n",
       "│    │    └─MultiheadAttention: 3-12                         4,224\n",
       "│    │    └─LayerNorm: 3-13                                  64\n",
       "│    │    └─ModuleList: 3-14                                 4,192\n",
       "│    └─ts_decoder: 2-9                                       --\n",
       "│    │    └─MultiheadAttention: 3-15                         4,224\n",
       "│    │    └─LayerNorm: 3-16                                  64\n",
       "│    │    └─MultiheadAttention: 3-17                         4,224\n",
       "│    │    └─LayerNorm: 3-18                                  64\n",
       "│    │    └─ModuleList: 3-19                                 4,192\n",
       "├─Sequential: 1-5                                            --\n",
       "│    └─Linear: 2-10                                          33\n",
       "=====================================================================================\n",
       "Total params: 48,995\n",
       "Trainable params: 48,993\n",
       "Non-trainable params: 2\n",
       "====================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(trans_encoder_decoder_tf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e797de",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d2745e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new best validation loss at epoch  1  with validation loss of  tensor(0.2360, device='cuda:0') .\n",
      "At  24.368682146072388  epoch  1 has training loss  tensor(0.2746, device='cuda:0')  and validation loss  tensor(0.2360, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  2  with validation loss of  tensor(0.2356, device='cuda:0') .\n",
      "A new best validation loss at epoch  3  with validation loss of  tensor(0.2337, device='cuda:0') .\n",
      "A new best validation loss at epoch  4  with validation loss of  tensor(0.2316, device='cuda:0') .\n",
      "At  126.59550452232361  epoch  5 has training loss  tensor(0.2489, device='cuda:0')  and validation loss  tensor(0.2322, device='cuda:0') .\n",
      "\n",
      "At  254.07837462425232  epoch  10 has training loss  tensor(0.2474, device='cuda:0')  and validation loss  tensor(0.2334, device='cuda:0') .\n",
      "\n",
      "At epoch 10, learning rate has been updated from 0.001 to 0.0005, reloading previous best model weights from epoch 4 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  11  with validation loss of  tensor(0.2305, device='cuda:0') .\n",
      "At  381.963880777359  epoch  15 has training loss  tensor(0.2455, device='cuda:0')  and validation loss  tensor(0.2338, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  16  with validation loss of  tensor(0.2302, device='cuda:0') .\n",
      "At  510.6774184703827  epoch  20 has training loss  tensor(0.2447, device='cuda:0')  and validation loss  tensor(0.2303, device='cuda:0') .\n",
      "\n",
      "At epoch 22, learning rate has been updated from 0.0005 to 0.00025, reloading previous best model weights from epoch 16 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  23  with validation loss of  tensor(0.2300, device='cuda:0') .\n",
      "At  640.1796228885651  epoch  25 has training loss  tensor(0.2438, device='cuda:0')  and validation loss  tensor(0.2301, device='cuda:0') .\n",
      "\n",
      "At epoch 29, learning rate has been updated from 0.00025 to 0.000125, reloading previous best model weights from epoch 23 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  770.478236913681  epoch  30 has training loss  tensor(0.2426, device='cuda:0')  and validation loss  tensor(0.2304, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  31  with validation loss of  tensor(0.2300, device='cuda:0') .\n",
      "A new best validation loss at epoch  34  with validation loss of  tensor(0.2299, device='cuda:0') .\n",
      "At  900.7824077606201  epoch  35 has training loss  tensor(0.2425, device='cuda:0')  and validation loss  tensor(0.2303, device='cuda:0') .\n",
      "\n",
      "At  1030.697425365448  epoch  40 has training loss  tensor(0.2423, device='cuda:0')  and validation loss  tensor(0.2314, device='cuda:0') .\n",
      "\n",
      "At epoch 40, learning rate has been updated from 0.000125 to 6.25e-05, reloading previous best model weights from epoch 34 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  42  with validation loss of  tensor(0.2298, device='cuda:0') .\n",
      "At  1160.9671623706818  epoch  45 has training loss  tensor(0.2418, device='cuda:0')  and validation loss  tensor(0.2302, device='cuda:0') .\n",
      "\n",
      "At epoch 48, learning rate has been updated from 6.25e-05 to 3.125e-05, reloading previous best model weights from epoch 42 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  1291.4897248744965  epoch  50 has training loss  tensor(0.2415, device='cuda:0')  and validation loss  tensor(0.2300, device='cuda:0') .\n",
      "\n",
      "At epoch 54, learning rate has been updated from 3.125e-05 to 1.5625e-05, reloading previous best model weights from epoch 42 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  1422.023981332779  epoch  55 has training loss  tensor(0.2414, device='cuda:0')  and validation loss  tensor(0.2302, device='cuda:0') .\n",
      "\n",
      "At  1552.516384601593  epoch  60 has training loss  tensor(0.2414, device='cuda:0')  and validation loss  tensor(0.2305, device='cuda:0') .\n",
      "\n",
      "At epoch 60, learning rate has been updated from 1.5625e-05 to 7.8125e-06, reloading previous best model weights from epoch 42 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "The validation loss has not improved for  20  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 42  with validation loss:  tensor(0.2298, device='cuda:0') .\n",
      " The total number of epoch trained is  62 .\n",
      " Training completed in:  1604.637042760849 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('frozen_conv.frozen_conv.weight',\n",
       "              tensor([[[-1.,  1.]]], device='cuda:0')),\n",
       "             ('pos_emb.ts_proj.weight',\n",
       "              tensor([[ 0.5243, -0.2829,  0.1898],\n",
       "                      [ 0.1887,  0.1524, -0.3744],\n",
       "                      [-0.4206, -0.2532,  0.3629],\n",
       "                      [-0.0923,  0.0470,  0.2774],\n",
       "                      [-0.3300, -0.5331, -0.2996],\n",
       "                      [-0.0594, -0.2248, -0.1744],\n",
       "                      [-0.2332, -0.2448, -0.3765],\n",
       "                      [ 0.3739,  0.1150,  0.2523],\n",
       "                      [ 0.2041, -0.5227, -0.2917],\n",
       "                      [-0.3198,  0.3190,  0.1713],\n",
       "                      [-0.1335,  0.1406,  0.0728],\n",
       "                      [-0.3787,  0.2856, -0.1774],\n",
       "                      [ 0.3807, -0.4102, -0.2325],\n",
       "                      [ 0.1481, -0.0115, -0.1855],\n",
       "                      [ 0.0272,  0.3295,  0.0888],\n",
       "                      [-0.5111, -0.2753, -0.0785],\n",
       "                      [ 0.5766, -0.0617, -0.3039],\n",
       "                      [ 0.0300, -0.1933,  0.3012],\n",
       "                      [-0.2925, -0.1909,  0.1449],\n",
       "                      [ 0.1767, -0.2107, -0.2445],\n",
       "                      [ 0.3357, -0.1387, -0.0987],\n",
       "                      [ 0.1075, -0.1032,  0.1972],\n",
       "                      [-0.0604, -0.3734,  0.3556],\n",
       "                      [-0.5212,  0.1433,  0.1022],\n",
       "                      [ 0.2228, -0.6037, -0.4338],\n",
       "                      [-0.0549,  0.4311,  0.1873],\n",
       "                      [-0.0392, -0.1887, -0.2457],\n",
       "                      [-0.1709, -0.0382,  0.3300],\n",
       "                      [ 0.1392, -0.4524, -0.2727],\n",
       "                      [-0.4222,  0.2107, -0.3827],\n",
       "                      [ 0.2909, -0.3976, -0.2844],\n",
       "                      [ 0.1624,  0.1208, -0.0796]], device='cuda:0')),\n",
       "             ('pos_emb.ts_proj.bias',\n",
       "              tensor([-0.3019,  0.2836,  0.3164,  0.2308, -0.3342,  0.0526,  0.5367, -0.4126,\n",
       "                       0.1862, -0.0467,  0.5539,  0.2931,  0.3527, -0.3284, -0.2934,  0.0896,\n",
       "                       0.1727,  0.5797,  0.2616, -0.5172, -0.1208, -0.5043,  0.1644, -0.0633,\n",
       "                       0.2328, -0.3714, -0.1860,  0.4513, -0.2515,  0.2336, -0.1001,  0.1191],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_emb.weight',\n",
       "              tensor([[-0.2467,  0.4863,  1.6131,  ...,  1.1534,  0.2963, -0.8521],\n",
       "                      [-0.3376, -0.9835, -1.0627,  ...,  1.1016, -0.1834,  1.7877],\n",
       "                      [ 0.0472, -1.4857, -0.4454,  ..., -1.4425,  0.2244,  0.5991],\n",
       "                      ...,\n",
       "                      [ 0.9575,  0.3539,  0.5004,  ..., -0.1378,  0.2249,  0.4509],\n",
       "                      [ 0.0184,  1.2176,  0.3489,  ..., -0.7481,  0.4166, -1.2956],\n",
       "                      [-0.5357, -0.8191,  1.2668,  ..., -0.4813,  1.7434, -0.7242]],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.in_proj_weight',\n",
       "              tensor([[-0.1231, -0.0008,  0.0761,  ..., -0.1161,  0.0555,  0.1632],\n",
       "                      [-0.1503,  0.2324, -0.1166,  ...,  0.1914,  0.1203,  0.1693],\n",
       "                      [ 0.1713, -0.0553, -0.0579,  ..., -0.1844, -0.1792, -0.0780],\n",
       "                      ...,\n",
       "                      [-0.0116,  0.1481,  0.0026,  ..., -0.0613,  0.0562, -0.0893],\n",
       "                      [-0.2117, -0.0792, -0.1069,  ...,  0.2165, -0.0380,  0.0240],\n",
       "                      [-0.0772,  0.0710,  0.1214,  ...,  0.1416,  0.1012, -0.1734]],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.in_proj_bias',\n",
       "              tensor([ 3.0062e-03, -3.7004e-02,  3.5549e-02, -4.2981e-02, -7.8523e-02,\n",
       "                       7.8567e-02, -7.4644e-03,  3.3989e-02,  9.3950e-03, -8.3335e-03,\n",
       "                      -1.1059e-02, -3.1734e-02, -3.6551e-02, -1.5213e-02, -1.2508e-02,\n",
       "                       1.4380e-03,  9.1067e-03, -7.4100e-02, -5.7430e-02, -1.0535e-01,\n",
       "                      -2.7744e-02, -1.6745e-02, -2.5163e-02,  7.0414e-03, -5.2830e-02,\n",
       "                      -4.7440e-02,  1.1116e-02,  1.8214e-03,  4.2013e-03, -4.0678e-03,\n",
       "                      -3.3766e-02,  1.9769e-02, -1.5446e-04,  1.0149e-04,  1.6614e-04,\n",
       "                       1.8696e-04, -1.2649e-04, -1.9372e-04, -5.0371e-05, -1.5630e-05,\n",
       "                       9.1292e-04, -1.0931e-04,  2.1912e-04, -2.0570e-04,  1.3884e-03,\n",
       "                       1.2917e-03,  9.1767e-04,  1.3007e-03,  9.5151e-04,  2.6730e-04,\n",
       "                       3.2024e-05,  4.4008e-04,  1.7973e-05, -3.3391e-06, -6.2993e-04,\n",
       "                      -2.8540e-04, -3.7127e-04, -3.1858e-05, -1.2717e-03, -3.3200e-04,\n",
       "                      -1.0735e-03,  3.9330e-04, -2.5387e-04, -1.4299e-04,  1.3293e-02,\n",
       "                       6.1166e-03, -1.1777e-02,  9.7340e-03,  5.5767e-03,  2.0809e-02,\n",
       "                      -6.1126e-03,  6.3271e-03, -6.0017e-07, -2.5717e-03, -1.6241e-02,\n",
       "                       1.4782e-02, -4.9354e-03,  2.2137e-02, -4.8672e-03,  5.1387e-03,\n",
       "                      -2.4967e-02,  3.6847e-02,  1.7531e-02,  1.2584e-02,  8.6297e-03,\n",
       "                      -1.6730e-02, -2.0741e-02, -2.7764e-03,  2.8159e-03, -1.5240e-02,\n",
       "                      -1.7978e-02, -2.7458e-02,  2.6943e-02, -1.9226e-02, -1.3017e-02,\n",
       "                      -5.1705e-03], device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.out_proj.weight',\n",
       "              tensor([[ 0.0666, -0.0550,  0.0465,  ..., -0.0664, -0.0596,  0.0212],\n",
       "                      [-0.1272, -0.0075,  0.0988,  ...,  0.0927, -0.0546, -0.0932],\n",
       "                      [ 0.0809,  0.0052, -0.0947,  ..., -0.0144, -0.3120,  0.0570],\n",
       "                      ...,\n",
       "                      [ 0.2611,  0.0193, -0.1007,  ..., -0.0282, -0.1919, -0.0735],\n",
       "                      [ 0.0590,  0.0455,  0.0423,  ..., -0.0334, -0.0478, -0.0057],\n",
       "                      [ 0.0256, -0.0153,  0.1413,  ...,  0.0381,  0.1662, -0.0841]],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_attn.out_proj.bias',\n",
       "              tensor([-0.0552, -0.0572,  0.0276, -0.0219, -0.0131,  0.0287,  0.0417, -0.0462,\n",
       "                       0.0137,  0.0191,  0.0618,  0.0211, -0.0130, -0.0554,  0.0034,  0.0573,\n",
       "                      -0.0203,  0.1088,  0.0338, -0.0598, -0.0354, -0.0423,  0.0737,  0.0475,\n",
       "                       0.0119, -0.0572, -0.0141,  0.0454, -0.0143,  0.0341, -0.0240, -0.0203],\n",
       "                     device='cuda:0')),\n",
       "             ('pos_emb.pos_norm.weight',\n",
       "              tensor([0.7693, 0.7927, 0.7920, 0.8385, 0.8735, 0.7715, 0.8902, 0.7407, 0.8927,\n",
       "                      0.8388, 0.8758, 0.7107, 0.8781, 0.8537, 0.8097, 0.8425, 0.8733, 0.9320,\n",
       "                      0.8618, 0.8903, 0.8158, 0.8657, 0.8183, 0.8893, 0.8740, 0.8947, 0.8166,\n",
       "                      0.7940, 0.7864, 0.9630, 0.8256, 0.7720], device='cuda:0')),\n",
       "             ('pos_emb.pos_norm.bias',\n",
       "              tensor([-0.0560, -0.0629,  0.0400, -0.0467, -0.0284,  0.0489,  0.0376, -0.0672,\n",
       "                       0.0022,  0.0337,  0.0602,  0.0242,  0.0052, -0.0458,  0.0014,  0.0674,\n",
       "                      -0.0167,  0.0966,  0.0286, -0.0469, -0.0560, -0.0348,  0.0858,  0.0329,\n",
       "                       0.0082, -0.0644, -0.0150,  0.0418, -0.0177,  0.0701, -0.0582, -0.0249],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.in_proj_weight',\n",
       "              tensor([[-0.0857, -0.0577,  0.2444,  ..., -0.0296,  0.0919, -0.0608],\n",
       "                      [ 0.1100,  0.0844,  0.1361,  ..., -0.1715, -0.0415,  0.1161],\n",
       "                      [-0.0781, -0.1478,  0.1472,  ...,  0.0464, -0.0973,  0.1314],\n",
       "                      ...,\n",
       "                      [ 0.0859, -0.0762,  0.0158,  ...,  0.0495, -0.1533,  0.0390],\n",
       "                      [-0.1645, -0.2093,  0.1861,  ...,  0.1269, -0.1537,  0.0024],\n",
       "                      [-0.0320,  0.0125,  0.1054,  ...,  0.1544, -0.0901, -0.0615]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.in_proj_bias',\n",
       "              tensor([ 3.9076e-04,  6.5478e-02,  2.3768e-03, -2.3574e-03, -2.2096e-02,\n",
       "                       1.0212e-02,  4.6086e-02,  5.2526e-02,  3.2172e-02,  1.8615e-02,\n",
       "                       1.4165e-02,  3.0548e-02, -2.9795e-02,  1.3548e-02, -1.7994e-02,\n",
       "                       2.4305e-02, -4.0787e-03, -3.4270e-02,  6.6212e-02, -2.5628e-02,\n",
       "                       1.0302e-04,  5.8059e-02, -3.9465e-02, -6.9291e-02, -2.7621e-02,\n",
       "                      -3.1315e-02,  4.7886e-02, -1.3429e-02, -2.6409e-03,  1.7148e-02,\n",
       "                      -2.0842e-02, -2.5637e-02,  1.8355e-04,  5.4450e-04, -1.3355e-04,\n",
       "                      -1.3366e-04, -3.1109e-04, -1.1327e-05, -2.6298e-06, -8.2522e-05,\n",
       "                      -1.0431e-04,  4.0279e-04, -4.1191e-04, -1.1963e-04, -9.5492e-04,\n",
       "                       4.1849e-04,  1.7599e-05,  1.8665e-04, -5.6309e-06,  4.5584e-04,\n",
       "                      -7.3484e-04,  8.0313e-05,  2.3242e-04, -5.5557e-04, -1.2065e-05,\n",
       "                       1.2808e-03,  2.2437e-04, -5.6524e-05, -3.3464e-04, -3.7693e-05,\n",
       "                       7.5631e-05, -2.4822e-04, -1.0486e-04,  1.3916e-04, -1.3690e-02,\n",
       "                      -1.2279e-01, -6.0034e-02, -1.4106e-02, -6.5099e-02,  5.7555e-03,\n",
       "                       1.0870e-01, -2.1801e-02,  3.7518e-02, -7.4257e-02, -1.0403e-02,\n",
       "                       3.1751e-02,  6.3104e-03, -1.5674e-02,  6.2430e-02,  6.7906e-02,\n",
       "                       1.5055e-02,  3.3312e-02, -2.9195e-02, -6.8381e-04,  7.9454e-02,\n",
       "                       2.6565e-02,  1.5417e-02, -9.8908e-04,  5.7144e-02, -7.2398e-02,\n",
       "                      -2.9876e-02, -2.4360e-02,  3.4461e-02, -2.0167e-02,  1.4147e-02,\n",
       "                       4.0709e-02], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.out_proj.weight',\n",
       "              tensor([[ 0.0651, -0.0177,  0.1170,  ..., -0.1347,  0.0672, -0.0917],\n",
       "                      [-0.1027, -0.0586, -0.0736,  ...,  0.0920,  0.1047,  0.0722],\n",
       "                      [ 0.1887, -0.2206, -0.0701,  ...,  0.0298, -0.1325, -0.1706],\n",
       "                      ...,\n",
       "                      [ 0.0857,  0.2289, -0.0285,  ..., -0.0831, -0.1344, -0.0740],\n",
       "                      [ 0.0408, -0.1611, -0.0106,  ...,  0.0180,  0.0331,  0.1752],\n",
       "                      [ 0.0923,  0.2026,  0.1122,  ..., -0.0312, -0.0286,  0.1495]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.0942, -0.0235,  0.0471, -0.0182, -0.0847,  0.0534,  0.0551, -0.1091,\n",
       "                       0.0256,  0.0197,  0.1162,  0.1028,  0.0470, -0.0734, -0.0382,  0.0676,\n",
       "                      -0.0160,  0.2070,  0.0530, -0.0818, -0.0575, -0.0889,  0.1670,  0.0080,\n",
       "                       0.0378, -0.1402, -0.0632,  0.0700, -0.1032,  0.0793, -0.0565, -0.0092],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm1.weight',\n",
       "              tensor([0.6604, 0.8014, 0.7758, 0.8456, 0.9644, 0.8520, 0.8763, 0.7457, 0.8002,\n",
       "                      0.7460, 0.8491, 0.6014, 0.9212, 0.7463, 0.7834, 0.7895, 0.8172, 0.9053,\n",
       "                      0.7757, 0.8541, 0.7601, 0.8697, 0.8151, 0.8625, 0.8927, 0.8804, 0.8545,\n",
       "                      0.7591, 0.7835, 0.9905, 0.7937, 0.7866], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm1.bias',\n",
       "              tensor([-4.9046e-02, -7.1880e-02,  4.9692e-02, -3.4719e-02, -1.4125e-02,\n",
       "                       7.8056e-02,  2.4349e-02, -5.8112e-02,  1.4162e-03,  2.5291e-02,\n",
       "                       4.3003e-02, -5.2113e-04,  9.3189e-03, -2.6015e-02, -1.0475e-02,\n",
       "                       4.9107e-02, -1.2934e-02,  7.6449e-02,  3.0627e-02, -3.1213e-02,\n",
       "                      -4.4244e-02, -1.8864e-02,  6.7877e-02,  2.7901e-02,  1.2069e-02,\n",
       "                      -8.6616e-02, -9.5137e-03,  3.5201e-02, -6.9819e-03,  6.7429e-02,\n",
       "                      -2.8050e-02, -4.0924e-06], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.0.weight',\n",
       "              tensor([[-0.1589, -0.1213, -0.0090,  ..., -0.0449, -0.1178, -0.1426],\n",
       "                      [-0.1838,  0.1027, -0.0931,  ..., -0.0645, -0.1459,  0.0330],\n",
       "                      [ 0.0442,  0.0107,  0.1440,  ..., -0.1254,  0.0247, -0.1932],\n",
       "                      ...,\n",
       "                      [-0.1061, -0.1138,  0.0863,  ...,  0.0906, -0.0673,  0.0654],\n",
       "                      [-0.1469, -0.0694,  0.1819,  ...,  0.0644,  0.0149, -0.1423],\n",
       "                      [ 0.1299,  0.0315,  0.1107,  ..., -0.1147, -0.1634,  0.0757]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.0.bias',\n",
       "              tensor([ 0.0160, -0.0121, -0.0749,  0.0793, -0.1146, -0.0078,  0.0056, -0.0333,\n",
       "                      -0.1110,  0.1009, -0.0430, -0.2746, -0.1503, -0.0748,  0.0640,  0.0192,\n",
       "                      -0.1369,  0.0358, -0.1050, -0.2184,  0.0659, -0.0185, -0.1371, -0.1457,\n",
       "                      -0.0069, -0.2198, -0.1009, -0.0512, -0.0457, -0.1439, -0.0746, -0.0268,\n",
       "                      -0.1934, -0.1520, -0.2091, -0.1585, -0.0549, -0.0420, -0.1320, -0.2390,\n",
       "                      -0.0301,  0.0931,  0.0775, -0.0207, -0.0065, -0.2174, -0.1460, -0.0485,\n",
       "                      -0.1442, -0.1657, -0.0240, -0.0430, -0.2342,  0.0700, -0.0507,  0.0509,\n",
       "                      -0.0385, -0.1399, -0.0317,  0.0525, -0.1493,  0.0862, -0.1013,  0.0546],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.2.weight',\n",
       "              tensor([[ 0.0478, -0.0395,  0.0982,  ..., -0.0017,  0.0521,  0.0704],\n",
       "                      [-0.1075,  0.0329,  0.1203,  ...,  0.1909,  0.0191, -0.0319],\n",
       "                      [-0.1289, -0.0846, -0.0464,  ..., -0.2306,  0.0038, -0.0546],\n",
       "                      ...,\n",
       "                      [-0.0423, -0.0457,  0.0840,  ..., -0.0596, -0.0293,  0.0253],\n",
       "                      [ 0.0052,  0.0567,  0.0365,  ..., -0.1615, -0.0922,  0.0324],\n",
       "                      [ 0.1605,  0.0399, -0.0396,  ..., -0.0720, -0.0115,  0.0462]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_feedforward.2.bias',\n",
       "              tensor([-0.0752, -0.1530,  0.0583,  0.0539, -0.1190, -0.0074,  0.0425, -0.0292,\n",
       "                       0.0656,  0.0885,  0.0599,  0.0176, -0.0270, -0.0475,  0.0657,  0.0839,\n",
       "                      -0.0302,  0.1002,  0.0302,  0.0421,  0.0385,  0.0245,  0.1248,  0.0411,\n",
       "                       0.0366, -0.0619,  0.0668,  0.0126,  0.1039,  0.0240, -0.0693, -0.0479],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm2.weight',\n",
       "              tensor([0.7918, 0.8200, 0.8231, 0.8589, 0.9153, 0.9641, 0.8849, 0.8123, 0.7913,\n",
       "                      0.7977, 0.8659, 0.7756, 0.8668, 0.8850, 0.8286, 0.8491, 0.8480, 0.9214,\n",
       "                      0.8626, 0.8701, 0.8254, 0.8398, 0.8158, 0.8455, 0.8663, 0.7942, 0.8547,\n",
       "                      0.8153, 0.7844, 0.9576, 0.8112, 0.7858], device='cuda:0')),\n",
       "             ('encoder_layers.0.encoder_norm2.bias',\n",
       "              tensor([-0.0278, -0.0512,  0.0479, -0.0361, -0.0262,  0.0678,  0.0071, -0.0451,\n",
       "                      -0.0105,  0.0245,  0.0257, -0.0362, -0.0167,  0.0035,  0.0086,  0.0248,\n",
       "                       0.0037,  0.0551,  0.0007, -0.0127,  0.0075,  0.0015,  0.0529,  0.0035,\n",
       "                      -0.0071, -0.0611,  0.0016,  0.0204,  0.0512,  0.0251, -0.0217, -0.0095],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.in_proj_weight',\n",
       "              tensor([[-0.1309, -0.0782,  0.2747,  ...,  0.0404, -0.1002, -0.1484],\n",
       "                      [-0.0100, -0.1477, -0.0887,  ...,  0.1451,  0.0134, -0.1002],\n",
       "                      [ 0.2084, -0.0503,  0.0901,  ..., -0.1200,  0.1400,  0.1876],\n",
       "                      ...,\n",
       "                      [ 0.1778,  0.0042,  0.1776,  ..., -0.1682, -0.0737, -0.1035],\n",
       "                      [ 0.1365, -0.1255, -0.0725,  ..., -0.1669,  0.0344, -0.0691],\n",
       "                      [ 0.2710, -0.0317,  0.2617,  ...,  0.0948,  0.0770,  0.1847]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.in_proj_bias',\n",
       "              tensor([ 1.6352e-02, -1.5222e-02, -2.7435e-02,  1.6997e-02, -2.5280e-02,\n",
       "                      -3.6986e-02,  3.2610e-02,  2.9154e-02,  3.5756e-02,  5.0532e-02,\n",
       "                      -5.3228e-02,  1.0151e-02,  1.8914e-02,  2.8553e-02,  1.9177e-02,\n",
       "                       3.1675e-02,  1.0937e-02, -6.8878e-03,  5.3203e-02,  4.2689e-02,\n",
       "                       2.8012e-02, -1.6727e-02,  2.2797e-02,  3.7702e-03,  4.2133e-02,\n",
       "                      -9.2815e-03,  3.6703e-02,  6.6466e-02, -5.3733e-02, -3.9813e-02,\n",
       "                      -5.5667e-02, -7.3637e-03,  2.1179e-05,  5.6928e-05,  1.6662e-04,\n",
       "                      -4.1785e-07, -1.1188e-04, -6.6413e-05,  3.1282e-05,  2.7543e-05,\n",
       "                       1.5508e-05, -4.8210e-05, -5.7735e-05, -8.4760e-05, -1.0240e-04,\n",
       "                      -1.6498e-04, -1.6776e-05,  4.4722e-05,  1.0666e-04,  2.8041e-05,\n",
       "                       2.6683e-05, -1.2865e-04, -5.2027e-05,  2.8099e-06,  5.7005e-05,\n",
       "                      -7.0133e-05, -2.3509e-05,  1.5154e-04,  4.1493e-05,  3.5384e-05,\n",
       "                      -2.8327e-05,  1.1512e-04,  5.9010e-04,  7.5139e-05,  2.9953e-02,\n",
       "                      -7.6004e-02, -5.3340e-04,  8.1826e-03,  4.6425e-03, -1.9377e-03,\n",
       "                      -1.0973e-02, -5.8123e-03,  1.7487e-02,  2.5306e-02, -2.6116e-02,\n",
       "                      -1.0982e-02,  7.4072e-03, -1.6609e-02,  6.7311e-03, -6.0299e-03,\n",
       "                       3.2332e-03,  1.4636e-02, -9.7991e-02, -6.1032e-02,  1.2227e-02,\n",
       "                      -2.3079e-02, -2.6073e-02,  2.8804e-02, -2.2059e-02, -2.7460e-02,\n",
       "                       1.8705e-02, -3.6062e-02,  8.2624e-02, -9.2484e-03, -7.3978e-02,\n",
       "                       5.0702e-02], device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.out_proj.weight',\n",
       "              tensor([[-0.0746,  0.0143,  0.0727,  ...,  0.0678, -0.0746, -0.0685],\n",
       "                      [-0.1691, -0.0141,  0.1317,  ...,  0.0564,  0.0252, -0.0834],\n",
       "                      [ 0.0112, -0.0076, -0.0253,  ...,  0.1491,  0.1478,  0.0110],\n",
       "                      ...,\n",
       "                      [ 0.0847,  0.1614, -0.2720,  ...,  0.1126, -0.0328,  0.0933],\n",
       "                      [ 0.0220, -0.0029,  0.0393,  ...,  0.1534,  0.0789, -0.0141],\n",
       "                      [ 0.0677, -0.1303,  0.0839,  ...,  0.1613, -0.0841, -0.0869]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_attn.out_proj.bias',\n",
       "              tensor([-0.0453, -0.0567,  0.0284, -0.0699, -0.0931,  0.0152,  0.0155, -0.0756,\n",
       "                       0.0037,  0.0032,  0.0693, -0.0547,  0.0445, -0.0111,  0.0029,  0.0389,\n",
       "                       0.0143,  0.1099, -0.0461, -0.0326,  0.0038, -0.0164,  0.1093, -0.0220,\n",
       "                      -0.0183, -0.1753, -0.0220,  0.0380, -0.0050,  0.0199, -0.0399, -0.0022],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm1.weight',\n",
       "              tensor([0.7647, 0.8394, 0.8162, 0.8727, 0.8959, 0.9347, 0.8480, 0.7658, 0.8339,\n",
       "                      0.7980, 0.8267, 0.7924, 0.8486, 0.9028, 0.7928, 0.8477, 0.8367, 0.8828,\n",
       "                      0.8891, 0.8565, 0.8755, 0.8321, 0.8018, 0.8236, 0.8917, 0.9603, 0.8628,\n",
       "                      0.7803, 0.8560, 0.9903, 0.8139, 0.8104], device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm1.bias',\n",
       "              tensor([-0.0074, -0.0463,  0.0337, -0.0468, -0.0074,  0.0321, -0.0034, -0.0240,\n",
       "                      -0.0039,  0.0174,  0.0075, -0.0410, -0.0269, -0.0004,  0.0192,  0.0079,\n",
       "                       0.0048,  0.0313, -0.0181, -0.0007,  0.0097,  0.0120,  0.0293,  0.0169,\n",
       "                      -0.0240, -0.0408,  0.0007,  0.0152,  0.0284,  0.0115, -0.0083, -0.0008],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.0.weight',\n",
       "              tensor([[-0.1589, -0.1213, -0.0090,  ..., -0.0449, -0.1178, -0.1426],\n",
       "                      [-0.1838,  0.1027, -0.0931,  ..., -0.0645, -0.1459,  0.0330],\n",
       "                      [ 0.0442,  0.0107,  0.1440,  ..., -0.1254,  0.0247, -0.1932],\n",
       "                      ...,\n",
       "                      [-0.1061, -0.1138,  0.0863,  ...,  0.0906, -0.0673,  0.0654],\n",
       "                      [-0.1469, -0.0694,  0.1819,  ...,  0.0644,  0.0149, -0.1423],\n",
       "                      [ 0.1299,  0.0315,  0.1107,  ..., -0.1147, -0.1634,  0.0757]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.0.bias',\n",
       "              tensor([ 0.0160, -0.0121, -0.0749,  0.0793, -0.1146, -0.0078,  0.0056, -0.0333,\n",
       "                      -0.1110,  0.1009, -0.0430, -0.2746, -0.1503, -0.0748,  0.0640,  0.0192,\n",
       "                      -0.1369,  0.0358, -0.1050, -0.2184,  0.0659, -0.0185, -0.1371, -0.1457,\n",
       "                      -0.0069, -0.2198, -0.1009, -0.0512, -0.0457, -0.1439, -0.0746, -0.0268,\n",
       "                      -0.1934, -0.1520, -0.2091, -0.1585, -0.0549, -0.0420, -0.1320, -0.2390,\n",
       "                      -0.0301,  0.0931,  0.0775, -0.0207, -0.0065, -0.2174, -0.1460, -0.0485,\n",
       "                      -0.1442, -0.1657, -0.0240, -0.0430, -0.2342,  0.0700, -0.0507,  0.0509,\n",
       "                      -0.0385, -0.1399, -0.0317,  0.0525, -0.1493,  0.0862, -0.1013,  0.0546],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.2.weight',\n",
       "              tensor([[ 0.0478, -0.0395,  0.0982,  ..., -0.0017,  0.0521,  0.0704],\n",
       "                      [-0.1075,  0.0329,  0.1203,  ...,  0.1909,  0.0191, -0.0319],\n",
       "                      [-0.1289, -0.0846, -0.0464,  ..., -0.2306,  0.0038, -0.0546],\n",
       "                      ...,\n",
       "                      [-0.0423, -0.0457,  0.0840,  ..., -0.0596, -0.0293,  0.0253],\n",
       "                      [ 0.0052,  0.0567,  0.0365,  ..., -0.1615, -0.0922,  0.0324],\n",
       "                      [ 0.1605,  0.0399, -0.0396,  ..., -0.0720, -0.0115,  0.0462]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_feedforward.2.bias',\n",
       "              tensor([-0.0752, -0.1530,  0.0583,  0.0539, -0.1190, -0.0074,  0.0425, -0.0292,\n",
       "                       0.0656,  0.0885,  0.0599,  0.0176, -0.0270, -0.0475,  0.0657,  0.0839,\n",
       "                      -0.0302,  0.1002,  0.0302,  0.0421,  0.0385,  0.0245,  0.1248,  0.0411,\n",
       "                       0.0366, -0.0619,  0.0668,  0.0126,  0.1039,  0.0240, -0.0693, -0.0479],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm2.weight',\n",
       "              tensor([0.8032, 0.8424, 0.8254, 0.8753, 0.8245, 0.8930, 0.8594, 0.8519, 0.8132,\n",
       "                      0.8030, 0.8025, 0.7930, 0.8285, 0.9241, 0.8162, 0.8407, 0.8096, 0.8757,\n",
       "                      0.7524, 0.8537, 0.8471, 0.8239, 0.7947, 0.7996, 0.9242, 0.9004, 0.8748,\n",
       "                      0.8316, 0.9003, 0.9145, 0.8242, 0.8100], device='cuda:0')),\n",
       "             ('encoder_layers.1.encoder_norm2.bias',\n",
       "              tensor([ 0.0010, -0.0526,  0.0571, -0.0232,  0.0474,  0.0106, -0.0072, -0.0066,\n",
       "                      -0.0437,  0.0263, -0.0182, -0.0583, -0.0641,  0.0028,  0.0140,  0.0211,\n",
       "                      -0.0240,  0.0101,  0.0233,  0.0126,  0.0165,  0.0233,  0.0036,  0.0332,\n",
       "                      -0.0172,  0.0247, -0.0021,  0.0063,  0.0480, -0.0225, -0.0081, -0.0303],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.in_proj_weight',\n",
       "              tensor([[ 1.8134e-01,  1.1529e-01,  1.1530e-01,  ..., -1.9448e-01,\n",
       "                        1.8422e-01,  2.3894e-01],\n",
       "                      [-1.2397e-01,  4.5207e-02, -8.1844e-02,  ...,  6.5512e-02,\n",
       "                       -7.1931e-02,  1.3955e-01],\n",
       "                      [ 1.9323e-01, -8.0288e-02,  1.1686e-04,  ..., -2.4550e-01,\n",
       "                        2.6280e-02, -1.1538e-01],\n",
       "                      ...,\n",
       "                      [-1.8157e-01, -1.1446e-01,  1.1977e-01,  ...,  1.4490e-01,\n",
       "                        1.8285e-01, -7.7089e-02],\n",
       "                      [-1.8180e-01, -9.3029e-02, -1.8051e-01,  ...,  2.8113e-01,\n",
       "                        1.6048e-02,  6.2523e-02],\n",
       "                      [ 3.2973e-03, -2.3543e-01, -1.7784e-02,  ...,  9.9511e-02,\n",
       "                       -2.1956e-01,  8.9868e-02]], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.in_proj_bias',\n",
       "              tensor([ 6.1420e-03, -1.7289e-02, -7.6377e-02, -1.2644e-02, -1.0588e-01,\n",
       "                       9.3235e-03, -7.4502e-02,  9.8430e-02,  2.0601e-02, -6.0012e-02,\n",
       "                       9.3253e-04,  3.6723e-02,  9.3423e-03,  5.8035e-02,  3.8202e-02,\n",
       "                       8.0863e-02, -8.3526e-02, -5.7854e-02,  8.2236e-03, -1.2203e-02,\n",
       "                       5.7406e-02, -1.2832e-01, -1.9755e-02, -3.5635e-02, -4.7777e-03,\n",
       "                       6.0975e-02,  2.7015e-02, -2.5294e-02, -5.3012e-02,  4.2629e-02,\n",
       "                       3.8385e-02, -4.2203e-03, -1.0913e-04, -1.3810e-04, -1.8360e-04,\n",
       "                      -1.2602e-04,  1.1477e-04, -3.6429e-04, -5.2885e-05, -7.2403e-06,\n",
       "                      -3.7917e-04, -1.4843e-04,  1.1822e-04, -2.4478e-04,  1.4965e-04,\n",
       "                      -1.2054e-04, -1.2539e-04, -3.0499e-04, -9.1501e-05, -5.7974e-07,\n",
       "                       4.1447e-05,  2.4171e-05, -7.8052e-05,  3.2743e-05,  1.3228e-04,\n",
       "                       7.6917e-05,  1.7183e-05, -5.2526e-05, -1.6237e-05,  9.1919e-05,\n",
       "                       4.5604e-04, -2.1632e-04, -1.3991e-04,  8.5175e-05,  3.1673e-02,\n",
       "                       6.4214e-02, -7.4086e-03, -4.9691e-02, -3.2877e-02, -7.7823e-03,\n",
       "                       1.6464e-02,  1.3988e-02, -1.4018e-02,  1.5923e-02, -5.0684e-03,\n",
       "                      -1.2213e-02, -2.8537e-03,  1.7284e-02, -4.2853e-03, -2.8184e-02,\n",
       "                      -1.0831e-02, -1.1075e-02, -2.6034e-03,  1.9378e-02,  1.1214e-02,\n",
       "                      -3.2835e-02,  1.8047e-02,  4.6012e-02, -2.7333e-02, -2.6944e-02,\n",
       "                       3.1693e-02,  3.1871e-02, -4.7174e-02, -1.9880e-02,  9.5333e-03,\n",
       "                      -3.4937e-02], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.out_proj.weight',\n",
       "              tensor([[ 0.0924, -0.0999, -0.1071,  ...,  0.1480,  0.1075, -0.1351],\n",
       "                      [-0.0448, -0.1310,  0.1592,  ..., -0.0122,  0.0482, -0.0114],\n",
       "                      [ 0.0011, -0.2178,  0.0610,  ..., -0.0490,  0.1339, -0.0833],\n",
       "                      ...,\n",
       "                      [ 0.1436, -0.1651,  0.0761,  ..., -0.2278, -0.1342, -0.0785],\n",
       "                      [-0.1431, -0.0651, -0.0119,  ..., -0.0887, -0.1100, -0.1471],\n",
       "                      [-0.0344, -0.0038, -0.2553,  ...,  0.1265,  0.1233, -0.0511]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_self_attn.out_proj.bias',\n",
       "              tensor([ 0.0045, -0.0385,  0.0053, -0.0042, -0.0728,  0.0659, -0.0084, -0.0477,\n",
       "                       0.0060,  0.0070,  0.0215, -0.0122, -0.0226,  0.0143, -0.0344,  0.0436,\n",
       "                       0.0154,  0.0979, -0.0357,  0.0032,  0.0101,  0.0195,  0.0570, -0.0115,\n",
       "                      -0.0187, -0.1233, -0.0053,  0.0176,  0.0341,  0.0029, -0.0207,  0.0092],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm1.weight',\n",
       "              tensor([0.8211, 0.8973, 0.7896, 0.7794, 0.8774, 0.9313, 0.8504, 0.9018, 0.8261,\n",
       "                      0.8175, 0.8701, 0.8541, 0.8758, 0.8453, 0.8433, 0.8985, 0.8512, 1.0096,\n",
       "                      0.8809, 0.8602, 0.8017, 0.8824, 0.8503, 0.8520, 0.8556, 0.9213, 0.8285,\n",
       "                      0.8457, 0.8926, 0.9181, 0.8373, 0.8491], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm1.bias',\n",
       "              tensor([ 0.0102, -0.0386,  0.0019, -0.0010, -0.0407,  0.0426, -0.0135, -0.0322,\n",
       "                       0.0030,  0.0058,  0.0101, -0.0128, -0.0493,  0.0170, -0.0288,  0.0322,\n",
       "                       0.0158,  0.0830, -0.0332,  0.0088,  0.0059,  0.0251,  0.0374, -0.0095,\n",
       "                      -0.0182, -0.0918,  0.0016,  0.0120,  0.0365,  0.0042, -0.0106,  0.0050],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.in_proj_weight',\n",
       "              tensor([[ 0.0488, -0.1115, -0.0246,  ..., -0.0358, -0.2227,  0.0747],\n",
       "                      [-0.2452,  0.1517, -0.0856,  ...,  0.0814,  0.0825, -0.3072],\n",
       "                      [ 0.1135,  0.0006,  0.0984,  ...,  0.1188,  0.0078, -0.0894],\n",
       "                      ...,\n",
       "                      [ 0.0088,  0.1769,  0.1388,  ...,  0.0853, -0.2462, -0.0212],\n",
       "                      [-0.1051, -0.0240,  0.1521,  ...,  0.1335, -0.0853, -0.1226],\n",
       "                      [ 0.0028,  0.1433,  0.1654,  ..., -0.1406,  0.1010, -0.0821]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.in_proj_bias',\n",
       "              tensor([ 5.7941e-02, -1.3652e-02, -3.3256e-02, -4.0252e-02,  7.8844e-03,\n",
       "                      -3.7990e-03,  5.6426e-03,  3.8241e-02, -2.5984e-02,  9.8772e-02,\n",
       "                      -5.0112e-02,  4.5502e-02,  3.3300e-03, -6.6569e-02,  4.1834e-02,\n",
       "                       4.4098e-02,  8.2156e-03,  7.1663e-02, -2.2703e-02, -1.7542e-02,\n",
       "                      -1.7442e-02, -2.1639e-02, -1.7377e-02, -4.0567e-03, -1.0612e-01,\n",
       "                       4.1080e-02,  2.3033e-02, -2.3860e-02, -4.7868e-02,  4.6102e-02,\n",
       "                      -6.1604e-02, -2.1300e-02, -2.7767e-05,  1.8395e-04, -2.1742e-04,\n",
       "                       7.4896e-05, -1.1088e-04,  1.6940e-04, -2.9863e-04,  4.5314e-05,\n",
       "                      -2.3826e-04,  3.3249e-04, -1.6046e-05,  4.6238e-04,  8.4529e-05,\n",
       "                      -3.7732e-05, -1.5074e-04,  1.0992e-05,  5.3538e-05, -6.5188e-05,\n",
       "                       1.1304e-04,  3.6310e-05,  9.8507e-06, -1.5983e-05,  3.2806e-04,\n",
       "                       1.4645e-04, -1.3902e-04,  2.1011e-04, -1.2367e-05, -2.0412e-04,\n",
       "                      -1.0703e-05, -2.7744e-05, -4.1492e-04,  3.4666e-05,  2.0167e-02,\n",
       "                       1.9294e-02,  1.0746e-02,  1.8161e-02, -2.1538e-02, -3.6563e-02,\n",
       "                      -1.1729e-02,  1.5824e-02,  2.3498e-02,  7.5295e-02,  1.0953e-02,\n",
       "                      -1.5679e-02,  3.1759e-02, -6.5657e-02, -1.7860e-02, -3.2424e-04,\n",
       "                       4.8822e-02,  2.1925e-02, -3.4427e-02,  5.6341e-03, -8.9421e-03,\n",
       "                      -1.0214e-03,  9.3976e-03, -5.7532e-02,  2.3710e-02, -2.3136e-02,\n",
       "                      -6.7989e-03,  1.8121e-02,  9.6699e-03,  4.1473e-02, -1.3228e-02,\n",
       "                      -3.1809e-02], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.out_proj.weight',\n",
       "              tensor([[ 9.7375e-02, -3.8083e-02,  1.5720e-01,  ..., -8.2844e-02,\n",
       "                       -1.4692e-01, -1.7345e-02],\n",
       "                      [ 1.8782e-01,  7.5479e-03,  5.6468e-02,  ...,  8.3096e-02,\n",
       "                       -2.1559e-01, -5.9459e-02],\n",
       "                      [ 2.9704e-02, -1.3618e-01, -9.8012e-02,  ..., -4.3336e-02,\n",
       "                        7.1078e-02,  1.1997e-02],\n",
       "                      ...,\n",
       "                      [ 9.6225e-02, -5.2840e-02, -4.5249e-02,  ...,  1.1491e-01,\n",
       "                       -1.3701e-01,  5.0999e-02],\n",
       "                      [ 8.7491e-02, -7.4306e-03, -6.9883e-03,  ..., -1.6978e-02,\n",
       "                        1.1115e-04,  8.3357e-02],\n",
       "                      [ 4.3101e-02,  8.1062e-02,  5.1532e-02,  ...,  3.9650e-02,\n",
       "                       -6.8143e-02,  1.2807e-01]], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_cross_attn.out_proj.bias',\n",
       "              tensor([ 0.0098, -0.0566,  0.0046,  0.0040, -0.0452,  0.0590, -0.0154, -0.0439,\n",
       "                       0.0002,  0.0167,  0.0097, -0.0003, -0.0684,  0.0183, -0.0306,  0.0506,\n",
       "                       0.0153,  0.0807, -0.0367,  0.0091,  0.0052,  0.0293,  0.0288, -0.0091,\n",
       "                      -0.0028, -0.0803,  0.0066,  0.0082,  0.0250,  0.0045, -0.0088, -0.0097],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm2.weight',\n",
       "              tensor([0.8111, 0.8671, 0.7535, 0.7746, 0.8332, 0.8561, 0.8386, 0.9195, 0.8105,\n",
       "                      0.7925, 0.8517, 0.8660, 0.8287, 0.7109, 0.8336, 0.9016, 0.8272, 0.9370,\n",
       "                      0.8730, 0.8424, 0.8161, 0.8617, 0.8055, 0.8409, 0.8037, 0.9327, 0.8052,\n",
       "                      0.8557, 0.8699, 0.8638, 0.8770, 0.8170], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_norm2.bias',\n",
       "              tensor([ 1.5379e-02, -5.6710e-02,  1.0069e-03,  7.3185e-03, -1.5632e-05,\n",
       "                       2.2147e-02, -1.6725e-02, -3.0625e-02, -3.6631e-03,  1.6883e-02,\n",
       "                      -6.4247e-04, -4.6991e-03, -1.2006e-01,  2.1036e-02, -1.8666e-02,\n",
       "                       4.6454e-02,  1.2158e-02,  5.1187e-02, -2.8638e-02,  1.5525e-02,\n",
       "                       5.1970e-04,  3.5090e-02, -6.5143e-03, -4.6877e-03, -5.7174e-03,\n",
       "                      -5.6363e-02,  1.2710e-02,  1.0357e-02,  3.7263e-02,  5.5294e-03,\n",
       "                       8.6171e-04, -1.4512e-02], device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.0.weight',\n",
       "              tensor([[ 0.1942,  0.2160,  0.1148,  ...,  0.0137,  0.0289,  0.0358],\n",
       "                      [ 0.0188, -0.0980,  0.0354,  ..., -0.0228,  0.4109, -0.1035],\n",
       "                      [-0.0710,  0.1747,  0.1740,  ...,  0.0782, -0.0925,  0.1080],\n",
       "                      ...,\n",
       "                      [-0.0162, -0.0157,  0.1829,  ..., -0.0006,  0.2555, -0.0665],\n",
       "                      [ 0.0711,  0.0441,  0.0283,  ..., -0.0334,  0.0944,  0.0794],\n",
       "                      [-0.0986,  0.1661,  0.0513,  ..., -0.0824,  0.0121, -0.0574]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.0.bias',\n",
       "              tensor([-0.0240, -0.0292, -0.0263,  0.0306,  0.0498,  0.0252,  0.0610,  0.0037,\n",
       "                       0.0644, -0.1367, -0.0077, -0.1354, -0.0931,  0.0443,  0.1253, -0.0569,\n",
       "                      -0.1446, -0.0027,  0.1267, -0.1551, -0.1205, -0.0390, -0.0285, -0.0014,\n",
       "                       0.0334,  0.0069, -0.0385, -0.0859,  0.0188, -0.1648, -0.0221,  0.0062,\n",
       "                       0.0170, -0.0077, -0.0101, -0.0925, -0.1386,  0.0521, -0.1224,  0.0219,\n",
       "                      -0.0764, -0.1314, -0.1003,  0.0379,  0.0293,  0.1768, -0.0427, -0.2523,\n",
       "                      -0.1017,  0.1148, -0.1618, -0.0901, -0.0075,  0.0513,  0.0084,  0.0059,\n",
       "                      -0.1267, -0.1364, -0.0212,  0.0116, -0.0427, -0.1307,  0.0206,  0.0819],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.2.weight',\n",
       "              tensor([[-0.0442,  0.1170,  0.0550,  ..., -0.1747, -0.0362, -0.0230],\n",
       "                      [-0.0793,  0.0893, -0.1893,  ..., -0.0946, -0.0733,  0.0667],\n",
       "                      [-0.0721,  0.0736, -0.1248,  ..., -0.0706, -0.0309,  0.0725],\n",
       "                      ...,\n",
       "                      [-0.2553,  0.1058,  0.0040,  ..., -0.1037,  0.0523, -0.1094],\n",
       "                      [ 0.0513, -0.0059,  0.1422,  ...,  0.0536,  0.0377, -0.0824],\n",
       "                      [-0.1472,  0.0874, -0.1464,  ..., -0.1791, -0.0899, -0.0042]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.0.decoder_feedforward.2.bias',\n",
       "              tensor([ 0.1009, -0.0170, -0.0263,  0.0153,  0.0952, -0.0446,  0.0222,  0.0534,\n",
       "                       0.0219,  0.1093, -0.1124, -0.0366,  0.0827, -0.0580, -0.0814, -0.0536,\n",
       "                      -0.0058,  0.0522,  0.0957, -0.0217, -0.1060, -0.0175,  0.0440, -0.0032,\n",
       "                       0.0145,  0.0693,  0.0889, -0.0015, -0.0317, -0.0619, -0.0443, -0.0717],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.in_proj_weight',\n",
       "              tensor([[ 0.1032, -0.0764, -0.0338,  ..., -0.3462, -0.0759,  0.0588],\n",
       "                      [-0.1803, -0.0366,  0.0587,  ..., -0.0335, -0.2702, -0.1835],\n",
       "                      [-0.1095, -0.0871, -0.1022,  ..., -0.0088, -0.1337, -0.0631],\n",
       "                      ...,\n",
       "                      [-0.1372,  0.1262, -0.2504,  ...,  0.1562, -0.0932, -0.0644],\n",
       "                      [-0.1021, -0.0507,  0.0533,  ...,  0.0390,  0.0706,  0.0569],\n",
       "                      [ 0.0489,  0.1828,  0.1174,  ..., -0.0605,  0.1025, -0.0393]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.in_proj_bias',\n",
       "              tensor([ 3.8762e-02, -6.9956e-03, -5.4446e-02, -3.9858e-02,  3.8683e-02,\n",
       "                      -5.5651e-02, -8.4083e-02,  5.8710e-02,  7.7136e-02, -1.3328e-02,\n",
       "                       2.3679e-02,  5.9608e-02, -3.0713e-03, -5.7874e-02,  4.9824e-02,\n",
       "                      -5.4228e-03, -1.7332e-03, -2.3426e-03, -5.1634e-02, -4.4909e-02,\n",
       "                      -1.3043e-02,  2.9508e-02, -5.2499e-02, -4.2734e-02, -9.0687e-02,\n",
       "                      -3.9678e-02,  8.9478e-02, -7.5696e-02, -1.8099e-02,  4.4351e-02,\n",
       "                       1.3596e-01,  5.0212e-02, -1.9062e-06, -2.2975e-04, -8.1902e-07,\n",
       "                      -2.5944e-04, -4.1780e-04,  1.0763e-05,  2.8408e-05, -5.9497e-05,\n",
       "                      -3.1989e-04,  3.8465e-05, -3.5404e-05,  9.7850e-05, -1.7220e-05,\n",
       "                      -1.6090e-04,  5.7025e-05, -1.0522e-04, -1.1697e-04,  5.0903e-05,\n",
       "                      -6.5813e-05, -1.8366e-04, -6.3908e-06, -2.7353e-04,  3.1709e-04,\n",
       "                      -4.3124e-04, -5.0788e-04, -6.3033e-04,  2.1326e-04,  1.8285e-04,\n",
       "                       4.2535e-05,  4.6109e-04,  4.3241e-06, -3.1890e-04, -5.3692e-02,\n",
       "                      -1.3465e-02,  3.0029e-02, -2.2864e-02,  2.2607e-02, -1.1045e-02,\n",
       "                      -5.0964e-04, -2.1506e-02,  2.5545e-02,  1.8509e-02, -3.4068e-02,\n",
       "                      -5.3875e-02, -3.9529e-02, -4.1114e-02, -1.1681e-02, -1.3114e-02,\n",
       "                       2.3758e-02,  1.7229e-02, -1.0586e-03,  6.1144e-03,  2.7897e-02,\n",
       "                       2.8820e-02, -2.2118e-02,  3.1276e-02, -5.1536e-02,  1.3690e-02,\n",
       "                       8.3695e-02,  9.8274e-03, -9.1298e-03,  4.5108e-02, -3.0639e-03,\n",
       "                       8.5842e-03], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.out_proj.weight',\n",
       "              tensor([[ 0.1123,  0.0039, -0.0957,  ...,  0.1090, -0.0574, -0.0324],\n",
       "                      [ 0.0261, -0.0276, -0.0563,  ..., -0.0088,  0.0220,  0.1528],\n",
       "                      [ 0.0785, -0.0553, -0.1602,  ...,  0.0971,  0.0810,  0.0147],\n",
       "                      ...,\n",
       "                      [ 0.1340,  0.1973, -0.1702,  ..., -0.0747,  0.0400,  0.0347],\n",
       "                      [-0.0787,  0.0696, -0.0815,  ...,  0.0431, -0.0057,  0.0659],\n",
       "                      [-0.1208, -0.0170, -0.1111,  ...,  0.1867,  0.0217,  0.1092]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_self_attn.out_proj.bias',\n",
       "              tensor([ 0.0134, -0.0124,  0.0205,  0.0145, -0.0978,  0.1054, -0.0212, -0.0010,\n",
       "                      -0.0135, -0.0081, -0.0192,  0.0381, -0.0047,  0.0087, -0.0217,  0.0120,\n",
       "                       0.0177,  0.1525, -0.0083,  0.0158, -0.0103,  0.0079, -0.0091, -0.0179,\n",
       "                       0.0008,  0.0026,  0.0203,  0.0144,  0.0097,  0.0394,  0.0222,  0.0168],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm1.weight',\n",
       "              tensor([0.7800, 0.9033, 0.7809, 0.7834, 0.8430, 0.8858, 0.8396, 0.8384, 0.7686,\n",
       "                      0.8101, 0.8003, 0.8596, 0.7840, 0.7595, 0.8295, 0.7286, 0.8336, 1.0169,\n",
       "                      0.8120, 0.8335, 0.8188, 0.8413, 0.8272, 0.8237, 0.7652, 0.8169, 0.7816,\n",
       "                      0.8403, 0.8017, 0.8566, 0.8088, 0.7776], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm1.bias',\n",
       "              tensor([ 0.0126, -0.0271,  0.0209,  0.0138, -0.0092,  0.0424, -0.0154,  0.0025,\n",
       "                      -0.0074, -0.0079, -0.0181,  0.0284, -0.0091,  0.0035, -0.0094,  0.0109,\n",
       "                       0.0076,  0.0877, -0.0006,  0.0174, -0.0073,  0.0199, -0.0336, -0.0112,\n",
       "                       0.0035,  0.0093,  0.0268,  0.0169,  0.0175,  0.0390,  0.0299,  0.0129],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.in_proj_weight',\n",
       "              tensor([[-0.1410, -0.0386, -0.0445,  ...,  0.1061, -0.0055, -0.1702],\n",
       "                      [ 0.0921,  0.1236,  0.0425,  ..., -0.0618, -0.0206,  0.0816],\n",
       "                      [ 0.0374, -0.0717, -0.0026,  ...,  0.0262, -0.1474,  0.1744],\n",
       "                      ...,\n",
       "                      [ 0.1345, -0.0406,  0.0038,  ...,  0.1949,  0.0069,  0.0856],\n",
       "                      [-0.2194, -0.1092, -0.0268,  ..., -0.1144, -0.1062, -0.0418],\n",
       "                      [-0.0124, -0.0453,  0.0381,  ..., -0.0430,  0.1170, -0.1531]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.in_proj_bias',\n",
       "              tensor([-4.2021e-02, -3.7537e-02, -2.3479e-02,  9.6580e-03, -3.6173e-02,\n",
       "                       2.7912e-02, -4.3817e-02,  8.5077e-02,  3.5880e-02, -1.6904e-02,\n",
       "                       8.9353e-02, -6.3359e-02,  1.0774e-01, -9.7786e-02, -6.2398e-02,\n",
       "                       8.2347e-03, -3.1909e-02, -6.0583e-03, -6.9724e-02,  1.5312e-02,\n",
       "                      -3.8167e-03, -1.5175e-02, -1.9111e-02,  5.8258e-03, -7.3892e-02,\n",
       "                      -3.7551e-02, -9.3588e-02,  2.3631e-02, -2.9928e-02, -1.2633e-01,\n",
       "                      -9.1331e-02, -2.0572e-02, -1.6951e-04,  2.5021e-05, -1.4494e-04,\n",
       "                       1.8690e-06, -2.4201e-04, -5.0336e-05,  1.6633e-04, -1.6632e-04,\n",
       "                       8.3561e-05, -6.9679e-05, -2.7179e-04, -3.7869e-05, -6.1157e-06,\n",
       "                      -2.3717e-06, -5.6099e-05,  1.4879e-04,  4.2233e-04,  1.0057e-04,\n",
       "                       2.0795e-05, -2.6262e-05, -9.8725e-05, -1.6735e-04, -1.8469e-05,\n",
       "                      -8.6025e-05, -6.0148e-04,  7.8636e-04,  8.7222e-04,  2.1077e-05,\n",
       "                       3.8157e-05,  1.3285e-03,  1.0204e-03, -2.4515e-04, -6.6413e-03,\n",
       "                       7.3426e-02,  1.7873e-02, -9.0974e-03, -1.5201e-02, -4.2252e-03,\n",
       "                       4.0120e-02,  1.5969e-02, -1.0349e-02,  2.1423e-02, -3.9850e-02,\n",
       "                       4.4517e-02,  3.6951e-03,  1.3340e-02, -1.8990e-02,  1.9958e-02,\n",
       "                      -2.0822e-02, -9.6458e-03, -1.2515e-02,  4.8718e-03,  2.0256e-03,\n",
       "                       4.3549e-02, -1.5992e-02,  2.9019e-03,  2.2977e-02, -2.6388e-02,\n",
       "                      -1.4305e-02, -2.4169e-02, -2.2922e-02, -2.0407e-03, -1.9860e-02,\n",
       "                      -4.6880e-02], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.out_proj.weight',\n",
       "              tensor([[ 0.1386,  0.0832, -0.1708,  ..., -0.0112, -0.1630, -0.1183],\n",
       "                      [ 0.0161,  0.1103, -0.1587,  ...,  0.0934,  0.0531, -0.0248],\n",
       "                      [-0.1018,  0.0878, -0.0738,  ...,  0.0311,  0.0715,  0.0842],\n",
       "                      ...,\n",
       "                      [ 0.0284,  0.1107, -0.0165,  ...,  0.1472,  0.0201, -0.1424],\n",
       "                      [-0.1368, -0.0495, -0.0949,  ..., -0.1282,  0.1487, -0.2646],\n",
       "                      [ 0.0938, -0.1324, -0.1712,  ..., -0.0039,  0.0089,  0.0773]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_cross_attn.out_proj.bias',\n",
       "              tensor([ 0.0163, -0.0025,  0.0235,  0.0159, -0.0026,  0.0568, -0.0160,  0.0092,\n",
       "                      -0.0066, -0.0128, -0.0220,  0.0379, -0.0041,  0.0076, -0.0113,  0.0062,\n",
       "                       0.0081,  0.0792, -0.0017,  0.0205, -0.0088,  0.0324, -0.0396, -0.0125,\n",
       "                       0.0297,  0.0046,  0.0044,  0.0090,  0.0120,  0.0172,  0.0219,  0.0098],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm2.weight',\n",
       "              tensor([0.7858, 0.7950, 0.7464, 0.8015, 0.7683, 0.7387, 0.8309, 0.7627, 0.7193,\n",
       "                      0.7701, 0.7809, 0.8459, 0.7525, 0.6484, 0.8083, 0.6943, 0.8158, 0.4945,\n",
       "                      0.7840, 0.8133, 0.7403, 0.8109, 0.7642, 0.8151, 0.6742, 0.7693, 0.7012,\n",
       "                      0.7744, 0.7952, 0.7533, 0.7923, 0.8054], device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_norm2.bias',\n",
       "              tensor([ 0.0202, -0.0341,  0.0284,  0.0215,  0.0757, -0.0792, -0.0069,  0.0250,\n",
       "                      -0.0003, -0.0127, -0.0232,  0.0377, -0.0010,  0.0083,  0.0029,  0.0057,\n",
       "                      -0.0089, -0.2191,  0.0110,  0.0299, -0.0051,  0.0442, -0.0852, -0.0052,\n",
       "                       0.0492,  0.0516,  0.0203,  0.0128,  0.0205,  0.0218,  0.0295,  0.0007],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.0.weight',\n",
       "              tensor([[ 0.1942,  0.2160,  0.1148,  ...,  0.0137,  0.0289,  0.0358],\n",
       "                      [ 0.0188, -0.0980,  0.0354,  ..., -0.0228,  0.4109, -0.1035],\n",
       "                      [-0.0710,  0.1747,  0.1740,  ...,  0.0782, -0.0925,  0.1080],\n",
       "                      ...,\n",
       "                      [-0.0162, -0.0157,  0.1829,  ..., -0.0006,  0.2555, -0.0665],\n",
       "                      [ 0.0711,  0.0441,  0.0283,  ..., -0.0334,  0.0944,  0.0794],\n",
       "                      [-0.0986,  0.1661,  0.0513,  ..., -0.0824,  0.0121, -0.0574]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.0.bias',\n",
       "              tensor([-0.0240, -0.0292, -0.0263,  0.0306,  0.0498,  0.0252,  0.0610,  0.0037,\n",
       "                       0.0644, -0.1367, -0.0077, -0.1354, -0.0931,  0.0443,  0.1253, -0.0569,\n",
       "                      -0.1446, -0.0027,  0.1267, -0.1551, -0.1205, -0.0390, -0.0285, -0.0014,\n",
       "                       0.0334,  0.0069, -0.0385, -0.0859,  0.0188, -0.1648, -0.0221,  0.0062,\n",
       "                       0.0170, -0.0077, -0.0101, -0.0925, -0.1386,  0.0521, -0.1224,  0.0219,\n",
       "                      -0.0764, -0.1314, -0.1003,  0.0379,  0.0293,  0.1768, -0.0427, -0.2523,\n",
       "                      -0.1017,  0.1148, -0.1618, -0.0901, -0.0075,  0.0513,  0.0084,  0.0059,\n",
       "                      -0.1267, -0.1364, -0.0212,  0.0116, -0.0427, -0.1307,  0.0206,  0.0819],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.2.weight',\n",
       "              tensor([[-0.0442,  0.1170,  0.0550,  ..., -0.1747, -0.0362, -0.0230],\n",
       "                      [-0.0793,  0.0893, -0.1893,  ..., -0.0946, -0.0733,  0.0667],\n",
       "                      [-0.0721,  0.0736, -0.1248,  ..., -0.0706, -0.0309,  0.0725],\n",
       "                      ...,\n",
       "                      [-0.2553,  0.1058,  0.0040,  ..., -0.1037,  0.0523, -0.1094],\n",
       "                      [ 0.0513, -0.0059,  0.1422,  ...,  0.0536,  0.0377, -0.0824],\n",
       "                      [-0.1472,  0.0874, -0.1464,  ..., -0.1791, -0.0899, -0.0042]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder_layers.1.decoder_feedforward.2.bias',\n",
       "              tensor([ 0.1009, -0.0170, -0.0263,  0.0153,  0.0952, -0.0446,  0.0222,  0.0534,\n",
       "                       0.0219,  0.1093, -0.1124, -0.0366,  0.0827, -0.0580, -0.0814, -0.0536,\n",
       "                      -0.0058,  0.0522,  0.0957, -0.0217, -0.1060, -0.0175,  0.0440, -0.0032,\n",
       "                       0.0145,  0.0693,  0.0889, -0.0015, -0.0317, -0.0619, -0.0443, -0.0717],\n",
       "                     device='cuda:0')),\n",
       "             ('output_feedforward.0.weight',\n",
       "              tensor([[ 0.0548,  0.0093,  0.0439,  0.0545, -0.0268,  0.0051, -0.0650,  0.0004,\n",
       "                       -0.0184,  0.0295, -0.0631, -0.0004,  0.0301, -0.0123, -0.0618,  0.0046,\n",
       "                        0.0428,  0.0113, -0.0318,  0.0563, -0.0281, -0.0197,  0.0114, -0.0934,\n",
       "                        0.0057,  0.0342, -0.0334, -0.0191, -0.0342,  0.0094, -0.0450,  0.0125]],\n",
       "                     device='cuda:0')),\n",
       "             ('output_feedforward.0.bias', tensor([0.0580], device='cuda:0'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(\n",
    "    optimizer=optimizer,\n",
    "    model=trans_encoder_decoder_tf_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    ot_steps=20,\n",
    "    report_interval=5,\n",
    "    n_epochs=200,\n",
    "    list_train_loss=train_loss,\n",
    "    list_val_loss=val_loss,\n",
    "    device=device,\n",
    "    eps=1e-8,\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22fa6546-4ebd-44d5-8316-21438df4d6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x525 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from proj_mod import visualization\n",
    "\n",
    "train_loss_cut = train_loss[10:]\n",
    "val_loss_cut = val_loss[10:]\n",
    "\n",
    "vis_dict={(\"decoder\",\"current as ground target\"):{\"train_loss\": train_loss_cut,\"val_loss\": val_loss_cut}}\n",
    "visualization.training_plots(vis_dict, fig_width=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe6253",
   "metadata": {},
   "source": [
    "#### The above model has already beaten all (native) rnn based model, I think adding more layers of encoder and decoder might improve it, let's try. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f09188",
   "metadata": {},
   "source": [
    "We will keep using the dataloader and components of the above model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a9ec2",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434510c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder_decoder_tf_model=training.encoder_decoder_teacherforcing(\n",
    "    pos_emb_model=pos_embedder,\n",
    "    output_feedforward=output_ff,\n",
    "    encoder_dropout=0.2,\n",
    "    decoder_dropout=0.2,\n",
    "    encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "    decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "    n_diff=n_diff,\n",
    "    encoder_layer_num=4, #Changed\n",
    "    decoder_layer_num=4, #Changed\n",
    "    input_scaler=10000,\n",
    "    ts_emb_dim=ts_emb_dim,\n",
    "    encoder_num_heads=4,\n",
    "    decoder_num_heads=4,\n",
    "    encoder_keep_mag=True,\n",
    "    decoder_keep_mag=True,\n",
    "    return_sum=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be4025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(trans_encoder_decoder_tf_model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75305284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "encoder_decoder_teacherforcing                               --\n",
       "├─frozen_diff_conv: 1-1                                      --\n",
       "│    └─Conv1d: 2-1                                           (2)\n",
       "├─pos_emb_cross_attn: 1-2                                    --\n",
       "│    └─Linear: 2-2                                           128\n",
       "│    └─Embedding: 2-3                                        1,920\n",
       "│    └─MultiheadAttention: 2-4                               3,168\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1             1,056\n",
       "│    └─LayerNorm: 2-5                                        64\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─ts_encoder: 2-6                                       --\n",
       "│    │    └─MultiheadAttention: 3-2                          4,224\n",
       "│    │    └─LayerNorm: 3-3                                   64\n",
       "│    │    └─ModuleList: 3-4                                  4,192\n",
       "│    │    └─LayerNorm: 3-5                                   64\n",
       "│    └─ts_encoder: 2-7                                       --\n",
       "│    │    └─MultiheadAttention: 3-6                          4,224\n",
       "│    │    └─LayerNorm: 3-7                                   64\n",
       "│    │    └─ModuleList: 3-8                                  4,192\n",
       "│    │    └─LayerNorm: 3-9                                   64\n",
       "│    └─ts_encoder: 2-8                                       --\n",
       "│    │    └─MultiheadAttention: 3-10                         4,224\n",
       "│    │    └─LayerNorm: 3-11                                  64\n",
       "│    │    └─ModuleList: 3-12                                 4,192\n",
       "│    │    └─LayerNorm: 3-13                                  64\n",
       "│    └─ts_encoder: 2-9                                       --\n",
       "│    │    └─MultiheadAttention: 3-14                         4,224\n",
       "│    │    └─LayerNorm: 3-15                                  64\n",
       "│    │    └─ModuleList: 3-16                                 4,192\n",
       "│    │    └─LayerNorm: 3-17                                  64\n",
       "├─ModuleList: 1-4                                            --\n",
       "│    └─ts_decoder: 2-10                                      --\n",
       "│    │    └─MultiheadAttention: 3-18                         4,224\n",
       "│    │    └─LayerNorm: 3-19                                  64\n",
       "│    │    └─MultiheadAttention: 3-20                         4,224\n",
       "│    │    └─LayerNorm: 3-21                                  64\n",
       "│    │    └─ModuleList: 3-22                                 4,192\n",
       "│    └─ts_decoder: 2-11                                      --\n",
       "│    │    └─MultiheadAttention: 3-23                         4,224\n",
       "│    │    └─LayerNorm: 3-24                                  64\n",
       "│    │    └─MultiheadAttention: 3-25                         4,224\n",
       "│    │    └─LayerNorm: 3-26                                  64\n",
       "│    │    └─ModuleList: 3-27                                 4,192\n",
       "│    └─ts_decoder: 2-12                                      --\n",
       "│    │    └─MultiheadAttention: 3-28                         4,224\n",
       "│    │    └─LayerNorm: 3-29                                  64\n",
       "│    │    └─MultiheadAttention: 3-30                         4,224\n",
       "│    │    └─LayerNorm: 3-31                                  64\n",
       "│    │    └─ModuleList: 3-32                                 4,192\n",
       "│    └─ts_decoder: 2-13                                      --\n",
       "│    │    └─MultiheadAttention: 3-33                         4,224\n",
       "│    │    └─LayerNorm: 3-34                                  64\n",
       "│    │    └─MultiheadAttention: 3-35                         4,224\n",
       "│    │    └─LayerNorm: 3-36                                  64\n",
       "│    │    └─ModuleList: 3-37                                 4,192\n",
       "├─Sequential: 1-5                                            --\n",
       "│    └─Linear: 2-14                                          33\n",
       "=====================================================================================\n",
       "Total params: 91,619\n",
       "Trainable params: 91,617\n",
       "Non-trainable params: 2\n",
       "====================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(trans_encoder_decoder_tf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd74f5",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e0d49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new best validation loss at epoch  1  with validation loss of  tensor(0.2409, device='cuda:0') .\n",
      "At  44.422447204589844  epoch  1 has training loss  tensor(0.3097, device='cuda:0')  and validation loss  tensor(0.2409, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  2  with validation loss of  tensor(0.2336, device='cuda:0') .\n",
      "A new best validation loss at epoch  4  with validation loss of  tensor(0.2336, device='cuda:0') .\n",
      "At  229.51333856582642  epoch  5 has training loss  tensor(0.2512, device='cuda:0')  and validation loss  tensor(0.2386, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  6  with validation loss of  tensor(0.2315, device='cuda:0') .\n",
      "A new best validation loss at epoch  8  with validation loss of  tensor(0.2313, device='cuda:0') .\n",
      "A new best validation loss at epoch  10  with validation loss of  tensor(0.2309, device='cuda:0') .\n",
      "At  460.97391963005066  epoch  10 has training loss  tensor(0.2481, device='cuda:0')  and validation loss  tensor(0.2309, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  12  with validation loss of  tensor(0.2308, device='cuda:0') .\n",
      "At  691.7925190925598  epoch  15 has training loss  tensor(0.2466, device='cuda:0')  and validation loss  tensor(0.2373, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  17  with validation loss of  tensor(0.2297, device='cuda:0') .\n",
      "At  922.9469921588898  epoch  20 has training loss  tensor(0.2462, device='cuda:0')  and validation loss  tensor(0.2339, device='cuda:0') .\n",
      "\n",
      "At epoch 23, learning rate has been updated from 0.001 to 0.0005, reloading previous best model weights from epoch 17 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "A new best validation loss at epoch  25  with validation loss of  tensor(0.2297, device='cuda:0') .\n",
      "At  1154.9313354492188  epoch  25 has training loss  tensor(0.2435, device='cuda:0')  and validation loss  tensor(0.2297, device='cuda:0') .\n",
      "\n",
      "At epoch 29, learning rate has been updated from 0.0005 to 0.00025, reloading previous best model weights from epoch 25 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  1386.9541459083557  epoch  30 has training loss  tensor(0.2421, device='cuda:0')  and validation loss  tensor(0.2300, device='cuda:0') .\n",
      "\n",
      "At  1619.2814273834229  epoch  35 has training loss  tensor(0.2417, device='cuda:0')  and validation loss  tensor(0.2299, device='cuda:0') .\n",
      "\n",
      "At epoch 35, learning rate has been updated from 0.00025 to 0.000125, reloading previous best model weights from epoch 25 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  1851.3628125190735  epoch  40 has training loss  tensor(0.2411, device='cuda:0')  and validation loss  tensor(0.2301, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  41  with validation loss of  tensor(0.2296, device='cuda:0') .\n",
      "A new best validation loss at epoch  45  with validation loss of  tensor(0.2295, device='cuda:0') .\n",
      "At  2083.4867866039276  epoch  45 has training loss  tensor(0.2408, device='cuda:0')  and validation loss  tensor(0.2295, device='cuda:0') .\n",
      "\n",
      "At  2315.502196788788  epoch  50 has training loss  tensor(0.2408, device='cuda:0')  and validation loss  tensor(0.2299, device='cuda:0') .\n",
      "\n",
      "At epoch 51, learning rate has been updated from 0.000125 to 6.25e-05, reloading previous best model weights from epoch 45 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  2548.260322570801  epoch  55 has training loss  tensor(0.2402, device='cuda:0')  and validation loss  tensor(0.2302, device='cuda:0') .\n",
      "\n",
      "At epoch 57, learning rate has been updated from 6.25e-05 to 3.125e-05, reloading previous best model weights from epoch 45 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  2780.677125453949  epoch  60 has training loss  tensor(0.2400, device='cuda:0')  and validation loss  tensor(0.2296, device='cuda:0') .\n",
      "\n",
      "At epoch 63, learning rate has been updated from 3.125e-05 to 1.5625e-05, reloading previous best model weights from epoch 45 ...\n",
      "\n",
      "Previous best model weights reloaded, training continues ... \n",
      "At  3013.402885913849  epoch  65 has training loss  tensor(0.2398, device='cuda:0')  and validation loss  tensor(0.2303, device='cuda:0') .\n",
      "\n",
      "The validation loss has not improved for  20  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 45  with validation loss:  tensor(0.2295, device='cuda:0') .\n",
      " The total number of epoch trained is  65 .\n",
      " Training completed in:  3013.402885913849 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en4de4_weight_dict=training.reg_training_loop_rmspe(\n",
    "    optimizer=optimizer,\n",
    "    model=trans_encoder_decoder_tf_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    ot_steps=20,\n",
    "    report_interval=5,\n",
    "    n_epochs=200,\n",
    "    list_train_loss=train_loss,\n",
    "    list_val_loss=val_loss,\n",
    "    device=device,\n",
    "    eps=1e-8,\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd6e72",
   "metadata": {},
   "source": [
    "## The model with above as base and adjusted with learned embedding of stock id by multiplication "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117dcad",
   "metadata": {},
   "source": [
    "Since we are sure that the native encoder with decoder transformer beat the native rnn based models, we will adjust them with the stock id and, hopefully, get improve result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f90cc",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e645f",
   "metadata": {},
   "source": [
    "#### Load in emb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5233136c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>RV</th>\n",
       "      <th>row_id</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>93-5</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>93-11</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>93-16</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>93-31</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>93-62</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>104-32751</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>104-32753</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>104-32758</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>104-32763</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>104-32767</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_id        RV     row_id stock_id\n",
       "0            5  0.002185       93-5       93\n",
       "1           11  0.001205      93-11       93\n",
       "2           16  0.001461      93-16       93\n",
       "3           31  0.001693      93-31       93\n",
       "4           62  0.001296      93-62       93\n",
       "...        ...       ...        ...      ...\n",
       "428927   32751  0.002337  104-32751      104\n",
       "428928   32753  0.001500  104-32753      104\n",
       "428929   32758  0.002272  104-32758      104\n",
       "428930   32763  0.001949  104-32763      104\n",
       "428931   32767  0.001347  104-32767      104\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RV_tab=pd.read_csv(\"../processed_data/RV_by_row_id.csv\")\n",
    "RV_tab[\"stock_id\"]=RV_tab[\"row_id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "RV_tab[\"time_id\"]=RV_tab[\"row_id\"].apply(lambda x: x.split(\"-\")[1])\n",
    "RV_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d81dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>RV</th>\n",
       "      <th>row_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>emb_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>93-5</td>\n",
       "      <td>93</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>93-11</td>\n",
       "      <td>93</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>93-16</td>\n",
       "      <td>93</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>93-31</td>\n",
       "      <td>93</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>93-62</td>\n",
       "      <td>93</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>32751</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>104-32751</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>32753</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>104-32753</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>32758</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>104-32758</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>32763</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>104-32763</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>32767</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>104-32767</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_id        RV     row_id stock_id  emb_id\n",
       "0            5  0.002185       93-5       93     105\n",
       "1           11  0.001205      93-11       93     105\n",
       "2           16  0.001461      93-16       93     105\n",
       "3           31  0.001693      93-31       93     105\n",
       "4           62  0.001296      93-62       93     105\n",
       "...        ...       ...        ...      ...     ...\n",
       "428927   32751  0.002337  104-32751      104       7\n",
       "428928   32753  0.001500  104-32753      104       7\n",
       "428929   32758  0.002272  104-32758      104       7\n",
       "428930   32763  0.001949  104-32763      104       7\n",
       "428931   32767  0.001347  104-32767      104       7\n",
       "\n",
       "[428932 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates tabular data, most specifically 'emb_id'\n",
    "unique_ids = sorted(RV_tab['stock_id'].unique())\n",
    "id_to_emb = {stock_id: i for i, stock_id in enumerate(unique_ids)}\n",
    "RV_tab['emb_id'] = RV_tab['stock_id'].map(id_to_emb)\n",
    "RV_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49e989",
   "metadata": {},
   "source": [
    "#### Recreate datasets and dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af9fe134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)\n",
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]\n",
    "\n",
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_tab_feat=RV_tab,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_tab_feat=RV_tab,df_target=df_target)\n",
    "\n",
    "ts_place, id_place=train_dataset.featureplace[\"sub_int_RV\"], train_dataset.featureplace[\"emb_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3a68afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e901666",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "addb7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder_decoder_tf_model=training.encoder_decoder_teacherforcing(\n",
    "    pos_emb_model=pos_embedder,\n",
    "    output_feedforward=output_ff,\n",
    "    encoder_dropout=0.2,\n",
    "    decoder_dropout=0.2,\n",
    "    encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "    decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "    n_diff=n_diff,\n",
    "    encoder_layer_num=4, #Changed\n",
    "    decoder_layer_num=4, #Changed\n",
    "    input_scaler=10000,\n",
    "    ts_emb_dim=ts_emb_dim,\n",
    "    encoder_num_heads=4,\n",
    "    decoder_num_heads=4,\n",
    "    encoder_keep_mag=True,\n",
    "    decoder_keep_mag=True,\n",
    "    return_sum=True\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba7e428b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "id_learned_embedding_adj_rnn_mtpl                                 --\n",
       "├─Embedding: 1-1                                                  896\n",
       "├─encoder_decoder_teacherforcing: 1-2                             --\n",
       "│    └─frozen_diff_conv: 2-1                                      --\n",
       "│    │    └─Conv1d: 3-1                                           (2)\n",
       "│    └─pos_emb_cross_attn: 2-2                                    --\n",
       "│    │    └─Linear: 3-2                                           128\n",
       "│    │    └─Embedding: 3-3                                        1,920\n",
       "│    │    └─MultiheadAttention: 3-4                               4,224\n",
       "│    │    └─LayerNorm: 3-5                                        64\n",
       "│    └─ModuleList: 2-3                                            --\n",
       "│    │    └─ts_encoder: 3-6                                       8,544\n",
       "│    │    └─ts_encoder: 3-7                                       8,544\n",
       "│    │    └─ts_encoder: 3-8                                       8,544\n",
       "│    │    └─ts_encoder: 3-9                                       8,544\n",
       "│    └─ModuleList: 2-4                                            --\n",
       "│    │    └─ts_decoder: 3-10                                      12,768\n",
       "│    │    └─ts_decoder: 3-11                                      12,768\n",
       "│    │    └─ts_decoder: 3-12                                      12,768\n",
       "│    │    └─ts_decoder: 3-13                                      12,768\n",
       "│    └─Sequential: 2-5                                            --\n",
       "│    │    └─Linear: 3-14                                          33\n",
       "├─Sequential: 1-3                                                 --\n",
       "│    └─Linear: 2-6                                                288\n",
       "│    └─Tanh: 2-7                                                  --\n",
       "│    └─Linear: 2-8                                                528\n",
       "│    └─Tanh: 2-9                                                  --\n",
       "│    └─Linear: 2-10                                               136\n",
       "│    └─Tanh: 2-11                                                 --\n",
       "│    └─Linear: 2-12                                               9\n",
       "==========================================================================================\n",
       "Total params: 93,476\n",
       "Trainable params: 93,474\n",
       "Non-trainable params: 2\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set base model \n",
    "base_trans_model=trans_encoder_decoder_tf_model\n",
    "\n",
    "#Create hidden layer for stock id learned embedding \n",
    "from collections import OrderedDict\n",
    "\n",
    "id_emb_dim=8\n",
    "\n",
    "id_hidden_dict=OrderedDict([(\"linear1\", nn.Linear(in_features=id_emb_dim, out_features=32),),\n",
    "                            (\"tanh1\", nn.Tanh()),\n",
    "                            (\"linear2\", nn.Linear(in_features=32, out_features=16)),\n",
    "                            (\"tanh2\", nn.Tanh()),\n",
    "                            (\"linear3\", nn.Linear(in_features=16, out_features=8)),\n",
    "                            (\"tanh3\", nn.Tanh()),\n",
    "                            (\"linear4\", nn.Linear(in_features=8,out_features=1))])\n",
    "\n",
    "id_hidden_layers=nn.Sequential(id_hidden_dict).to(device=device)\n",
    "\n",
    "#Create the adjustment by multiplication model (notice training.id_learned_embedding_adj_rnn_mtpl still works, we are just using the transformer model in place of the rnn based model). \n",
    "transformer_adjusted_model=training.id_learned_embedding_adj_rnn_mtpl(ts_place=ts_place,id_place=id_place, rnn_model=base_trans_model,id_hidden_model=id_hidden_layers,id_input_num=112,emb_dim=id_emb_dim).to(device)\n",
    "\n",
    "#Create the optimizer and the scheduler \n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(transformer_adjusted_model.parameters(), lr=1e-3)\n",
    "\n",
    "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "#Show summary\n",
    "from torchinfo import summary\n",
    "summary(transformer_adjusted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50365129",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af2b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer_adjusted_weight_dict=training.reg_training_loop_rmspe(\n",
    "    optimizer=optimizer,\n",
    "    model=transformer_adjusted_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    ot_steps=20,\n",
    "    report_interval=5,\n",
    "    n_epochs=200,\n",
    "    list_train_loss=train_loss,\n",
    "    list_val_loss=val_loss,\n",
    "    device=device,\n",
    "    eps=1e-8,\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da1e3a13-44cd-4fa5-b8de-8ea36d1788eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing, visualization\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);\n",
    "importlib.reload(visualization);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d6c97-f2c6-4fa0-a025-56fb53efe545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
