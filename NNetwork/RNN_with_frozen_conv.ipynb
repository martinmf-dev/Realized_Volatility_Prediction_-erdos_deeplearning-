{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77e9ebe",
   "metadata": {},
   "source": [
    "## RNN with frozen convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b854e",
   "metadata": {},
   "source": [
    "The idea is inspired by an application of RNN in predicting if an online review (say, for a product) is positive or negative (1 or 0). Each word in the review sentence is first projected to a large dimension vector space and then sent into an RNN network sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b66b",
   "metadata": {},
   "source": [
    "We will make use of frozen convolution layer to create derivative like features for our time series features. Then, we use the derivative values of a certain time as if a \"word\" in a \"review\", the target RV as if the \"score\" of the \"review\" to train an RNN network that predicts the target RV with the time series features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd88f7",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "907e47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12efccd",
   "metadata": {},
   "source": [
    "Below is what Yuan needed to get his gpu working, do not run if you do not need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd83d5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../dotenv_env/deep_learning.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd9ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb26cd",
   "metadata": {},
   "source": [
    "Let's say, we have following timeseries feature (created randomly), we will create the derivative features first. As a reminder, the zero dimension is the batch size, the dimension one is channel (needed for the first convolution layer, not really a \"thing\" for time series like features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3d0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feature=torch.randn(1,1,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28d5ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8983,  0.5294,  1.4815,  0.3880,  0.3053, -1.5859,  0.0210,\n",
       "          -0.6448, -1.3457, -1.2188, -0.0443, -1.0198, -0.6600, -0.2874,\n",
       "          -0.0385, -1.2630, -0.1093, -2.1785, -1.1292, -0.4585,  1.7239,\n",
       "           0.9651,  0.1646, -0.7025,  0.6864,  1.0771, -0.9391,  0.5244,\n",
       "           0.2303, -0.5098,  0.1424, -1.3571, -0.7195, -0.5237, -0.0643,\n",
       "           0.4805, -0.4966, -0.7618,  0.3211,  0.0881, -1.3531, -1.1748,\n",
       "          -2.0773, -0.4347,  0.3090,  0.9424,  0.8958,  1.6446,  1.7279,\n",
       "           0.7207, -0.0487, -0.8066, -0.2446,  0.1778, -0.6278, -2.7156,\n",
       "           0.2626,  1.2195, -0.9886, -1.0032]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739b2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_create_diff=training.frozen_diff_conv(n_diff=4)\n",
    "ts_diff_feature=conv_create_diff(ts_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48f9ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.9833e-01,  5.2935e-01,  1.4815e+00,  3.8805e-01,  3.0529e-01,\n",
       "          -1.5859e+00,  2.1027e-02, -6.4484e-01, -1.3457e+00, -1.2188e+00,\n",
       "          -4.4305e-02, -1.0198e+00, -6.5998e-01, -2.8743e-01, -3.8456e-02,\n",
       "          -1.2630e+00, -1.0929e-01, -2.1785e+00, -1.1292e+00, -4.5855e-01,\n",
       "           1.7239e+00,  9.6507e-01,  1.6458e-01, -7.0247e-01,  6.8645e-01,\n",
       "           1.0771e+00, -9.3911e-01,  5.2441e-01,  2.3027e-01, -5.0984e-01,\n",
       "           1.4243e-01, -1.3571e+00, -7.1949e-01, -5.2366e-01, -6.4339e-02,\n",
       "           4.8050e-01, -4.9660e-01, -7.6183e-01,  3.2113e-01,  8.8130e-02,\n",
       "          -1.3531e+00, -1.1748e+00, -2.0773e+00, -4.3466e-01,  3.0903e-01,\n",
       "           9.4244e-01,  8.9581e-01,  1.6446e+00,  1.7279e+00,  7.2071e-01,\n",
       "          -4.8721e-02, -8.0657e-01, -2.4458e-01,  1.7781e-01, -6.2775e-01,\n",
       "          -2.7156e+00,  2.6262e-01,  1.2195e+00, -9.8861e-01, -1.0032e+00],\n",
       "         [ 1.4277e+00,  9.5216e-01, -1.0935e+00, -8.2756e-02, -1.8912e+00,\n",
       "           1.6069e+00, -6.6587e-01, -7.0087e-01,  1.2694e-01,  1.1745e+00,\n",
       "          -9.7550e-01,  3.5983e-01,  3.7255e-01,  2.4898e-01, -1.2246e+00,\n",
       "           1.1537e+00, -2.0692e+00,  1.0493e+00,  6.7064e-01,  2.1825e+00,\n",
       "          -7.5884e-01, -8.0049e-01, -8.6705e-01,  1.3889e+00,  3.9068e-01,\n",
       "          -2.0162e+00,  1.4635e+00, -2.9414e-01, -7.4011e-01,  6.5227e-01,\n",
       "          -1.4996e+00,  6.3764e-01,  1.9583e-01,  4.5932e-01,  5.4484e-01,\n",
       "          -9.7711e-01, -2.6522e-01,  1.0830e+00, -2.3300e-01, -1.4412e+00,\n",
       "           1.7825e-01, -9.0244e-01,  1.6426e+00,  7.4369e-01,  6.3342e-01,\n",
       "          -4.6635e-02,  7.4877e-01,  8.3366e-02, -1.0072e+00, -7.6943e-01,\n",
       "          -7.5785e-01,  5.6199e-01,  4.2239e-01, -8.0557e-01, -2.0878e+00,\n",
       "           2.9782e+00,  9.5686e-01, -2.2081e+00, -1.4631e-02,  0.0000e+00],\n",
       "         [-4.7552e-01, -2.0456e+00,  1.0107e+00, -1.8085e+00,  3.4982e+00,\n",
       "          -2.2728e+00, -3.5007e-02,  8.2781e-01,  1.0475e+00, -2.1500e+00,\n",
       "           1.3353e+00,  1.2722e-02, -1.2357e-01, -1.4736e+00,  2.3783e+00,\n",
       "          -3.2230e+00,  3.1185e+00, -3.7867e-01,  1.5118e+00, -2.9413e+00,\n",
       "          -4.1642e-02, -6.6564e-02,  2.2560e+00, -9.9824e-01, -2.4069e+00,\n",
       "           3.4797e+00, -1.7577e+00, -4.4597e-01,  1.3924e+00, -2.1518e+00,\n",
       "           2.1372e+00, -4.4180e-01,  2.6349e-01,  8.5520e-02, -1.5220e+00,\n",
       "           7.1189e-01,  1.3482e+00, -1.3160e+00, -1.2082e+00,  1.6195e+00,\n",
       "          -1.0807e+00,  2.5451e+00, -8.9893e-01, -1.1027e-01, -6.8005e-01,\n",
       "           7.9541e-01, -6.6541e-01, -1.0906e+00,  2.3782e-01,  1.1577e-02,\n",
       "           1.3198e+00, -1.3960e-01, -1.2280e+00, -1.2823e+00,  5.0661e+00,\n",
       "          -2.0214e+00, -3.1649e+00,  2.1935e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.5701e+00,  3.0563e+00, -2.8192e+00,  5.3066e+00, -5.7710e+00,\n",
       "           2.2378e+00,  8.6282e-01,  2.1971e-01, -3.1975e+00,  3.4853e+00,\n",
       "          -1.3226e+00, -1.3629e-01, -1.3500e+00,  3.8519e+00, -5.6013e+00,\n",
       "           6.3415e+00, -3.4972e+00,  1.8905e+00, -4.4531e+00,  2.8997e+00,\n",
       "          -2.4921e-02,  2.3225e+00, -3.2542e+00, -1.4087e+00,  5.8867e+00,\n",
       "          -5.2374e+00,  1.3117e+00,  1.8383e+00, -3.5442e+00,  4.2890e+00,\n",
       "          -2.5790e+00,  7.0530e-01, -1.7797e-01, -1.6075e+00,  2.2338e+00,\n",
       "           6.3629e-01, -2.6641e+00,  1.0774e-01,  2.8277e+00, -2.7002e+00,\n",
       "           3.6258e+00, -3.4440e+00,  7.8866e-01, -5.6978e-01,  1.4755e+00,\n",
       "          -1.4608e+00, -4.2520e-01,  1.3284e+00, -2.2624e-01,  1.3083e+00,\n",
       "          -1.4594e+00, -1.0884e+00, -5.4327e-02,  6.3484e+00, -7.0874e+00,\n",
       "          -1.1436e+00,  5.3584e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 4.6265e+00, -5.8755e+00,  8.1258e+00, -1.1078e+01,  8.0088e+00,\n",
       "          -1.3750e+00, -6.4310e-01, -3.4172e+00,  6.6828e+00, -4.8079e+00,\n",
       "           1.1863e+00, -1.2137e+00,  5.2019e+00, -9.4532e+00,  1.1943e+01,\n",
       "          -9.8387e+00,  5.3877e+00, -6.3436e+00,  7.3528e+00, -2.9246e+00,\n",
       "           2.3475e+00, -5.5767e+00,  1.8455e+00,  7.2953e+00, -1.1124e+01,\n",
       "           6.5491e+00,  5.2667e-01, -5.3825e+00,  7.8332e+00, -6.8680e+00,\n",
       "           3.2843e+00, -8.8327e-01, -1.4295e+00,  3.8413e+00, -1.5976e+00,\n",
       "          -3.3004e+00,  2.7719e+00,  2.7199e+00, -5.5278e+00,  6.3259e+00,\n",
       "          -7.0698e+00,  4.2327e+00, -1.3584e+00,  2.0452e+00, -2.9363e+00,\n",
       "           1.0356e+00,  1.7536e+00, -1.5547e+00,  1.5345e+00, -2.7677e+00,\n",
       "           3.7109e-01,  1.0340e+00,  6.4027e+00, -1.3436e+01,  5.9439e+00,\n",
       "           6.5020e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79075913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 60])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3354b4",
   "metadata": {},
   "source": [
    "We now permute the last two dimensions, essentially, this is just a transposition of the last two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76682d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_diff_feature=ts_diff_feature.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed4b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cff150",
   "metadata": {},
   "source": [
    "Now, this is a batch (of size 1) of timeseries feature with 60 time steps, and 5 features in each step. We then expand the 5 features, say, to 32 by projection. As a reminder, nn.Linear automatically apply to the last dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b7d96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj=nn.Linear(5,32)\n",
    "ts_feature_proj=linear_proj(ts_diff_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f82b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40c151",
   "metadata": {},
   "source": [
    "In this example, we will not go into too much detail, so let's make the most simple version. We then pass through a layer of RNN, then a layer of linear (to project to dimension 1 again). Learn more about RNN at https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f611ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_layer=nn.RNN(input_size=32,hidden_size=32,num_layers=1,nonlinearity=\"tanh\",batch_first=True,dropout=0)\n",
    "linear_end=nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "207568f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_out=RNN_layer(ts_feature_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc91386",
   "metadata": {},
   "source": [
    "As a reminder, since no training is done, the RNN basically just applies the initial weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f31dfe8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e423255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_proj=linear_end(RNN_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ba4ee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22784470",
   "metadata": {},
   "source": [
    "We will use sum, but we may change this to other functions, according to context or just feeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e293bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_out=torch.sum(out_proj,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b35c7bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8542]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f52c",
   "metadata": {},
   "source": [
    "## Creating the actual NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d007bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created 07/02/25\n",
    "#07/02/25: Moved to training.py \n",
    "class RV_RNN_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    :param n_diff: Decides how many derivative features is wanted in the time series. \n",
    "    :param rnn_num_layer: num_layer parameter for rnn. \n",
    "    :param rnn_drop_out: dropout parameter for rnn. \n",
    "    :param rnn_act: Defaulted to \"tanh\". Nonlinearity parameter for rnn. \n",
    "    :param proj_dim: Defaulted to 32. Decided the dimension of projection before feeding into rnn. \n",
    "    :param rnn_hidden_size: Defaulted to 32. The hidden_size parameter for rnn. \n",
    "    \"\"\"\n",
    "    def __init__(self,n_diff,rnn_num_layer,rnn_drop_out,rnn_act=\"tanh\",proj_dim=32,rnn_hidden_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frozen_conv=training.frozen_diff_conv(n_diff=n_diff)\n",
    "        self.linear_proj_input=nn.Linear(n_diff+1,proj_dim)\n",
    "        self.RNN_layer=nn.RNN(input_size=proj_dim,hidden_size=rnn_hidden_size,num_layers=rnn_num_layer,nonlinearity=rnn_act,batch_first=True,dropout=rnn_drop_out)\n",
    "        self.linear_post_rnn=nn.Linear(rnn_hidden_size,1)\n",
    "        self.frozen_list=[\"frozen_conv\"] \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #First, unsqueese to add in one dimension in dim 1 as channel. This is needed for convolution. \n",
    "        x=torch.unsqueeze(x,dim=1)\n",
    "        x=self.frozen_conv(x)\n",
    "        x=x.permute(0,2,1)\n",
    "        x=self.linear_proj_input(x)\n",
    "        x=self.RNN_layer(x)[0]\n",
    "        x=self.linear_post_rnn(x)\n",
    "        \n",
    "        return torch.sum(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71432b31",
   "metadata": {},
   "source": [
    "## Basic testing on the RNN with frozen convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9583750",
   "metadata": {},
   "source": [
    "Test it first. As a reminder, the expected input dimensions are (Batch Size, Time Series Length), this is distinct from the example above in that the channel dimension is not present. The channel dimension is added with unsqueeze at dimension 1 so that the first convolution layer can be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df3d8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feature=torch.randn(10,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33fd643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0699e-01,  7.1855e-01,  1.0534e+00, -2.7195e-02, -5.3953e-01,\n",
       "         -2.1916e-01,  2.6058e-01,  9.4682e-01, -5.7270e-01,  6.1821e-01,\n",
       "          7.4706e-01,  8.5857e-01, -6.3387e-01,  1.0074e-01,  1.0871e+00,\n",
       "          4.4188e-01,  6.0410e-01, -3.4202e-02, -1.5008e-01,  8.9701e-01,\n",
       "         -6.9344e-01,  1.7957e+00, -5.8575e-01, -2.4478e+00,  7.0324e-01,\n",
       "         -1.2766e+00, -3.7510e-01, -3.5719e-01, -1.3143e+00,  6.8766e-01,\n",
       "         -2.5387e-01, -1.0662e+00,  5.9732e-01,  1.1774e+00,  1.7849e-01,\n",
       "         -7.2704e-02, -6.2228e-01, -9.0305e-01, -7.6060e-01,  2.4960e+00,\n",
       "         -3.7462e-01, -5.7164e-01,  7.3972e-02, -1.5861e+00, -7.6500e-01,\n",
       "          8.6943e-01,  4.5075e-01,  1.3651e-02,  2.1515e-01, -1.4589e+00,\n",
       "          1.1666e+00, -2.4947e-01, -6.6102e-01,  7.9651e-01,  7.1330e-01,\n",
       "         -5.8549e-01,  8.9783e-01, -5.2981e-01, -1.6684e-01, -5.2264e-01],\n",
       "        [ 6.6567e-01,  1.5717e-01, -8.9532e-01,  9.2971e-01, -6.0536e-01,\n",
       "          5.8591e-02, -5.4184e-01,  3.6571e-01, -4.7071e-01,  9.3300e-01,\n",
       "         -5.3932e-01, -8.8753e-01,  1.7115e+00,  2.2261e-02,  1.0817e+00,\n",
       "         -5.2810e-01,  4.6068e-01, -4.0759e-01, -2.1657e+00, -8.6611e-01,\n",
       "          1.1546e+00,  3.8663e-01, -3.0246e-01,  1.7155e-01,  1.5774e-01,\n",
       "          9.7471e-01, -1.0136e+00, -1.1826e+00, -1.0026e+00,  1.4194e-01,\n",
       "          3.0013e-01,  7.5464e-01,  1.3789e+00,  3.0984e-01,  9.0080e-01,\n",
       "         -2.6490e-01, -5.7665e-01, -1.6571e+00, -3.0413e-01,  1.2294e+00,\n",
       "          8.7073e-02, -1.5744e-01, -4.1935e-01, -4.0364e-01, -5.4075e-01,\n",
       "          4.4449e-01,  6.8356e-01,  2.0351e+00,  7.3060e-01,  3.7839e-01,\n",
       "          1.5160e+00,  8.3184e-01, -1.0474e+00,  1.3620e+00,  1.1588e-01,\n",
       "          1.5253e+00,  1.7736e-01,  8.9396e-01,  1.3331e+00, -1.1479e+00],\n",
       "        [-3.1383e-01,  1.7709e+00,  2.6152e-01,  4.4231e-01,  1.0152e+00,\n",
       "          2.5837e-01, -1.1688e+00, -5.6972e-01, -2.7012e-01, -3.6764e-01,\n",
       "          6.9320e-01,  6.5906e-01,  2.7373e-01, -4.1007e-01,  1.2787e+00,\n",
       "          3.7425e-01,  1.8299e-01,  1.2031e+00,  8.7180e-02, -2.2043e-01,\n",
       "          2.1431e-01,  8.1473e-01, -3.8800e-01,  8.1544e-01,  1.5113e+00,\n",
       "          8.5249e-01, -1.0512e+00, -1.5145e-01, -4.3640e-01,  6.4185e-01,\n",
       "         -3.6349e-01, -3.4011e-01, -1.6253e-02, -1.5880e+00, -1.5473e+00,\n",
       "         -1.2960e+00, -1.0107e+00,  2.2210e-02, -9.6097e-01, -1.9353e+00,\n",
       "         -1.3860e-01, -2.3329e-01, -8.0765e-01, -1.2327e-01, -1.0109e-01,\n",
       "         -2.0385e+00,  5.9140e-01, -6.8946e-01,  7.7740e-01, -1.5059e+00,\n",
       "         -1.4401e+00, -1.8734e+00, -5.5187e-01,  8.5734e-01,  8.0007e-01,\n",
       "         -7.4051e-02, -1.4025e+00,  9.7038e-01,  8.4608e-01, -7.2799e-01],\n",
       "        [-6.1188e-01, -3.7782e-01, -6.9515e-01,  1.7807e+00, -6.2075e-01,\n",
       "          3.7258e-01, -1.6661e+00,  3.5429e-01,  9.0524e-01, -1.5668e+00,\n",
       "         -1.0589e+00,  2.7080e-01, -1.2181e-01,  6.8743e-01,  6.4531e-01,\n",
       "          8.4906e-01,  3.5709e-03, -1.8106e+00, -4.3988e-03,  1.1574e+00,\n",
       "         -1.3036e+00,  9.2590e-01,  3.3343e-01,  4.5248e-02, -1.3245e+00,\n",
       "          3.3481e-01,  1.8628e+00, -4.5933e-01, -1.9357e+00, -1.3864e+00,\n",
       "         -2.6935e-01,  1.3430e+00,  4.7692e-01, -4.2193e-01, -6.1142e-01,\n",
       "         -5.6804e-01,  1.9160e+00,  7.5138e-01, -2.8756e-01, -5.6903e-01,\n",
       "          4.2587e-01,  2.6432e-01,  1.2855e+00,  4.6892e-01,  4.5061e-01,\n",
       "          7.0222e-02, -1.3397e+00,  2.4306e-02, -1.4951e+00, -1.2806e+00,\n",
       "         -2.0086e-01, -1.3639e+00,  2.8384e+00, -4.9187e-01,  1.3123e+00,\n",
       "          1.6335e+00, -2.0656e-01, -5.9957e-01, -1.3313e+00, -4.8102e-01],\n",
       "        [ 9.7902e-02, -4.8628e-02, -1.5149e+00,  1.8632e+00,  1.4713e-01,\n",
       "          6.5375e-01,  6.5687e-01, -6.3389e-01,  2.6316e+00,  4.8502e-01,\n",
       "          5.6849e-01, -1.4237e+00, -1.0267e+00,  2.3458e-01, -1.1950e+00,\n",
       "         -4.8989e-01,  7.3510e-01,  6.7091e-01,  2.2866e-01, -5.1608e-01,\n",
       "         -5.1772e-01, -4.2767e-01,  1.0707e+00,  3.4465e-01, -8.9433e-02,\n",
       "          1.0571e+00, -2.1829e-01, -1.2362e+00, -1.5092e+00, -2.7615e-01,\n",
       "         -2.8056e-01, -1.4141e+00, -1.8467e+00, -1.0360e-01,  1.3964e+00,\n",
       "          8.5886e-01, -3.9587e-01,  7.5805e-01,  3.8120e-01, -9.9700e-01,\n",
       "         -5.1492e-01, -1.6261e+00,  4.3083e-01, -7.8798e-01,  1.1481e+00,\n",
       "          1.3981e-01,  4.1850e-01,  8.0180e-01,  1.5969e+00,  2.1907e-01,\n",
       "          1.2043e-01, -9.7177e-01, -1.2572e-01,  2.0148e-01, -6.7343e-01,\n",
       "          7.4242e-01, -1.4649e+00,  1.7106e-01, -2.2052e-02,  3.7862e-01],\n",
       "        [-7.6046e-01,  7.7445e-01,  1.0674e+00,  6.0515e-01,  5.6979e-02,\n",
       "         -2.0358e-01,  1.3249e+00, -9.6556e-01, -1.2466e+00, -4.7124e-05,\n",
       "         -5.4907e-01,  1.6015e+00, -1.1331e+00, -3.4846e-01,  6.5020e-01,\n",
       "          1.8926e+00,  9.8035e-01,  5.4590e-02,  1.5252e+00,  3.9066e-01,\n",
       "          1.1330e+00, -4.0262e-01,  8.3597e-01, -1.9556e+00, -8.3535e-01,\n",
       "          1.5365e+00,  8.4672e-01,  7.6476e-01,  1.5271e+00, -3.1132e-01,\n",
       "         -5.8377e-01, -2.7592e-01,  6.5311e-01,  2.3342e-01,  3.5443e-01,\n",
       "         -1.5415e-01,  1.1575e+00,  6.4139e-01,  4.2455e-01, -2.2738e+00,\n",
       "         -1.5026e+00, -2.3264e-01,  1.0121e+00, -1.6208e+00, -1.1724e+00,\n",
       "          8.3668e-01,  8.8240e-01,  1.2998e+00, -2.3566e-01, -1.8489e+00,\n",
       "         -5.8500e-01,  5.0238e-01, -9.1927e-01, -6.3203e-01,  3.8581e-01,\n",
       "          7.2006e-01,  5.2369e-03, -1.7417e+00,  1.3047e+00,  7.0037e-01],\n",
       "        [-7.1683e-01,  1.5823e-01, -6.2349e-01,  2.1356e-01, -1.0327e+00,\n",
       "         -9.6736e-01,  8.5647e-01, -3.0849e-01,  2.1342e-01, -1.0970e+00,\n",
       "         -1.2257e-01, -4.1890e-01, -1.3747e+00, -1.0818e+00, -2.5439e-01,\n",
       "         -8.9761e-01, -5.9607e-01,  6.8220e-01, -7.3358e-01,  2.1102e-01,\n",
       "          5.0198e-01,  7.1287e-01,  4.7946e-01, -8.9818e-01, -7.8512e-01,\n",
       "          4.6087e-01,  2.3871e-01,  8.8274e-01, -3.7661e-01,  7.7403e-01,\n",
       "          7.0285e-01,  7.3269e-01, -1.1015e+00, -1.0830e-01,  2.0790e-01,\n",
       "         -1.2537e+00,  1.4871e+00, -2.9796e+00, -7.9944e-02, -6.0269e-01,\n",
       "         -3.0241e-01, -2.4495e+00,  1.4325e+00, -1.3013e+00, -1.4146e+00,\n",
       "          9.6618e-01, -6.8042e-01,  1.5329e-01,  5.5032e-02,  1.1112e+00,\n",
       "          1.5633e+00, -5.1884e-01,  1.1944e+00, -9.0681e-01, -4.9503e-01,\n",
       "          4.2033e-01, -8.4554e-01, -2.5663e-01,  1.8645e+00,  8.6548e-01],\n",
       "        [-1.0031e+00,  1.0106e+00,  9.0124e-01, -6.1037e-02,  1.2433e+00,\n",
       "         -2.8391e-01, -1.2603e+00, -1.1845e-01, -3.6038e-01,  1.2522e+00,\n",
       "          2.5964e+00,  2.7716e-01,  1.1813e+00,  2.4513e-02,  4.4231e-01,\n",
       "          9.9420e-01, -5.9467e-01,  7.0462e-01, -6.7140e-02, -7.9125e-01,\n",
       "          8.5289e-01, -4.2252e-01,  3.1974e-01,  1.0101e+00, -1.4330e-01,\n",
       "          1.2879e+00,  6.8264e-01, -3.7084e-01,  2.1376e-01, -1.4327e-01,\n",
       "         -5.7791e-01,  4.7522e-02, -1.4555e+00, -7.3328e-01,  1.5527e+00,\n",
       "         -9.9591e-02, -3.3893e-01,  7.5888e-01, -5.4199e-01, -1.0971e-01,\n",
       "         -4.0091e-01, -6.8656e-01,  8.7550e-02, -7.5264e-01, -1.1896e+00,\n",
       "          5.4613e-01,  6.4360e-03,  1.3247e+00,  1.4591e-01, -7.0127e-01,\n",
       "          3.0877e-01,  1.6118e+00,  2.0603e+00, -5.1066e-01,  2.1066e+00,\n",
       "         -5.9266e-02,  9.5798e-01, -1.4065e+00, -3.9040e-01, -3.0182e-01],\n",
       "        [ 5.7502e-02, -2.0137e+00, -1.7316e+00,  2.2969e-01,  9.6013e-01,\n",
       "         -1.2587e+00,  7.6647e-01, -1.8146e+00,  3.4629e-01,  4.7381e-02,\n",
       "          8.7413e-01,  1.3099e+00, -1.6870e+00, -1.6495e+00, -2.8130e+00,\n",
       "         -7.0553e-02, -7.5668e-02,  3.6507e-01,  6.6190e-01,  6.3048e-01,\n",
       "         -6.2638e-01,  4.0515e-01,  1.7310e+00,  1.6105e-01,  2.2605e-01,\n",
       "          1.3939e+00, -5.8125e-01,  6.7055e-01, -7.3460e-01,  2.0369e-01,\n",
       "         -2.2090e+00,  1.4305e-01, -5.9041e-01,  1.8142e+00,  7.9174e-01,\n",
       "         -9.2927e-01,  1.1031e+00, -1.1953e+00, -7.5676e-01,  6.2964e-01,\n",
       "         -4.7575e-02, -7.9687e-01,  1.4241e+00, -5.4193e-01,  1.8558e-01,\n",
       "         -1.5719e+00,  9.3078e-01, -1.3936e-01,  1.4639e+00,  9.8803e-01,\n",
       "         -7.1421e-02, -2.0910e+00, -8.5636e-01,  7.3226e-01,  7.3974e-01,\n",
       "         -1.2444e+00, -1.4409e+00,  1.5782e-01, -9.7425e-01, -6.7974e-01],\n",
       "        [ 2.2948e+00, -1.8640e+00, -5.5448e-01,  3.2076e-01, -2.3818e-01,\n",
       "          1.2985e+00, -1.6192e+00,  5.2382e-01,  1.3727e+00, -1.1024e+00,\n",
       "          5.4309e-01,  1.2560e+00, -1.7994e+00, -5.8652e-01, -1.2797e+00,\n",
       "          1.2038e+00, -2.3111e-01,  5.3263e-01, -1.4184e-01,  8.9734e-01,\n",
       "         -1.5853e+00, -7.6677e-01, -3.9792e-01,  2.5489e-01, -2.3910e+00,\n",
       "          1.4045e+00,  1.0795e+00, -6.9067e-01, -1.2928e+00, -1.3707e-01,\n",
       "         -5.3978e-01, -1.9825e+00, -1.4532e-01,  1.6245e+00, -6.2019e-01,\n",
       "         -1.8293e+00, -7.2871e-01,  6.8437e-01,  3.0371e-03, -6.0490e-01,\n",
       "         -3.5266e-01,  8.5015e-01, -4.6574e-01, -2.2452e+00, -9.5815e-01,\n",
       "          1.1328e+00, -8.0109e-01,  3.8276e-01, -1.6251e+00,  7.1061e-01,\n",
       "         -1.3349e+00,  3.2079e-01,  2.6394e+00, -9.4284e-01,  1.2536e+00,\n",
       "         -2.1126e+00, -8.2753e-01, -9.7723e-01, -7.8323e-01,  8.3279e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72cd7276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.0699e-01,  7.1855e-01,  1.0534e+00, -2.7195e-02, -5.3953e-01,\n",
       "          -2.1916e-01,  2.6058e-01,  9.4682e-01, -5.7270e-01,  6.1821e-01,\n",
       "           7.4706e-01,  8.5857e-01, -6.3387e-01,  1.0074e-01,  1.0871e+00,\n",
       "           4.4188e-01,  6.0410e-01, -3.4202e-02, -1.5008e-01,  8.9701e-01,\n",
       "          -6.9344e-01,  1.7957e+00, -5.8575e-01, -2.4478e+00,  7.0324e-01,\n",
       "          -1.2766e+00, -3.7510e-01, -3.5719e-01, -1.3143e+00,  6.8766e-01,\n",
       "          -2.5387e-01, -1.0662e+00,  5.9732e-01,  1.1774e+00,  1.7849e-01,\n",
       "          -7.2704e-02, -6.2228e-01, -9.0305e-01, -7.6060e-01,  2.4960e+00,\n",
       "          -3.7462e-01, -5.7164e-01,  7.3972e-02, -1.5861e+00, -7.6500e-01,\n",
       "           8.6943e-01,  4.5075e-01,  1.3651e-02,  2.1515e-01, -1.4589e+00,\n",
       "           1.1666e+00, -2.4947e-01, -6.6102e-01,  7.9651e-01,  7.1330e-01,\n",
       "          -5.8549e-01,  8.9783e-01, -5.2981e-01, -1.6684e-01, -5.2264e-01]],\n",
       "\n",
       "        [[ 6.6567e-01,  1.5717e-01, -8.9532e-01,  9.2971e-01, -6.0536e-01,\n",
       "           5.8591e-02, -5.4184e-01,  3.6571e-01, -4.7071e-01,  9.3300e-01,\n",
       "          -5.3932e-01, -8.8753e-01,  1.7115e+00,  2.2261e-02,  1.0817e+00,\n",
       "          -5.2810e-01,  4.6068e-01, -4.0759e-01, -2.1657e+00, -8.6611e-01,\n",
       "           1.1546e+00,  3.8663e-01, -3.0246e-01,  1.7155e-01,  1.5774e-01,\n",
       "           9.7471e-01, -1.0136e+00, -1.1826e+00, -1.0026e+00,  1.4194e-01,\n",
       "           3.0013e-01,  7.5464e-01,  1.3789e+00,  3.0984e-01,  9.0080e-01,\n",
       "          -2.6490e-01, -5.7665e-01, -1.6571e+00, -3.0413e-01,  1.2294e+00,\n",
       "           8.7073e-02, -1.5744e-01, -4.1935e-01, -4.0364e-01, -5.4075e-01,\n",
       "           4.4449e-01,  6.8356e-01,  2.0351e+00,  7.3060e-01,  3.7839e-01,\n",
       "           1.5160e+00,  8.3184e-01, -1.0474e+00,  1.3620e+00,  1.1588e-01,\n",
       "           1.5253e+00,  1.7736e-01,  8.9396e-01,  1.3331e+00, -1.1479e+00]],\n",
       "\n",
       "        [[-3.1383e-01,  1.7709e+00,  2.6152e-01,  4.4231e-01,  1.0152e+00,\n",
       "           2.5837e-01, -1.1688e+00, -5.6972e-01, -2.7012e-01, -3.6764e-01,\n",
       "           6.9320e-01,  6.5906e-01,  2.7373e-01, -4.1007e-01,  1.2787e+00,\n",
       "           3.7425e-01,  1.8299e-01,  1.2031e+00,  8.7180e-02, -2.2043e-01,\n",
       "           2.1431e-01,  8.1473e-01, -3.8800e-01,  8.1544e-01,  1.5113e+00,\n",
       "           8.5249e-01, -1.0512e+00, -1.5145e-01, -4.3640e-01,  6.4185e-01,\n",
       "          -3.6349e-01, -3.4011e-01, -1.6253e-02, -1.5880e+00, -1.5473e+00,\n",
       "          -1.2960e+00, -1.0107e+00,  2.2210e-02, -9.6097e-01, -1.9353e+00,\n",
       "          -1.3860e-01, -2.3329e-01, -8.0765e-01, -1.2327e-01, -1.0109e-01,\n",
       "          -2.0385e+00,  5.9140e-01, -6.8946e-01,  7.7740e-01, -1.5059e+00,\n",
       "          -1.4401e+00, -1.8734e+00, -5.5187e-01,  8.5734e-01,  8.0007e-01,\n",
       "          -7.4051e-02, -1.4025e+00,  9.7038e-01,  8.4608e-01, -7.2799e-01]],\n",
       "\n",
       "        [[-6.1188e-01, -3.7782e-01, -6.9515e-01,  1.7807e+00, -6.2075e-01,\n",
       "           3.7258e-01, -1.6661e+00,  3.5429e-01,  9.0524e-01, -1.5668e+00,\n",
       "          -1.0589e+00,  2.7080e-01, -1.2181e-01,  6.8743e-01,  6.4531e-01,\n",
       "           8.4906e-01,  3.5709e-03, -1.8106e+00, -4.3988e-03,  1.1574e+00,\n",
       "          -1.3036e+00,  9.2590e-01,  3.3343e-01,  4.5248e-02, -1.3245e+00,\n",
       "           3.3481e-01,  1.8628e+00, -4.5933e-01, -1.9357e+00, -1.3864e+00,\n",
       "          -2.6935e-01,  1.3430e+00,  4.7692e-01, -4.2193e-01, -6.1142e-01,\n",
       "          -5.6804e-01,  1.9160e+00,  7.5138e-01, -2.8756e-01, -5.6903e-01,\n",
       "           4.2587e-01,  2.6432e-01,  1.2855e+00,  4.6892e-01,  4.5061e-01,\n",
       "           7.0222e-02, -1.3397e+00,  2.4306e-02, -1.4951e+00, -1.2806e+00,\n",
       "          -2.0086e-01, -1.3639e+00,  2.8384e+00, -4.9187e-01,  1.3123e+00,\n",
       "           1.6335e+00, -2.0656e-01, -5.9957e-01, -1.3313e+00, -4.8102e-01]],\n",
       "\n",
       "        [[ 9.7902e-02, -4.8628e-02, -1.5149e+00,  1.8632e+00,  1.4713e-01,\n",
       "           6.5375e-01,  6.5687e-01, -6.3389e-01,  2.6316e+00,  4.8502e-01,\n",
       "           5.6849e-01, -1.4237e+00, -1.0267e+00,  2.3458e-01, -1.1950e+00,\n",
       "          -4.8989e-01,  7.3510e-01,  6.7091e-01,  2.2866e-01, -5.1608e-01,\n",
       "          -5.1772e-01, -4.2767e-01,  1.0707e+00,  3.4465e-01, -8.9433e-02,\n",
       "           1.0571e+00, -2.1829e-01, -1.2362e+00, -1.5092e+00, -2.7615e-01,\n",
       "          -2.8056e-01, -1.4141e+00, -1.8467e+00, -1.0360e-01,  1.3964e+00,\n",
       "           8.5886e-01, -3.9587e-01,  7.5805e-01,  3.8120e-01, -9.9700e-01,\n",
       "          -5.1492e-01, -1.6261e+00,  4.3083e-01, -7.8798e-01,  1.1481e+00,\n",
       "           1.3981e-01,  4.1850e-01,  8.0180e-01,  1.5969e+00,  2.1907e-01,\n",
       "           1.2043e-01, -9.7177e-01, -1.2572e-01,  2.0148e-01, -6.7343e-01,\n",
       "           7.4242e-01, -1.4649e+00,  1.7106e-01, -2.2052e-02,  3.7862e-01]],\n",
       "\n",
       "        [[-7.6046e-01,  7.7445e-01,  1.0674e+00,  6.0515e-01,  5.6979e-02,\n",
       "          -2.0358e-01,  1.3249e+00, -9.6556e-01, -1.2466e+00, -4.7124e-05,\n",
       "          -5.4907e-01,  1.6015e+00, -1.1331e+00, -3.4846e-01,  6.5020e-01,\n",
       "           1.8926e+00,  9.8035e-01,  5.4590e-02,  1.5252e+00,  3.9066e-01,\n",
       "           1.1330e+00, -4.0262e-01,  8.3597e-01, -1.9556e+00, -8.3535e-01,\n",
       "           1.5365e+00,  8.4672e-01,  7.6476e-01,  1.5271e+00, -3.1132e-01,\n",
       "          -5.8377e-01, -2.7592e-01,  6.5311e-01,  2.3342e-01,  3.5443e-01,\n",
       "          -1.5415e-01,  1.1575e+00,  6.4139e-01,  4.2455e-01, -2.2738e+00,\n",
       "          -1.5026e+00, -2.3264e-01,  1.0121e+00, -1.6208e+00, -1.1724e+00,\n",
       "           8.3668e-01,  8.8240e-01,  1.2998e+00, -2.3566e-01, -1.8489e+00,\n",
       "          -5.8500e-01,  5.0238e-01, -9.1927e-01, -6.3203e-01,  3.8581e-01,\n",
       "           7.2006e-01,  5.2369e-03, -1.7417e+00,  1.3047e+00,  7.0037e-01]],\n",
       "\n",
       "        [[-7.1683e-01,  1.5823e-01, -6.2349e-01,  2.1356e-01, -1.0327e+00,\n",
       "          -9.6736e-01,  8.5647e-01, -3.0849e-01,  2.1342e-01, -1.0970e+00,\n",
       "          -1.2257e-01, -4.1890e-01, -1.3747e+00, -1.0818e+00, -2.5439e-01,\n",
       "          -8.9761e-01, -5.9607e-01,  6.8220e-01, -7.3358e-01,  2.1102e-01,\n",
       "           5.0198e-01,  7.1287e-01,  4.7946e-01, -8.9818e-01, -7.8512e-01,\n",
       "           4.6087e-01,  2.3871e-01,  8.8274e-01, -3.7661e-01,  7.7403e-01,\n",
       "           7.0285e-01,  7.3269e-01, -1.1015e+00, -1.0830e-01,  2.0790e-01,\n",
       "          -1.2537e+00,  1.4871e+00, -2.9796e+00, -7.9944e-02, -6.0269e-01,\n",
       "          -3.0241e-01, -2.4495e+00,  1.4325e+00, -1.3013e+00, -1.4146e+00,\n",
       "           9.6618e-01, -6.8042e-01,  1.5329e-01,  5.5032e-02,  1.1112e+00,\n",
       "           1.5633e+00, -5.1884e-01,  1.1944e+00, -9.0681e-01, -4.9503e-01,\n",
       "           4.2033e-01, -8.4554e-01, -2.5663e-01,  1.8645e+00,  8.6548e-01]],\n",
       "\n",
       "        [[-1.0031e+00,  1.0106e+00,  9.0124e-01, -6.1037e-02,  1.2433e+00,\n",
       "          -2.8391e-01, -1.2603e+00, -1.1845e-01, -3.6038e-01,  1.2522e+00,\n",
       "           2.5964e+00,  2.7716e-01,  1.1813e+00,  2.4513e-02,  4.4231e-01,\n",
       "           9.9420e-01, -5.9467e-01,  7.0462e-01, -6.7140e-02, -7.9125e-01,\n",
       "           8.5289e-01, -4.2252e-01,  3.1974e-01,  1.0101e+00, -1.4330e-01,\n",
       "           1.2879e+00,  6.8264e-01, -3.7084e-01,  2.1376e-01, -1.4327e-01,\n",
       "          -5.7791e-01,  4.7522e-02, -1.4555e+00, -7.3328e-01,  1.5527e+00,\n",
       "          -9.9591e-02, -3.3893e-01,  7.5888e-01, -5.4199e-01, -1.0971e-01,\n",
       "          -4.0091e-01, -6.8656e-01,  8.7550e-02, -7.5264e-01, -1.1896e+00,\n",
       "           5.4613e-01,  6.4360e-03,  1.3247e+00,  1.4591e-01, -7.0127e-01,\n",
       "           3.0877e-01,  1.6118e+00,  2.0603e+00, -5.1066e-01,  2.1066e+00,\n",
       "          -5.9266e-02,  9.5798e-01, -1.4065e+00, -3.9040e-01, -3.0182e-01]],\n",
       "\n",
       "        [[ 5.7502e-02, -2.0137e+00, -1.7316e+00,  2.2969e-01,  9.6013e-01,\n",
       "          -1.2587e+00,  7.6647e-01, -1.8146e+00,  3.4629e-01,  4.7381e-02,\n",
       "           8.7413e-01,  1.3099e+00, -1.6870e+00, -1.6495e+00, -2.8130e+00,\n",
       "          -7.0553e-02, -7.5668e-02,  3.6507e-01,  6.6190e-01,  6.3048e-01,\n",
       "          -6.2638e-01,  4.0515e-01,  1.7310e+00,  1.6105e-01,  2.2605e-01,\n",
       "           1.3939e+00, -5.8125e-01,  6.7055e-01, -7.3460e-01,  2.0369e-01,\n",
       "          -2.2090e+00,  1.4305e-01, -5.9041e-01,  1.8142e+00,  7.9174e-01,\n",
       "          -9.2927e-01,  1.1031e+00, -1.1953e+00, -7.5676e-01,  6.2964e-01,\n",
       "          -4.7575e-02, -7.9687e-01,  1.4241e+00, -5.4193e-01,  1.8558e-01,\n",
       "          -1.5719e+00,  9.3078e-01, -1.3936e-01,  1.4639e+00,  9.8803e-01,\n",
       "          -7.1421e-02, -2.0910e+00, -8.5636e-01,  7.3226e-01,  7.3974e-01,\n",
       "          -1.2444e+00, -1.4409e+00,  1.5782e-01, -9.7425e-01, -6.7974e-01]],\n",
       "\n",
       "        [[ 2.2948e+00, -1.8640e+00, -5.5448e-01,  3.2076e-01, -2.3818e-01,\n",
       "           1.2985e+00, -1.6192e+00,  5.2382e-01,  1.3727e+00, -1.1024e+00,\n",
       "           5.4309e-01,  1.2560e+00, -1.7994e+00, -5.8652e-01, -1.2797e+00,\n",
       "           1.2038e+00, -2.3111e-01,  5.3263e-01, -1.4184e-01,  8.9734e-01,\n",
       "          -1.5853e+00, -7.6677e-01, -3.9792e-01,  2.5489e-01, -2.3910e+00,\n",
       "           1.4045e+00,  1.0795e+00, -6.9067e-01, -1.2928e+00, -1.3707e-01,\n",
       "          -5.3978e-01, -1.9825e+00, -1.4532e-01,  1.6245e+00, -6.2019e-01,\n",
       "          -1.8293e+00, -7.2871e-01,  6.8437e-01,  3.0371e-03, -6.0490e-01,\n",
       "          -3.5266e-01,  8.5015e-01, -4.6574e-01, -2.2452e+00, -9.5815e-01,\n",
       "           1.1328e+00, -8.0109e-01,  3.8276e-01, -1.6251e+00,  7.1061e-01,\n",
       "          -1.3349e+00,  3.2079e-01,  2.6394e+00, -9.4284e-01,  1.2536e+00,\n",
       "          -2.1126e+00, -8.2753e-01, -9.7723e-01, -7.8323e-01,  8.3279e-01]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(ts_feature,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "455ed170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_num_layer=2,rnn_drop_out=0.3,rnn_hidden_size=32,proj_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85bf9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(ts_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc35034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.4432],\n",
       "        [14.1024],\n",
       "        [14.2141],\n",
       "        [10.5330],\n",
       "        [11.9638],\n",
       "        [10.5465],\n",
       "        [13.0989],\n",
       "        [10.8659],\n",
       "        [11.3673],\n",
       "        [12.0642]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfbac7",
   "metadata": {},
   "source": [
    "## Training loop (basic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d4fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825838c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72aedca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3338fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de450e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">sub_int_RV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_int_num</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-1000</th>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>3.818799e-07</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>2.313288e-04</td>\n",
       "      <td>1.060893e-05</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10000</th>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>3.154886e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>3.777703e-08</td>\n",
       "      <td>3.777703e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10005</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>4.375100e-04</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>2.552987e-03</td>\n",
       "      <td>5.364106e-04</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10017</th>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>6.771948e-05</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>3.104404e-03</td>\n",
       "      <td>1.224910e-03</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10030</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>2.586782e-04</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>4.842470e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9972</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>2.503467e-04</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>9.916624e-05</td>\n",
       "      <td>1.809852e-04</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9973</th>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>9.650211e-04</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>1.862600e-03</td>\n",
       "      <td>7.668418e-04</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9976</th>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>7.203531e-04</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>5.946683e-04</td>\n",
       "      <td>1.509652e-04</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9988</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.957402e-04</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.440137e-04</td>\n",
       "      <td>3.200939e-05</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9993</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>1.798617e-04</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>2.728767e-04</td>\n",
       "      <td>2.108380e-04</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sub_int_RV                                                        \\\n",
       "sub_int_num         1         2         3         4         5             6    \n",
       "row_id                                                                         \n",
       "0-1000        0.000341  0.000000  0.000023  0.000000  0.000170  3.818799e-07   \n",
       "0-10000       0.000290  0.000191  0.000087  0.000193  0.000241  3.154886e-04   \n",
       "0-10005       0.000000  0.000000  0.001554  0.002177  0.002303  4.375100e-04   \n",
       "0-10017       0.000142  0.000142  0.001464  0.001086  0.000068  6.771948e-05   \n",
       "0-10030       0.000327  0.000058  0.000293  0.000842  0.000120  2.586782e-04   \n",
       "...                ...       ...       ...       ...       ...           ...   \n",
       "99-9972       0.000197  0.000181  0.000171  0.000172  0.000369  2.503467e-04   \n",
       "99-9973       0.000821  0.000346  0.000691  0.001591  0.000863  9.650211e-04   \n",
       "99-9976       0.000569  0.001101  0.001002  0.000430  0.000797  7.203531e-04   \n",
       "99-9988       0.000040  0.000069  0.000123  0.000056  0.000016  1.957402e-04   \n",
       "99-9993       0.000249  0.000179  0.000155  0.000025  0.000325  1.798617e-04   \n",
       "\n",
       "                                                     ...                      \\\n",
       "sub_int_num        7         8         9         10  ...        51        52   \n",
       "row_id                                               ...                       \n",
       "0-1000       0.000089  0.000552  0.000012  0.000000  ...  0.000265  0.000000   \n",
       "0-10000      0.000000  0.000247  0.000265  0.000000  ...  0.000202  0.000375   \n",
       "0-10005      0.000617  0.001199  0.002306  0.001215  ...  0.000000  0.000486   \n",
       "0-10017      0.000899  0.000064  0.000593  0.000451  ...  0.000029  0.000000   \n",
       "0-10030      0.000221  0.000436  0.000099  0.000008  ...  0.000410  0.000437   \n",
       "...               ...       ...       ...       ...  ...       ...       ...   \n",
       "99-9972      0.000349  0.000356  0.000390  0.000050  ...  0.000075  0.000185   \n",
       "99-9973      0.000504  0.001925  0.000641  0.000382  ...  0.001081  0.001095   \n",
       "99-9976      0.000586  0.000538  0.000570  0.000781  ...  0.000508  0.000406   \n",
       "99-9988      0.000071  0.000095  0.000063  0.000030  ...  0.000034  0.000176   \n",
       "99-9993      0.000080  0.000125  0.000126  0.000205  ...  0.000099  0.000370   \n",
       "\n",
       "                                                                   \\\n",
       "sub_int_num        53        54        55        56            57   \n",
       "row_id                                                              \n",
       "0-1000       0.000214  0.000003  0.000000  0.000118  2.313288e-04   \n",
       "0-10000      0.000616  0.000564  0.000000  0.000023  3.777703e-08   \n",
       "0-10005      0.000050  0.001761  0.001617  0.001801  2.552987e-03   \n",
       "0-10017      0.001293  0.002092  0.000994  0.000848  3.104404e-03   \n",
       "0-10030      0.000004  0.000215  0.000457  0.000183  4.842470e-04   \n",
       "...               ...       ...       ...       ...           ...   \n",
       "99-9972      0.000314  0.000318  0.000115  0.000143  9.916624e-05   \n",
       "99-9973      0.000425  0.000789  0.001295  0.000596  1.862600e-03   \n",
       "99-9976      0.000662  0.000338  0.000710  0.000179  5.946683e-04   \n",
       "99-9988      0.000140  0.000129  0.000175  0.000019  1.440137e-04   \n",
       "99-9993      0.000929  0.000328  0.000251  0.000204  2.728767e-04   \n",
       "\n",
       "                                               \n",
       "sub_int_num            58        59        60  \n",
       "row_id                                         \n",
       "0-1000       1.060893e-05  0.000111  0.000288  \n",
       "0-10000      3.777703e-08  0.000020  0.000310  \n",
       "0-10005      5.364106e-04  0.000872  0.000000  \n",
       "0-10017      1.224910e-03  0.001316  0.003287  \n",
       "0-10030      0.000000e+00  0.000756  0.000005  \n",
       "...                   ...       ...       ...  \n",
       "99-9972      1.809852e-04  0.000334  0.000089  \n",
       "99-9973      7.668418e-04  0.001035  0.002115  \n",
       "99-9976      1.509652e-04  0.000388  0.000403  \n",
       "99-9988      3.200939e-05  0.000041  0.000007  \n",
       "99-9993      2.108380e-04  0.000126  0.000353  \n",
       "\n",
       "[428932 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RV_ts.pivot(index=\"row_id\",columns=[\"sub_int_num\"],values=[\"sub_int_RV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677c1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252d6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb28caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2a07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n",
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c432cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386036, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07885f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386036, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0056627",
   "metadata": {},
   "source": [
    "Not entirely sure what the error code is about, I am literally doing exactly as suggested by the warning. https://stackoverflow.com/questions/23688307/settingwithcopywarning-even-when-using-loc says this is a false positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efd16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b95e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_drop_out=0,rnn_hidden_size=32,proj_dim=32,rnn_num_layer=1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3846a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.Adam(RNN_model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55dd6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=256,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "292fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "810267b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  12.731416463851929  epoch  1 has training loss  tensor(0.2636, device='cuda:0')  and validation loss  tensor(0.2465, device='cuda:0') .\n",
      "\n",
      "At  65.36981534957886  epoch  5 has training loss  tensor(0.2542, device='cuda:0')  and validation loss  tensor(0.2378, device='cuda:0') .\n",
      "\n",
      "At  131.1433711051941  epoch  10 has training loss  tensor(0.2530, device='cuda:0')  and validation loss  tensor(0.2405, device='cuda:0') .\n",
      "\n",
      "At  197.14231967926025  epoch  15 has training loss  tensor(0.2525, device='cuda:0')  and validation loss  tensor(0.2353, device='cuda:0') .\n",
      "\n",
      "At  262.7027373313904  epoch  20 has training loss  tensor(0.2520, device='cuda:0')  and validation loss  tensor(0.2355, device='cuda:0') .\n",
      "\n",
      "At  328.84028601646423  epoch  25 has training loss  tensor(0.2512, device='cuda:0')  and validation loss  tensor(0.2355, device='cuda:0') .\n",
      "\n",
      "At  394.21911120414734  epoch  30 has training loss  tensor(0.2512, device='cuda:0')  and validation loss  tensor(0.2375, device='cuda:0') .\n",
      "\n",
      "At  459.89581966400146  epoch  35 has training loss  tensor(0.2511, device='cuda:0')  and validation loss  tensor(0.2346, device='cuda:0') .\n",
      "\n",
      "At  525.3399140834808  epoch  40 has training loss  tensor(0.2505, device='cuda:0')  and validation loss  tensor(0.2366, device='cuda:0') .\n",
      "\n",
      "At  591.0108163356781  epoch  45 has training loss  tensor(0.2505, device='cuda:0')  and validation loss  tensor(0.2342, device='cuda:0') .\n",
      "\n",
      "At  656.605872631073  epoch  50 has training loss  tensor(0.2502, device='cuda:0')  and validation loss  tensor(0.2379, device='cuda:0') .\n",
      "\n",
      "At  722.3282012939453  epoch  55 has training loss  tensor(0.2499, device='cuda:0')  and validation loss  tensor(0.2360, device='cuda:0') .\n",
      "\n",
      "At  788.1603856086731  epoch  60 has training loss  tensor(0.2498, device='cuda:0')  and validation loss  tensor(0.2359, device='cuda:0') .\n",
      "\n",
      "The validation loss has not improved for  10  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 51  with validation loss:  tensor(0.2340, device='cuda:0') .\n",
      " The total number of epoch trained is  61 .\n",
      " Training completed in:  801.4168922901154 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('frozen_conv.frozen_conv.weight',\n",
       "              tensor([[[-0.5742,  0.8249]]], device='cuda:0')),\n",
       "             ('linear_proj_input.weight',\n",
       "              tensor([[-0.0010,  0.0140, -0.0248, -0.0627, -0.0048],\n",
       "                      [-0.0101, -0.1089,  0.2952,  0.7073,  0.6144],\n",
       "                      [ 0.0114, -0.4419, -0.4059,  0.2944,  0.5035],\n",
       "                      [ 0.0105, -0.0283,  0.1350,  0.1587,  0.0275],\n",
       "                      [ 0.0115,  0.0615, -0.2302, -0.5448, -0.2916],\n",
       "                      [ 0.0177, -0.0194, -0.2500, -0.3596, -0.1169],\n",
       "                      [-0.0426,  0.3984,  0.2889,  0.0072,  0.4095],\n",
       "                      [-0.3627, -0.5428, -0.0983, -0.3433, -0.2666],\n",
       "                      [-0.0452,  0.1052,  0.1317, -0.0240,  0.0493],\n",
       "                      [ 0.1153, -0.0013,  0.5546,  0.4882,  0.1270],\n",
       "                      [ 0.0209, -0.0862,  0.0433,  0.2496,  0.1208],\n",
       "                      [ 0.0843, -0.2831, -0.1314, -0.1532,  0.1174],\n",
       "                      [ 0.1329, -0.4190, -0.8514, -0.5761, -0.0456],\n",
       "                      [-0.0901, -0.0507,  0.1532, -0.1264, -0.6370],\n",
       "                      [ 0.0233, -0.0301,  0.0850,  0.0791,  0.0400],\n",
       "                      [ 0.0103, -0.0132, -0.0172, -0.0313,  0.0112],\n",
       "                      [ 0.0179, -0.2605, -0.2713, -0.4835,  0.5503],\n",
       "                      [ 0.3679,  0.3694, -0.3611,  0.0833,  0.2049],\n",
       "                      [ 0.1926,  0.6323,  0.5884,  0.0823, -0.1637],\n",
       "                      [ 0.0357,  0.6058,  0.4701,  0.1145,  0.0587],\n",
       "                      [ 0.0825, -0.2932, -0.5307, -0.2750, -0.0528],\n",
       "                      [ 0.0190, -0.0227,  0.0123,  0.0339,  0.0383],\n",
       "                      [-0.0528,  0.2803,  0.2555, -0.3986,  0.4775],\n",
       "                      [-0.2669,  0.3133, -0.5719, -0.4551,  0.2065],\n",
       "                      [ 0.0010, -0.3380, -0.0653,  0.4477,  0.2569],\n",
       "                      [ 0.0452, -0.2006, -0.1431,  0.3212,  0.3149],\n",
       "                      [ 0.0266, -0.2057, -0.5958, -0.5501, -0.4116],\n",
       "                      [-0.1395,  0.3316,  0.5020,  0.2884,  0.1465],\n",
       "                      [ 0.3262,  0.7264, -0.0330, -0.1059, -0.1689],\n",
       "                      [ 0.0114, -0.0223, -0.1999, -0.1779,  0.1615],\n",
       "                      [ 0.0365,  0.0179,  0.2637,  0.4233, -0.1694],\n",
       "                      [-0.0915, -0.4354, -0.4500, -0.4426, -0.2059]], device='cuda:0')),\n",
       "             ('linear_proj_input.bias',\n",
       "              tensor([ 0.0100,  0.0071,  0.0844, -0.0034,  0.0269,  0.0238, -0.0045,  0.0793,\n",
       "                       0.0625, -0.3188, -0.0099,  0.0415,  0.0757,  0.0760, -0.0203,  0.0051,\n",
       "                       0.0982,  0.0564, -0.3540,  0.1301,  0.0385, -0.0146,  0.0923,  0.2359,\n",
       "                       0.0144,  0.0318,  0.0294,  0.0225, -0.0627,  0.0361, -0.1484,  0.2764],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_ih_l0',\n",
       "              tensor([[-0.1278,  0.0376, -0.1842,  ..., -0.1980, -0.0040,  0.4868],\n",
       "                      [-0.1214, -0.6813, -0.2087,  ..., -0.6877,  0.8646,  0.1304],\n",
       "                      [-0.0069,  0.1466, -0.0187,  ..., -0.0277,  0.0916, -0.3438],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0358,  0.1499,  ...,  0.0340, -0.0016,  0.0703],\n",
       "                      [ 0.0815,  0.1674,  0.2160,  ..., -0.0316, -0.0213, -0.1411],\n",
       "                      [-0.0575, -0.3069, -0.4484,  ..., -0.3001,  0.3361,  0.0985]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_hh_l0',\n",
       "              tensor([[ 0.2189,  0.0428,  0.0844,  ...,  0.2504, -0.0268,  0.0073],\n",
       "                      [ 0.0630, -0.2894, -0.0723,  ...,  0.2273, -0.0201, -0.2922],\n",
       "                      [-0.2359,  0.0580,  0.5384,  ...,  0.1263,  0.1494,  0.1284],\n",
       "                      ...,\n",
       "                      [-0.2278, -0.0027, -0.6881,  ..., -0.2839,  0.3137,  0.5305],\n",
       "                      [-0.4135,  0.3994,  0.3394,  ...,  0.0406, -0.2632,  0.4696],\n",
       "                      [-0.3352,  0.2047, -0.3199,  ...,  0.3021, -0.0407, -0.7308]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_ih_l0',\n",
       "              tensor([ 0.0130,  0.1700, -0.1462, -0.0976, -0.0626, -0.1343, -0.2688, -0.0255,\n",
       "                       0.0917,  0.0529, -0.1381, -0.1915, -0.3057, -0.0766, -0.1117, -0.1336,\n",
       "                      -0.4264, -0.1256,  0.0339,  0.2293,  0.1963, -0.0931,  0.2635, -0.0293,\n",
       "                      -0.0093, -0.1926,  0.0024, -0.3110, -0.1134, -0.0915, -0.2517,  0.2798],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_hh_l0',\n",
       "              tensor([ 0.0424,  0.1134, -0.1306, -0.1947,  0.1363, -0.2255, -0.1964,  0.0232,\n",
       "                      -0.0263, -0.0247, -0.1665,  0.0081, -0.2575, -0.1038,  0.1560,  0.0181,\n",
       "                      -0.4006, -0.2014, -0.1151,  0.3788,  0.1222,  0.0599,  0.1869, -0.0109,\n",
       "                       0.0263, -0.2602,  0.2703, -0.3092, -0.0599, -0.1606, -0.3937,  0.1160],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.weight',\n",
       "              tensor([[-0.0009,  0.0021,  0.1302, -0.0460,  0.1906, -0.2232,  0.0866, -0.1921,\n",
       "                        0.0141, -0.3874,  0.0812, -0.0047,  0.1224, -0.3435,  0.1172,  0.1629,\n",
       "                       -0.2513, -0.2081,  0.0180,  0.0354, -0.1746,  0.3808, -0.0896, -0.0624,\n",
       "                       -0.2368,  0.0778,  0.0821,  0.0196,  0.0193, -0.0776, -0.1531,  0.0097]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.bias', tensor([0.1542], device='cuda:0'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(optimizer=optimizer,model=RNN_model,train_loader=train_loader,val_loader=test_loader,list_train_loss=train_loss,list_val_loss=val_loss,device=device,n_epochs=100,ot_steps=10,report_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4ccf4",
   "metadata": {},
   "source": [
    "Above is an example of the training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_3_11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
